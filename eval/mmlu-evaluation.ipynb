{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9de9656",
   "metadata": {},
   "source": [
    "## Imports, Setup, Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "ab2496e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Common thresholds (used for all model–dataset combinations)\n",
    "THRESHOLDS = [0.25, 0.5, 0.75, 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "9750d8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 2 — Metric functions =====\n",
    "\n",
    "def decide(p: float, t: float) -> bool:\n",
    "    \"\"\"\n",
    "    Decision rule: returns True if the model 'answers' the question.\n",
    "    We assume the model only answers when its confidence p exceeds the threshold t.\n",
    "    \"\"\"\n",
    "    return p > t\n",
    "\n",
    "\n",
    "def penalty_score(pred: str, gold: str, p: float, t: float) -> float:\n",
    "    \"\"\"\n",
    "    Penalty-adjusted score (as defined in the paper/proposal):\n",
    "      - If p <= t  → the model abstains → score = 0.\n",
    "      - If p > t and prediction == gold → score = +1.\n",
    "      - If p > t and prediction != gold → score = - (p * t) / (1 - t)\n",
    "        (this is a confidence-proportional penalty for being overconfident).\n",
    "    \"\"\"\n",
    "    if p <= t:\n",
    "        return 0.0\n",
    "    if pred == gold:\n",
    "        return 1.0\n",
    "    return - (p * t) / (1.0 - t)\n",
    "\n",
    "\n",
    "#Metric 1\n",
    "def accuracy_at_threshold(df: pd.DataFrame, t: float) -> float:\n",
    "    \"\"\"\n",
    "    Accuracy@t:\n",
    "      Fraction of *answered* questions that are correct.\n",
    "      = (# correct with p>t) / (# answered with p>t)\n",
    "    If the model abstains on all (no p>t), returns 0.0.\n",
    "    \"\"\"\n",
    "    answered = df[\"confidence\"] > t\n",
    "    answered_n = answered.sum()\n",
    "    if answered_n == 0:\n",
    "        return 0.0\n",
    "    correct = (df[\"predicted_answer\"] == df[\"answer\"]) & answered\n",
    "    return float(correct.sum() / answered_n)\n",
    "\n",
    "#Metric 2\n",
    "def coverage(df: pd.DataFrame, t: float) -> float:\n",
    "    \"\"\"\n",
    "    Coverage:\n",
    "      Fraction of total questions that the model *answers*.\n",
    "      = (# p>t) / total\n",
    "    \"\"\"\n",
    "    if len(df) == 0:\n",
    "        return 0.0\n",
    "    return float((df[\"confidence\"] > t).sum() / len(df))\n",
    "\n",
    "#Metric 3\n",
    "def penalty_adjusted_mean(df: pd.DataFrame, t: float) -> float:\n",
    "    \"\"\"\n",
    "    Mean penalty-adjusted score across all rows (including abstains).\n",
    "    Abstentions contribute 0.\n",
    "    \"\"\"\n",
    "    scores = [\n",
    "        penalty_score(r.predicted_answer, r.answer, float(r.confidence), t)\n",
    "        for r in df.itertuples(index=False)\n",
    "    ]\n",
    "    return float(np.mean(scores)) if scores else 0.0\n",
    "\n",
    "#Metric 4\n",
    "def overconfidence_rate(df: pd.DataFrame, t: float) -> float:\n",
    "    \"\"\"\n",
    "    Overconfidence rate:\n",
    "      Fraction of questions where the model is *wrong* but still confident (p>t).\n",
    "      = (# wrong & p>t) / total\n",
    "    \"\"\"\n",
    "    if len(df) == 0:\n",
    "        return 0.0\n",
    "    mask = (df[\"predicted_answer\"] != df[\"answer\"]) & (df[\"confidence\"] > t)\n",
    "    return float(mask.sum() / len(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae96d5f1",
   "metadata": {},
   "source": [
    "## Qwen Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "cb006450",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = Path(\"../inference/outputs/qwen-mmlu.csv\")  # <-- change this for each run\n",
    "OUTPUT_PATH = Path(\"outputs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "5c99043c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 3400 rows from qwen-mmlu.csv\n",
      "Thresholds found: [np.float64(0.25), np.float64(0.5), np.float64(0.75), np.float64(0.9)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>threshold</th>\n",
       "      <th>question</th>\n",
       "      <th>choices</th>\n",
       "      <th>answer</th>\n",
       "      <th>predicted_answer</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>596</td>\n",
       "      <td>0.25</td>\n",
       "      <td>A manufacturer is currently selling 2000 units...</td>\n",
       "      <td>['$2.50' '$1.90' '$2.70' '$2.60' '$1.80' '$2.2...</td>\n",
       "      <td>F</td>\n",
       "      <td>IDK</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>814</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Fred Lowes is a typewriter salesman. He receiv...</td>\n",
       "      <td>['$210' '$200' '$225' '$175' '$195' '$150' '$2...</td>\n",
       "      <td>I</td>\n",
       "      <td>B</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>817</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Mary Redmond purchased a $28,500 home with 20%...</td>\n",
       "      <td>['$305' '$190' '$171' '$285.50' '$399' '$323' ...</td>\n",
       "      <td>F</td>\n",
       "      <td>IDK</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  threshold                                           question  \\\n",
       "0  596       0.25  A manufacturer is currently selling 2000 units...   \n",
       "1  814       0.25  Fred Lowes is a typewriter salesman. He receiv...   \n",
       "2  817       0.25  Mary Redmond purchased a $28,500 home with 20%...   \n",
       "\n",
       "                                             choices answer predicted_answer  \\\n",
       "0  ['$2.50' '$1.90' '$2.70' '$2.60' '$1.80' '$2.2...      F              IDK   \n",
       "1  ['$210' '$200' '$225' '$175' '$195' '$150' '$2...      I                B   \n",
       "2  ['$305' '$190' '$171' '$285.50' '$399' '$323' ...      F              IDK   \n",
       "\n",
       "   confidence  \n",
       "0    0.500000  \n",
       "1    0.833333  \n",
       "2    1.000000  "
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Normalize strings & types\n",
    "df[\"answer\"] = df[\"answer\"].astype(str).str.strip().str.upper()\n",
    "df[\"predicted_answer\"] = df[\"predicted_answer\"].astype(str).str.strip().str.upper()\n",
    "df[\"confidence\"] = pd.to_numeric(df[\"confidence\"], errors=\"coerce\").fillna(0.0)\n",
    "df[\"threshold\"] = pd.to_numeric(df[\"threshold\"], errors=\"coerce\")\n",
    "\n",
    "print(f\"✅ Loaded {len(df)} rows from {CSV_PATH.name}\")\n",
    "print(\"Thresholds found:\", sorted(df[\"threshold\"].unique()))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "059deb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=0.25: 850 rows\n",
      "t=0.5: 850 rows\n",
      "t=0.75: 850 rows\n",
      "t=0.9: 850 rows\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 4 — Split into per-threshold DataFrames =====\n",
    "dfs_by_t = {t: df[df[\"threshold\"] == t].copy() for t in THRESHOLDS}\n",
    "\n",
    "for t in THRESHOLDS:\n",
    "    print(f\"t={t}: {len(dfs_by_t[t])} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "c25d465a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "      <th>answered_n</th>\n",
       "      <th>total_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.079710</td>\n",
       "      <td>0.974118</td>\n",
       "      <td>-0.170190</td>\n",
       "      <td>0.896471</td>\n",
       "      <td>828</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.094183</td>\n",
       "      <td>0.849412</td>\n",
       "      <td>-0.601471</td>\n",
       "      <td>0.769412</td>\n",
       "      <td>722</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.081481</td>\n",
       "      <td>0.635294</td>\n",
       "      <td>-1.636941</td>\n",
       "      <td>0.583529</td>\n",
       "      <td>540</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.081013</td>\n",
       "      <td>0.464706</td>\n",
       "      <td>-3.805882</td>\n",
       "      <td>0.427059</td>\n",
       "      <td>395</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   threshold  accuracy_at_t  coverage  penalty_mean  overconf_rate  \\\n",
       "0       0.25       0.079710  0.974118     -0.170190       0.896471   \n",
       "1       0.50       0.094183  0.849412     -0.601471       0.769412   \n",
       "2       0.75       0.081481  0.635294     -1.636941       0.583529   \n",
       "3       0.90       0.081013  0.464706     -3.805882       0.427059   \n",
       "\n",
       "   answered_n  total_n  \n",
       "0         828      850  \n",
       "1         722      850  \n",
       "2         540      850  \n",
       "3         395      850  "
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== Cell 5 — Compute metrics per threshold =====\n",
    "metrics_rows = []\n",
    "\n",
    "for t in THRESHOLDS:\n",
    "    df_t = dfs_by_t[t]\n",
    "\n",
    "    metrics_rows.append({\n",
    "        \"threshold\": t,\n",
    "        \"accuracy_at_t\": accuracy_at_threshold(df_t, t),\n",
    "        \"coverage\": coverage(df_t, t),\n",
    "        \"penalty_mean\": penalty_adjusted_mean(df_t, t),\n",
    "        \"overconf_rate\": overconfidence_rate(df_t, t),\n",
    "        \"answered_n\": int((df_t[\"confidence\"] > t).sum()),\n",
    "        \"total_n\": len(df_t)\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_rows).sort_values(\"threshold\").reset_index(drop=True)\n",
    "metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "a7360cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4×4 Evaluation Table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.079710</td>\n",
       "      <td>0.974118</td>\n",
       "      <td>-0.170190</td>\n",
       "      <td>0.896471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.094183</td>\n",
       "      <td>0.849412</td>\n",
       "      <td>-0.601471</td>\n",
       "      <td>0.769412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.081481</td>\n",
       "      <td>0.635294</td>\n",
       "      <td>-1.636941</td>\n",
       "      <td>0.583529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>0.081013</td>\n",
       "      <td>0.464706</td>\n",
       "      <td>-3.805882</td>\n",
       "      <td>0.427059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "threshold                                                      \n",
       "0.25            0.079710  0.974118     -0.170190       0.896471\n",
       "0.50            0.094183  0.849412     -0.601471       0.769412\n",
       "0.75            0.081481  0.635294     -1.636941       0.583529\n",
       "0.90            0.081013  0.464706     -3.805882       0.427059"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results in: outputs\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 6 — 4×4 evaluation table =====\n",
    "eval_table = (\n",
    "    metrics_df[[\"threshold\", \"accuracy_at_t\", \"coverage\", \"penalty_mean\", \"overconf_rate\"]]\n",
    "    .set_index(\"threshold\")\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# Display\n",
    "print(\"4×4 Evaluation Table:\")\n",
    "display(eval_table)\n",
    "\n",
    "# Save both detailed and compact tables\n",
    "eval_table.to_csv(OUTPUT_PATH / \"qwen-mmlu-metric-eval.csv\")\n",
    "\n",
    "print(f\"Saved results in: {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6e8a28",
   "metadata": {},
   "source": [
    "### Baseline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "a6757486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG — eligible rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.079710</td>\n",
       "      <td>0.974118</td>\n",
       "      <td>-0.170190</td>\n",
       "      <td>0.896471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.094183</td>\n",
       "      <td>0.849412</td>\n",
       "      <td>-0.601471</td>\n",
       "      <td>0.769412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.081481</td>\n",
       "      <td>0.635294</td>\n",
       "      <td>-1.636941</td>\n",
       "      <td>0.583529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>0.081013</td>\n",
       "      <td>0.464706</td>\n",
       "      <td>-3.805882</td>\n",
       "      <td>0.427059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "threshold                                                      \n",
       "0.25            0.079710  0.974118     -0.170190       0.896471\n",
       "0.50            0.094183  0.849412     -0.601471       0.769412\n",
       "0.75            0.081481  0.635294     -1.636941       0.583529\n",
       "0.90            0.081013  0.464706     -3.805882       0.427059"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected t* = 0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "accuracy_at_t    0.094183\n",
       "coverage         0.849412\n",
       "penalty_mean    -0.601471\n",
       "overconf_rate    0.769412\n",
       "Name: 0.5, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "coverage_floor = 0.3\n",
    "\n",
    "eligible = eval_table[eval_table[\"coverage\"] >= coverage_floor]\n",
    "print(\"DEBUG — eligible rows:\")\n",
    "display(eligible)\n",
    "\n",
    "best_row = eligible.loc[eligible[\"accuracy_at_t\"].idxmax()]\n",
    "t_star = best_row.name\n",
    "\n",
    "print(f\"Selected t* = {t_star}\")\n",
    "display(best_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "4e278d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary-grading baseline:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Binary</th>\n",
       "      <td>0.078824</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.921176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "Binary       0.078824       1.0           NaN       0.921176"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Always-abstain baseline:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Abstain</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "Abstain            0.0       0.0           0.0            0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Cell 2 – Compute binary-grading and always-abstain baselines =====\n",
    "import numpy as np\n",
    "\n",
    "# Load the original prediction CSV (same used to compute metrics_df)\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Compute overall accuracy\n",
    "binary_acc = np.mean(df[\"predicted_answer\"] == df[\"answer\"])\n",
    "\n",
    "# Wrong rate (for overconfidence)\n",
    "wrong_rate = 1 - binary_acc\n",
    "\n",
    "# Binary baseline metrics (answers everything)\n",
    "binary_row = {\n",
    "    \"accuracy_at_t\": binary_acc,\n",
    "    \"coverage\": 1.0,\n",
    "    \"penalty_mean\": np.nan,  # you can fill with projected penalty if desired\n",
    "    \"overconf_rate\": wrong_rate,\n",
    "}\n",
    "\n",
    "# Always-abstain baseline metrics\n",
    "abstain_row = {\n",
    "    \"accuracy_at_t\": 0.0,\n",
    "    \"coverage\": 0.0,\n",
    "    \"penalty_mean\": 0.0,\n",
    "    \"overconf_rate\": 0.0,\n",
    "}\n",
    "\n",
    "print(\"Binary-grading baseline:\")\n",
    "display(pd.DataFrame([binary_row], index=[\"Binary\"]))\n",
    "print(\"Always-abstain baseline:\")\n",
    "display(pd.DataFrame([abstain_row], index=[\"Abstain\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "eb6d0541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 3x4 Headline Evaluation Table ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Confidence-aware (t*=0.5)</th>\n",
       "      <td>0.094183</td>\n",
       "      <td>0.849412</td>\n",
       "      <td>-0.601471</td>\n",
       "      <td>0.769412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Binary grading</th>\n",
       "      <td>0.078824</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.921176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Always abstain</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           accuracy_at_t  coverage  penalty_mean  \\\n",
       "Confidence-aware (t*=0.5)       0.094183  0.849412     -0.601471   \n",
       "Binary grading                  0.078824  1.000000           NaN   \n",
       "Always abstain                  0.000000  0.000000      0.000000   \n",
       "\n",
       "                           overconf_rate  \n",
       "Confidence-aware (t*=0.5)       0.769412  \n",
       "Binary grading                  0.921176  \n",
       "Always abstain                  0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Cell 3 – Build final 3x4 headline table =====\n",
    "\n",
    "headline_df = pd.DataFrame([\n",
    "    best_row[[\"accuracy_at_t\", \"coverage\", \"penalty_mean\", \"overconf_rate\"]],\n",
    "    pd.Series(binary_row),\n",
    "    pd.Series(abstain_row)\n",
    "], index=[f\"Confidence-aware (t*={t_star})\", \"Binary grading\", \"Always abstain\"])\n",
    "\n",
    "print(\"=== 3x4 Headline Evaluation Table ===\")\n",
    "display(headline_df)\n",
    "headline_df.to_csv(OUTPUT_PATH / \"qwen-mmlu-baseline-eval.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bab1ca",
   "metadata": {},
   "source": [
    "## GPT Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "d2e305c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = Path(\"../inference/outputs/gpt-mmlu.csv\")  # <-- change this for each run\n",
    "OUTPUT_PATH = Path(\"outputs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "f67a564c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 3400 rows from gpt-mmlu.csv\n",
      "Thresholds found: [np.float64(0.25), np.float64(0.5), np.float64(0.75), np.float64(0.9)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>threshold</th>\n",
       "      <th>question</th>\n",
       "      <th>choices</th>\n",
       "      <th>answer</th>\n",
       "      <th>predicted_answer</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>596</td>\n",
       "      <td>0.25</td>\n",
       "      <td>A manufacturer is currently selling 2000 units...</td>\n",
       "      <td>['$2.50' '$1.90' '$2.70' '$2.60' '$1.80' '$2.2...</td>\n",
       "      <td>F</td>\n",
       "      <td>D</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>814</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Fred Lowes is a typewriter salesman. He receiv...</td>\n",
       "      <td>['$210' '$200' '$225' '$175' '$195' '$150' '$2...</td>\n",
       "      <td>I</td>\n",
       "      <td>B</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>817</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Mary Redmond purchased a $28,500 home with 20%...</td>\n",
       "      <td>['$305' '$190' '$171' '$285.50' '$399' '$323' ...</td>\n",
       "      <td>F</td>\n",
       "      <td>C</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  threshold                                           question  \\\n",
       "0  596       0.25  A manufacturer is currently selling 2000 units...   \n",
       "1  814       0.25  Fred Lowes is a typewriter salesman. He receiv...   \n",
       "2  817       0.25  Mary Redmond purchased a $28,500 home with 20%...   \n",
       "\n",
       "                                             choices answer predicted_answer  \\\n",
       "0  ['$2.50' '$1.90' '$2.70' '$2.60' '$1.80' '$2.2...      F                D   \n",
       "1  ['$210' '$200' '$225' '$175' '$195' '$150' '$2...      I                B   \n",
       "2  ['$305' '$190' '$171' '$285.50' '$399' '$323' ...      F                C   \n",
       "\n",
       "   confidence  \n",
       "0    0.333333  \n",
       "1    0.666667  \n",
       "2    0.500000  "
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Normalize strings & types\n",
    "df[\"answer\"] = df[\"answer\"].astype(str).str.strip().str.upper()\n",
    "df[\"predicted_answer\"] = df[\"predicted_answer\"].astype(str).str.strip().str.upper()\n",
    "df[\"confidence\"] = pd.to_numeric(df[\"confidence\"], errors=\"coerce\").fillna(0.0)\n",
    "df[\"threshold\"] = pd.to_numeric(df[\"threshold\"], errors=\"coerce\")\n",
    "\n",
    "print(f\"✅ Loaded {len(df)} rows from {CSV_PATH.name}\")\n",
    "print(\"Thresholds found:\", sorted(df[\"threshold\"].unique()))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "9052885d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=0.25: 850 rows\n",
      "t=0.5: 850 rows\n",
      "t=0.75: 850 rows\n",
      "t=0.9: 850 rows\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 4 — Split into per-threshold DataFrames =====\n",
    "dfs_by_t = {t: df[df[\"threshold\"] == t].copy() for t in THRESHOLDS}\n",
    "\n",
    "for t in THRESHOLDS:\n",
    "    print(f\"t={t}: {len(dfs_by_t[t])} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "056e243e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "      <th>answered_n</th>\n",
       "      <th>total_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.197647</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.011542</td>\n",
       "      <td>0.802353</td>\n",
       "      <td>850</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.208517</td>\n",
       "      <td>0.801176</td>\n",
       "      <td>-0.386275</td>\n",
       "      <td>0.634118</td>\n",
       "      <td>681</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.248000</td>\n",
       "      <td>0.441176</td>\n",
       "      <td>-0.834118</td>\n",
       "      <td>0.331765</td>\n",
       "      <td>375</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   threshold  accuracy_at_t  coverage  penalty_mean  overconf_rate  \\\n",
       "0       0.25       0.197647  1.000000     -0.011542       0.802353   \n",
       "1       0.50       0.208517  0.801176     -0.386275       0.634118   \n",
       "2       0.75       0.248000  0.441176     -0.834118       0.331765   \n",
       "3       0.90       0.000000  0.000000      0.000000       0.000000   \n",
       "\n",
       "   answered_n  total_n  \n",
       "0         850      850  \n",
       "1         681      850  \n",
       "2         375      850  \n",
       "3           0      850  "
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== Cell 5 — Compute metrics per threshold =====\n",
    "metrics_rows = []\n",
    "\n",
    "for t in THRESHOLDS:\n",
    "    df_t = dfs_by_t[t]\n",
    "\n",
    "    metrics_rows.append({\n",
    "        \"threshold\": t,\n",
    "        \"accuracy_at_t\": accuracy_at_threshold(df_t, t),\n",
    "        \"coverage\": coverage(df_t, t),\n",
    "        \"penalty_mean\": penalty_adjusted_mean(df_t, t),\n",
    "        \"overconf_rate\": overconfidence_rate(df_t, t),\n",
    "        \"answered_n\": int((df_t[\"confidence\"] > t).sum()),\n",
    "        \"total_n\": len(df_t)\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_rows).sort_values(\"threshold\").reset_index(drop=True)\n",
    "metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "fd275867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4×4 Evaluation Table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.197647</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.011542</td>\n",
       "      <td>0.802353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.208517</td>\n",
       "      <td>0.801176</td>\n",
       "      <td>-0.386275</td>\n",
       "      <td>0.634118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.248000</td>\n",
       "      <td>0.441176</td>\n",
       "      <td>-0.834118</td>\n",
       "      <td>0.331765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "threshold                                                      \n",
       "0.25            0.197647  1.000000     -0.011542       0.802353\n",
       "0.50            0.208517  0.801176     -0.386275       0.634118\n",
       "0.75            0.248000  0.441176     -0.834118       0.331765\n",
       "0.90            0.000000  0.000000      0.000000       0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results in: outputs\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 6 — 4×4 evaluation table =====\n",
    "eval_table = (\n",
    "    metrics_df[[\"threshold\", \"accuracy_at_t\", \"coverage\", \"penalty_mean\", \"overconf_rate\"]]\n",
    "    .set_index(\"threshold\")\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# Display\n",
    "print(\"4×4 Evaluation Table:\")\n",
    "display(eval_table)\n",
    "\n",
    "# Save both detailed and compact tables\n",
    "eval_table.to_csv(OUTPUT_PATH / \"gpt-mmlu-metric-eval.csv\")\n",
    "\n",
    "print(f\"Saved results in: {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37f5925",
   "metadata": {},
   "source": [
    "### Baseline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "709cdde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG — eligible rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.197647</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.011542</td>\n",
       "      <td>0.802353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.208517</td>\n",
       "      <td>0.801176</td>\n",
       "      <td>-0.386275</td>\n",
       "      <td>0.634118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.248000</td>\n",
       "      <td>0.441176</td>\n",
       "      <td>-0.834118</td>\n",
       "      <td>0.331765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "threshold                                                      \n",
       "0.25            0.197647  1.000000     -0.011542       0.802353\n",
       "0.50            0.208517  0.801176     -0.386275       0.634118\n",
       "0.75            0.248000  0.441176     -0.834118       0.331765"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected t* = 0.75\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "accuracy_at_t    0.248000\n",
       "coverage         0.441176\n",
       "penalty_mean    -0.834118\n",
       "overconf_rate    0.331765\n",
       "Name: 0.75, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "coverage_floor = 0.3\n",
    "\n",
    "eligible = eval_table[eval_table[\"coverage\"] >= coverage_floor]\n",
    "print(\"DEBUG — eligible rows:\")\n",
    "display(eligible)\n",
    "\n",
    "best_row = eligible.loc[eligible[\"accuracy_at_t\"].idxmax()]\n",
    "t_star = best_row.name\n",
    "\n",
    "print(f\"Selected t* = {t_star}\")\n",
    "display(best_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "290126c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary-grading baseline:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Binary</th>\n",
       "      <td>0.134118</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.865882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "Binary       0.134118       1.0           NaN       0.865882"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Always-abstain baseline:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Abstain</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "Abstain            0.0       0.0           0.0            0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Cell 2 – Compute binary-grading and always-abstain baselines =====\n",
    "import numpy as np\n",
    "\n",
    "# Load the original prediction CSV (same used to compute metrics_df)\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Compute overall accuracy\n",
    "binary_acc = np.mean(df[\"predicted_answer\"] == df[\"answer\"])\n",
    "\n",
    "# Wrong rate (for overconfidence)\n",
    "wrong_rate = 1 - binary_acc\n",
    "\n",
    "# Binary baseline metrics (answers everything)\n",
    "binary_row = {\n",
    "    \"accuracy_at_t\": binary_acc,\n",
    "    \"coverage\": 1.0,\n",
    "    \"penalty_mean\": np.nan,  # you can fill with projected penalty if desired\n",
    "    \"overconf_rate\": wrong_rate,\n",
    "}\n",
    "\n",
    "# Always-abstain baseline metrics\n",
    "abstain_row = {\n",
    "    \"accuracy_at_t\": 0.0,\n",
    "    \"coverage\": 0.0,\n",
    "    \"penalty_mean\": 0.0,\n",
    "    \"overconf_rate\": 0.0,\n",
    "}\n",
    "\n",
    "print(\"Binary-grading baseline:\")\n",
    "display(pd.DataFrame([binary_row], index=[\"Binary\"]))\n",
    "print(\"Always-abstain baseline:\")\n",
    "display(pd.DataFrame([abstain_row], index=[\"Abstain\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "f8b39a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 3x4 Headline Evaluation Table ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Confidence-aware (t*=0.75)</th>\n",
       "      <td>0.248000</td>\n",
       "      <td>0.441176</td>\n",
       "      <td>-0.834118</td>\n",
       "      <td>0.331765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Binary grading</th>\n",
       "      <td>0.134118</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.865882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Always abstain</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            accuracy_at_t  coverage  penalty_mean  \\\n",
       "Confidence-aware (t*=0.75)       0.248000  0.441176     -0.834118   \n",
       "Binary grading                   0.134118  1.000000           NaN   \n",
       "Always abstain                   0.000000  0.000000      0.000000   \n",
       "\n",
       "                            overconf_rate  \n",
       "Confidence-aware (t*=0.75)       0.331765  \n",
       "Binary grading                   0.865882  \n",
       "Always abstain                   0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Cell 3 – Build final 3x4 headline table =====\n",
    "\n",
    "headline_df = pd.DataFrame([\n",
    "    best_row[[\"accuracy_at_t\", \"coverage\", \"penalty_mean\", \"overconf_rate\"]],\n",
    "    pd.Series(binary_row),\n",
    "    pd.Series(abstain_row)\n",
    "], index=[f\"Confidence-aware (t*={t_star})\", \"Binary grading\", \"Always abstain\"])\n",
    "\n",
    "print(\"=== 3x4 Headline Evaluation Table ===\")\n",
    "display(headline_df)\n",
    "headline_df.to_csv(OUTPUT_PATH / \"gpt-mmlu-baseline-eval.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99be7c33",
   "metadata": {},
   "source": [
    "## Claude Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "0059b7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPQA - Qwen Evalution \n",
    "CSV_PATH = Path(\"../inference/outputs/claude-mmlu.csv\")  # <-- change this for each run\n",
    "OUTPUT_PATH = Path(\"outputs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "56bd997c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 3400 rows from claude-mmlu.csv\n",
      "Thresholds found: [np.float64(0.25), np.float64(0.5), np.float64(0.75), np.float64(0.9)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>threshold</th>\n",
       "      <th>question</th>\n",
       "      <th>choices</th>\n",
       "      <th>answer</th>\n",
       "      <th>predicted_answer</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>596</td>\n",
       "      <td>0.25</td>\n",
       "      <td>A manufacturer is currently selling 2000 units...</td>\n",
       "      <td>['$2.50' '$1.90' '$2.70' '$2.60' '$1.80' '$2.2...</td>\n",
       "      <td>F</td>\n",
       "      <td>I</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>814</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Fred Lowes is a typewriter salesman. He receiv...</td>\n",
       "      <td>['$210' '$200' '$225' '$175' '$195' '$150' '$2...</td>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>817</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Mary Redmond purchased a $28,500 home with 20%...</td>\n",
       "      <td>['$305' '$190' '$171' '$285.50' '$399' '$323' ...</td>\n",
       "      <td>F</td>\n",
       "      <td>I</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  threshold                                           question  \\\n",
       "0  596       0.25  A manufacturer is currently selling 2000 units...   \n",
       "1  814       0.25  Fred Lowes is a typewriter salesman. He receiv...   \n",
       "2  817       0.25  Mary Redmond purchased a $28,500 home with 20%...   \n",
       "\n",
       "                                             choices answer predicted_answer  \\\n",
       "0  ['$2.50' '$1.90' '$2.70' '$2.60' '$1.80' '$2.2...      F                I   \n",
       "1  ['$210' '$200' '$225' '$175' '$195' '$150' '$2...      I                I   \n",
       "2  ['$305' '$190' '$171' '$285.50' '$399' '$323' ...      F                I   \n",
       "\n",
       "   confidence  \n",
       "0         1.0  \n",
       "1         1.0  \n",
       "2         1.0  "
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Normalize strings & types\n",
    "df[\"answer\"] = df[\"answer\"].astype(str).str.strip().str.upper()\n",
    "df[\"predicted_answer\"] = df[\"predicted_answer\"].astype(str).str.strip().str.upper()\n",
    "df[\"confidence\"] = pd.to_numeric(df[\"confidence\"], errors=\"coerce\").fillna(0.0)\n",
    "df[\"threshold\"] = pd.to_numeric(df[\"threshold\"], errors=\"coerce\")\n",
    "\n",
    "print(f\"✅ Loaded {len(df)} rows from {CSV_PATH.name}\")\n",
    "print(\"Thresholds found:\", sorted(df[\"threshold\"].unique()))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "bc7688f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=0.25: 850 rows\n",
      "t=0.5: 850 rows\n",
      "t=0.75: 850 rows\n",
      "t=0.9: 850 rows\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 4 — Split into per-threshold DataFrames =====\n",
    "dfs_by_t = {t: df[df[\"threshold\"] == t].copy() for t in THRESHOLDS}\n",
    "\n",
    "for t in THRESHOLDS:\n",
    "    print(f\"t={t}: {len(dfs_by_t[t])} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "9c25f754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 5 — Compute metrics per threshold =====\n",
    "metrics_rows = []\n",
    "\n",
    "for t in THRESHOLDS:\n",
    "    df_t = dfs_by_t[t]\n",
    "\n",
    "    metrics_rows.append({\n",
    "        \"threshold\": t,\n",
    "        \"accuracy_at_t\": accuracy_at_threshold(df_t, t),\n",
    "        \"coverage\": coverage(df_t, t),\n",
    "        \"penalty_mean\": penalty_adjusted_mean(df_t, t),\n",
    "        \"overconf_rate\": overconfidence_rate(df_t, t),\n",
    "        \"answered_n\": int((df_t[\"confidence\"] > t).sum()),\n",
    "        \"total_n\": len(df_t)\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_rows).sort_values(\"threshold\").reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "92a93fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4×4 Evaluation Table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.134454</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>-0.133105</td>\n",
       "      <td>0.848235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.137107</td>\n",
       "      <td>0.935294</td>\n",
       "      <td>-0.642902</td>\n",
       "      <td>0.807059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>-2.230353</td>\n",
       "      <td>0.794118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>0.110957</td>\n",
       "      <td>0.848235</td>\n",
       "      <td>-6.692941</td>\n",
       "      <td>0.754118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "threshold                                                      \n",
       "0.25            0.134454  0.980000     -0.133105       0.848235\n",
       "0.50            0.137107  0.935294     -0.642902       0.807059\n",
       "0.75            0.129032  0.911765     -2.230353       0.794118\n",
       "0.90            0.110957  0.848235     -6.692941       0.754118"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results in: outputs\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 6 — 4×4 evaluation table =====\n",
    "eval_table = (\n",
    "    metrics_df[[\"threshold\", \"accuracy_at_t\", \"coverage\", \"penalty_mean\", \"overconf_rate\"]]\n",
    "    .set_index(\"threshold\")\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# Display\n",
    "print(\"4×4 Evaluation Table:\")\n",
    "display(eval_table)\n",
    "\n",
    "# Save both detailed and compact tables\n",
    "eval_table.to_csv(OUTPUT_PATH / \"claude-mmlu-metric-eval.csv\")\n",
    "\n",
    "print(f\"Saved results in: {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1742c3",
   "metadata": {},
   "source": [
    "### Baseline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "40cddaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG — eligible rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.134454</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>-0.133105</td>\n",
       "      <td>0.848235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.137107</td>\n",
       "      <td>0.935294</td>\n",
       "      <td>-0.642902</td>\n",
       "      <td>0.807059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>-2.230353</td>\n",
       "      <td>0.794118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>0.110957</td>\n",
       "      <td>0.848235</td>\n",
       "      <td>-6.692941</td>\n",
       "      <td>0.754118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "threshold                                                      \n",
       "0.25            0.134454  0.980000     -0.133105       0.848235\n",
       "0.50            0.137107  0.935294     -0.642902       0.807059\n",
       "0.75            0.129032  0.911765     -2.230353       0.794118\n",
       "0.90            0.110957  0.848235     -6.692941       0.754118"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected t* = 0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "accuracy_at_t    0.137107\n",
       "coverage         0.935294\n",
       "penalty_mean    -0.642902\n",
       "overconf_rate    0.807059\n",
       "Name: 0.5, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "coverage_floor = 0.3\n",
    "\n",
    "eligible = eval_table[eval_table[\"coverage\"] >= coverage_floor]\n",
    "print(\"DEBUG — eligible rows:\")\n",
    "display(eligible)\n",
    "\n",
    "best_row = eligible.loc[eligible[\"accuracy_at_t\"].idxmax()]\n",
    "t_star = best_row.name\n",
    "\n",
    "print(f\"Selected t* = {t_star}\")\n",
    "display(best_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "9634128b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary-grading baseline:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Binary</th>\n",
       "      <td>0.127647</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.872353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "Binary       0.127647       1.0           NaN       0.872353"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Always-abstain baseline:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Abstain</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "Abstain            0.0       0.0           0.0            0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Cell 2 – Compute binary-grading and always-abstain baselines =====\n",
    "import numpy as np\n",
    "\n",
    "# Load the original prediction CSV (same used to compute metrics_df)\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Compute overall accuracy\n",
    "binary_acc = np.mean(df[\"predicted_answer\"] == df[\"answer\"])\n",
    "\n",
    "# Wrong rate (for overconfidence)\n",
    "wrong_rate = 1 - binary_acc\n",
    "\n",
    "# Binary baseline metrics (answers everything)\n",
    "binary_row = {\n",
    "    \"accuracy_at_t\": binary_acc,\n",
    "    \"coverage\": 1.0,\n",
    "    \"penalty_mean\": np.nan,  # you can fill with projected penalty if desired\n",
    "    \"overconf_rate\": wrong_rate,\n",
    "}\n",
    "\n",
    "# Always-abstain baseline metrics\n",
    "abstain_row = {\n",
    "    \"accuracy_at_t\": 0.0,\n",
    "    \"coverage\": 0.0,\n",
    "    \"penalty_mean\": 0.0,\n",
    "    \"overconf_rate\": 0.0,\n",
    "}\n",
    "\n",
    "print(\"Binary-grading baseline:\")\n",
    "display(pd.DataFrame([binary_row], index=[\"Binary\"]))\n",
    "print(\"Always-abstain baseline:\")\n",
    "display(pd.DataFrame([abstain_row], index=[\"Abstain\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "098a62f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 3x4 Headline Evaluation Table ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Confidence-aware (t*=0.5)</th>\n",
       "      <td>0.137107</td>\n",
       "      <td>0.935294</td>\n",
       "      <td>-0.642902</td>\n",
       "      <td>0.807059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Binary grading</th>\n",
       "      <td>0.127647</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.872353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Always abstain</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           accuracy_at_t  coverage  penalty_mean  \\\n",
       "Confidence-aware (t*=0.5)       0.137107  0.935294     -0.642902   \n",
       "Binary grading                  0.127647  1.000000           NaN   \n",
       "Always abstain                  0.000000  0.000000      0.000000   \n",
       "\n",
       "                           overconf_rate  \n",
       "Confidence-aware (t*=0.5)       0.807059  \n",
       "Binary grading                  0.872353  \n",
       "Always abstain                  0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Cell 3 – Build final 3x4 headline table =====\n",
    "\n",
    "headline_df = pd.DataFrame([\n",
    "    best_row[[\"accuracy_at_t\", \"coverage\", \"penalty_mean\", \"overconf_rate\"]],\n",
    "    pd.Series(binary_row),\n",
    "    pd.Series(abstain_row)\n",
    "], index=[f\"Confidence-aware (t*={t_star})\", \"Binary grading\", \"Always abstain\"])\n",
    "\n",
    "print(\"=== 3x4 Headline Evaluation Table ===\")\n",
    "display(headline_df)\n",
    "headline_df.to_csv(OUTPUT_PATH / \"claude-mmlu-baseline-eval.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe2d7bb",
   "metadata": {},
   "source": [
    "## LLama Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "eaba8e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPQA - Qwen Evalution \n",
    "CSV_PATH = Path(\"../inference/outputs/llama-mmlu.csv\")  # <-- change this for each run\n",
    "OUTPUT_PATH = Path(\"outputs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "1b089aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# # Normalize strings & types\n",
    "# df[\"answer\"] = df[\"answer\"].astype(str).str.strip().str.upper()\n",
    "# df[\"predicted_answer\"] = df[\"predicted_answer\"].astype(str).str.strip().str.upper()\n",
    "# df[\"confidence\"] = pd.to_numeric(df[\"confidence\"], errors=\"coerce\").fillna(0.0)\n",
    "# df[\"threshold\"] = pd.to_numeric(df[\"threshold\"], errors=\"coerce\")\n",
    "\n",
    "# print(f\"✅ Loaded {len(df)} rows from {CSV_PATH.name}\")\n",
    "# print(\"Thresholds found:\", sorted(df[\"threshold\"].unique()))\n",
    "# df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "f2bfbd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===== Cell 4 — Split into per-threshold DataFrames =====\n",
    "# dfs_by_t = {t: df[df[\"threshold\"] == t].copy() for t in THRESHOLDS}\n",
    "\n",
    "# for t in THRESHOLDS:\n",
    "#     print(f\"t={t}: {len(dfs_by_t[t])} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "bc1fe3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===== Cell 5 — Compute metrics per threshold =====\n",
    "# metrics_rows = []\n",
    "\n",
    "# for t in THRESHOLDS:\n",
    "#     df_t = dfs_by_t[t]\n",
    "\n",
    "#     metrics_rows.append({\n",
    "#         \"threshold\": t,\n",
    "#         \"accuracy_at_t\": accuracy_at_threshold(df_t, t),\n",
    "#         \"coverage\": coverage(df_t, t),\n",
    "#         \"penalty_mean\": penalty_adjusted_mean(df_t, t),\n",
    "#         \"overconf_rate\": overconfidence_rate(df_t, t),\n",
    "#         \"answered_n\": int((df_t[\"confidence\"] > t).sum()),\n",
    "#         \"total_n\": len(df_t)\n",
    "#     })\n",
    "\n",
    "# metrics_df = pd.DataFrame(metrics_rows).sort_values(\"threshold\").reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "1e9cd275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===== Cell 6 — 4×4 evaluation table =====\n",
    "# eval_table = (\n",
    "#     metrics_df[[\"threshold\", \"accuracy_at_t\", \"coverage\", \"penalty_mean\", \"overconf_rate\"]]\n",
    "#     .set_index(\"threshold\")\n",
    "#     .sort_index()\n",
    "# )\n",
    "\n",
    "# # Display\n",
    "# print(\"4×4 Evaluation Table:\")\n",
    "# display(eval_table)\n",
    "\n",
    "# # Save both detailed and compact tables\n",
    "# eval_table.to_csv(OUTPUT_PATH / \"llama-mmlu-metric-eval.csv\")\n",
    "\n",
    "# print(f\"Saved results in: {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cfe38a",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "f5bed551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coverage_floor = 0.3\n",
    "\n",
    "# eligible = eval_table[eval_table[\"coverage\"] >= coverage_floor]\n",
    "# print(\"DEBUG — eligible rows:\")\n",
    "# display(eligible)\n",
    "\n",
    "# best_row = eligible.loc[eligible[\"accuracy_at_t\"].idxmax()]\n",
    "# t_star = best_row.name\n",
    "\n",
    "# print(f\"Selected t* = {t_star}\")\n",
    "# display(best_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "02c2e9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===== Cell 2 – Compute binary-grading and always-abstain baselines =====\n",
    "# import numpy as np\n",
    "\n",
    "# # Load the original prediction CSV (same used to compute metrics_df)\n",
    "# df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# # Compute overall accuracy\n",
    "# binary_acc = np.mean(df[\"predicted_answer\"] == df[\"answer\"])\n",
    "\n",
    "# # Wrong rate (for overconfidence)\n",
    "# wrong_rate = 1 - binary_acc\n",
    "\n",
    "# # Binary baseline metrics (answers everything)\n",
    "# binary_row = {\n",
    "#     \"accuracy_at_t\": binary_acc,\n",
    "#     \"coverage\": 1.0,\n",
    "#     \"penalty_mean\": np.nan,  # you can fill with projected penalty if desired\n",
    "#     \"overconf_rate\": wrong_rate,\n",
    "# }\n",
    "\n",
    "# # Always-abstain baseline metrics\n",
    "# abstain_row = {\n",
    "#     \"accuracy_at_t\": 0.0,\n",
    "#     \"coverage\": 0.0,\n",
    "#     \"penalty_mean\": 0.0,\n",
    "#     \"overconf_rate\": 0.0,\n",
    "# }\n",
    "\n",
    "# print(\"Binary-grading baseline:\")\n",
    "# display(pd.DataFrame([binary_row], index=[\"Binary\"]))\n",
    "# print(\"Always-abstain baseline:\")\n",
    "# display(pd.DataFrame([abstain_row], index=[\"Abstain\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "59a99052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===== Cell 3 – Build final 3x4 headline table =====\n",
    "\n",
    "# headline_df = pd.DataFrame([\n",
    "#     best_row[[\"accuracy_at_t\", \"coverage\", \"penalty_mean\", \"overconf_rate\"]],\n",
    "#     pd.Series(binary_row),\n",
    "#     pd.Series(abstain_row)\n",
    "# ], index=[f\"Confidence-aware (t*={t_star})\", \"Binary grading\", \"Always abstain\"])\n",
    "\n",
    "# print(\"=== 3x4 Headline Evaluation Table ===\")\n",
    "# display(headline_df)\n",
    "# headline_df.to_csv(OUTPUT_PATH / \"llama-mmlu-baseline-eval.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c18581",
   "metadata": {},
   "source": [
    "## Gemini Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "4cae51bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPQA - Qwen Evalution \n",
    "CSV_PATH = Path(\"../inference/outputs/gemini-mmlu.csv\")  # <-- change this for each run\n",
    "OUTPUT_PATH = Path(\"outputs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "5c6288b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 3400 rows from gemini-mmlu.csv\n",
      "Thresholds found: [np.float64(0.25), np.float64(0.5), np.float64(0.75), np.float64(0.9)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>threshold</th>\n",
       "      <th>question</th>\n",
       "      <th>choices</th>\n",
       "      <th>answer</th>\n",
       "      <th>predicted_answer</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>596</td>\n",
       "      <td>0.25</td>\n",
       "      <td>A manufacturer is currently selling 2000 units...</td>\n",
       "      <td>['$2.50' '$1.90' '$2.70' '$2.60' '$1.80' '$2.2...</td>\n",
       "      <td>F</td>\n",
       "      <td>X</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>814</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Fred Lowes is a typewriter salesman. He receiv...</td>\n",
       "      <td>['$210' '$200' '$225' '$175' '$195' '$150' '$2...</td>\n",
       "      <td>I</td>\n",
       "      <td>S</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>817</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Mary Redmond purchased a $28,500 home with 20%...</td>\n",
       "      <td>['$305' '$190' '$171' '$285.50' '$399' '$323' ...</td>\n",
       "      <td>F</td>\n",
       "      <td>S</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  threshold                                           question  \\\n",
       "0  596       0.25  A manufacturer is currently selling 2000 units...   \n",
       "1  814       0.25  Fred Lowes is a typewriter salesman. He receiv...   \n",
       "2  817       0.25  Mary Redmond purchased a $28,500 home with 20%...   \n",
       "\n",
       "                                             choices answer predicted_answer  \\\n",
       "0  ['$2.50' '$1.90' '$2.70' '$2.60' '$1.80' '$2.2...      F                X   \n",
       "1  ['$210' '$200' '$225' '$175' '$195' '$150' '$2...      I                S   \n",
       "2  ['$305' '$190' '$171' '$285.50' '$399' '$323' ...      F                S   \n",
       "\n",
       "   confidence  \n",
       "0         1.0  \n",
       "1         1.0  \n",
       "2         1.0  "
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Normalize strings & types\n",
    "df[\"answer\"] = df[\"answer\"].astype(str).str.strip().str.upper()\n",
    "df[\"predicted_answer\"] = df[\"predicted_answer\"].astype(str).str.strip().str.upper()\n",
    "df[\"confidence\"] = pd.to_numeric(df[\"confidence\"], errors=\"coerce\").fillna(0.0)\n",
    "df[\"threshold\"] = pd.to_numeric(df[\"threshold\"], errors=\"coerce\")\n",
    "\n",
    "print(f\"✅ Loaded {len(df)} rows from {CSV_PATH.name}\")\n",
    "print(\"Thresholds found:\", sorted(df[\"threshold\"].unique()))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "f0828fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=0.25: 850 rows\n",
      "t=0.5: 850 rows\n",
      "t=0.75: 850 rows\n",
      "t=0.9: 850 rows\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 4 — Split into per-threshold DataFrames =====\n",
    "dfs_by_t = {t: df[df[\"threshold\"] == t].copy() for t in THRESHOLDS}\n",
    "\n",
    "for t in THRESHOLDS:\n",
    "    print(f\"t={t}: {len(dfs_by_t[t])} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "1d96f27c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "      <th>answered_n</th>\n",
       "      <th>total_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.123693</td>\n",
       "      <td>0.675294</td>\n",
       "      <td>-0.087301</td>\n",
       "      <td>0.591765</td>\n",
       "      <td>574</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.133231</td>\n",
       "      <td>0.768235</td>\n",
       "      <td>-0.515431</td>\n",
       "      <td>0.665882</td>\n",
       "      <td>653</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.138739</td>\n",
       "      <td>0.652941</td>\n",
       "      <td>-1.559059</td>\n",
       "      <td>0.562353</td>\n",
       "      <td>555</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.139831</td>\n",
       "      <td>0.555294</td>\n",
       "      <td>-4.221176</td>\n",
       "      <td>0.477647</td>\n",
       "      <td>472</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   threshold  accuracy_at_t  coverage  penalty_mean  overconf_rate  \\\n",
       "0       0.25       0.123693  0.675294     -0.087301       0.591765   \n",
       "1       0.50       0.133231  0.768235     -0.515431       0.665882   \n",
       "2       0.75       0.138739  0.652941     -1.559059       0.562353   \n",
       "3       0.90       0.139831  0.555294     -4.221176       0.477647   \n",
       "\n",
       "   answered_n  total_n  \n",
       "0         574      850  \n",
       "1         653      850  \n",
       "2         555      850  \n",
       "3         472      850  "
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== Cell 5 — Compute metrics per threshold =====\n",
    "metrics_rows = []\n",
    "\n",
    "for t in THRESHOLDS:\n",
    "    df_t = dfs_by_t[t]\n",
    "\n",
    "    metrics_rows.append({\n",
    "        \"threshold\": t,\n",
    "        \"accuracy_at_t\": accuracy_at_threshold(df_t, t),\n",
    "        \"coverage\": coverage(df_t, t),\n",
    "        \"penalty_mean\": penalty_adjusted_mean(df_t, t),\n",
    "        \"overconf_rate\": overconfidence_rate(df_t, t),\n",
    "        \"answered_n\": int((df_t[\"confidence\"] > t).sum()),\n",
    "        \"total_n\": len(df_t)\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_rows).sort_values(\"threshold\").reset_index(drop=True)\n",
    "metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "69297fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4×4 Evaluation Table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.123693</td>\n",
       "      <td>0.675294</td>\n",
       "      <td>-0.087301</td>\n",
       "      <td>0.591765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.133231</td>\n",
       "      <td>0.768235</td>\n",
       "      <td>-0.515431</td>\n",
       "      <td>0.665882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.138739</td>\n",
       "      <td>0.652941</td>\n",
       "      <td>-1.559059</td>\n",
       "      <td>0.562353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>0.139831</td>\n",
       "      <td>0.555294</td>\n",
       "      <td>-4.221176</td>\n",
       "      <td>0.477647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "threshold                                                      \n",
       "0.25            0.123693  0.675294     -0.087301       0.591765\n",
       "0.50            0.133231  0.768235     -0.515431       0.665882\n",
       "0.75            0.138739  0.652941     -1.559059       0.562353\n",
       "0.90            0.139831  0.555294     -4.221176       0.477647"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results in: outputs\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 6 — 4×4 evaluation table =====\n",
    "eval_table = (\n",
    "    metrics_df[[\"threshold\", \"accuracy_at_t\", \"coverage\", \"penalty_mean\", \"overconf_rate\"]]\n",
    "    .set_index(\"threshold\")\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# Display\n",
    "print(\"4×4 Evaluation Table:\")\n",
    "display(eval_table)\n",
    "\n",
    "# Save both detailed and compact tables\n",
    "eval_table.to_csv(OUTPUT_PATH / \"gemini-mmlu-metric-eval.csv\")\n",
    "\n",
    "print(f\"Saved results in: {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b35a66",
   "metadata": {},
   "source": [
    "### Baseline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "bbaa912a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG — eligible rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.123693</td>\n",
       "      <td>0.675294</td>\n",
       "      <td>-0.087301</td>\n",
       "      <td>0.591765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.133231</td>\n",
       "      <td>0.768235</td>\n",
       "      <td>-0.515431</td>\n",
       "      <td>0.665882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.138739</td>\n",
       "      <td>0.652941</td>\n",
       "      <td>-1.559059</td>\n",
       "      <td>0.562353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>0.139831</td>\n",
       "      <td>0.555294</td>\n",
       "      <td>-4.221176</td>\n",
       "      <td>0.477647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "threshold                                                      \n",
       "0.25            0.123693  0.675294     -0.087301       0.591765\n",
       "0.50            0.133231  0.768235     -0.515431       0.665882\n",
       "0.75            0.138739  0.652941     -1.559059       0.562353\n",
       "0.90            0.139831  0.555294     -4.221176       0.477647"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected t* = 0.9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "accuracy_at_t    0.139831\n",
       "coverage         0.555294\n",
       "penalty_mean    -4.221176\n",
       "overconf_rate    0.477647\n",
       "Name: 0.9, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "coverage_floor = 0.3\n",
    "\n",
    "eligible = eval_table[eval_table[\"coverage\"] >= coverage_floor]\n",
    "print(\"DEBUG — eligible rows:\")\n",
    "display(eligible)\n",
    "\n",
    "best_row = eligible.loc[eligible[\"accuracy_at_t\"].idxmax()]\n",
    "t_star = best_row.name\n",
    "\n",
    "print(f\"Selected t* = {t_star}\")\n",
    "display(best_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "8fd7728d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary-grading baseline:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Binary</th>\n",
       "      <td>0.105588</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.894412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "Binary       0.105588       1.0           NaN       0.894412"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Always-abstain baseline:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Abstain</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "Abstain            0.0       0.0           0.0            0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Cell 2 – Compute binary-grading and always-abstain baselines =====\n",
    "import numpy as np\n",
    "\n",
    "# Load the original prediction CSV (same used to compute metrics_df)\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Compute overall accuracy\n",
    "binary_acc = np.mean(df[\"predicted_answer\"] == df[\"answer\"])\n",
    "\n",
    "# Wrong rate (for overconfidence)\n",
    "wrong_rate = 1 - binary_acc\n",
    "\n",
    "# Binary baseline metrics (answers everything)\n",
    "binary_row = {\n",
    "    \"accuracy_at_t\": binary_acc,\n",
    "    \"coverage\": 1.0,\n",
    "    \"penalty_mean\": np.nan,  # you can fill with projected penalty if desired\n",
    "    \"overconf_rate\": wrong_rate,\n",
    "}\n",
    "\n",
    "# Always-abstain baseline metrics\n",
    "abstain_row = {\n",
    "    \"accuracy_at_t\": 0.0,\n",
    "    \"coverage\": 0.0,\n",
    "    \"penalty_mean\": 0.0,\n",
    "    \"overconf_rate\": 0.0,\n",
    "}\n",
    "\n",
    "print(\"Binary-grading baseline:\")\n",
    "display(pd.DataFrame([binary_row], index=[\"Binary\"]))\n",
    "print(\"Always-abstain baseline:\")\n",
    "display(pd.DataFrame([abstain_row], index=[\"Abstain\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "41bc13df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 3x4 Headline Evaluation Table ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Confidence-aware (t*=0.9)</th>\n",
       "      <td>0.139831</td>\n",
       "      <td>0.555294</td>\n",
       "      <td>-4.221176</td>\n",
       "      <td>0.477647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Binary grading</th>\n",
       "      <td>0.105588</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.894412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Always abstain</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           accuracy_at_t  coverage  penalty_mean  \\\n",
       "Confidence-aware (t*=0.9)       0.139831  0.555294     -4.221176   \n",
       "Binary grading                  0.105588  1.000000           NaN   \n",
       "Always abstain                  0.000000  0.000000      0.000000   \n",
       "\n",
       "                           overconf_rate  \n",
       "Confidence-aware (t*=0.9)       0.477647  \n",
       "Binary grading                  0.894412  \n",
       "Always abstain                  0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Cell 3 – Build final 3x4 headline table =====\n",
    "\n",
    "headline_df = pd.DataFrame([\n",
    "    best_row[[\"accuracy_at_t\", \"coverage\", \"penalty_mean\", \"overconf_rate\"]],\n",
    "    pd.Series(binary_row),\n",
    "    pd.Series(abstain_row)\n",
    "], index=[f\"Confidence-aware (t*={t_star})\", \"Binary grading\", \"Always abstain\"])\n",
    "\n",
    "print(\"=== 3x4 Headline Evaluation Table ===\")\n",
    "display(headline_df)\n",
    "headline_df.to_csv(OUTPUT_PATH / \"gemini-mmlu-baseline-eval.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a937e888",
   "metadata": {},
   "source": [
    "## Mistral Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "779f4cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #GPQA - Qwen Evalution \n",
    "# CSV_PATH = Path(\"../inference/outputs/mistral-mmlu.csv\")  # <-- change this for each run\n",
    "# OUTPUT_PATH = Path(\"outputs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "eb63118c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# # Normalize strings & types\n",
    "# df[\"answer\"] = df[\"answer\"].astype(str).str.strip().str.upper()\n",
    "# df[\"predicted_answer\"] = df[\"predicted_answer\"].astype(str).str.strip().str.upper()\n",
    "# df[\"confidence\"] = pd.to_numeric(df[\"confidence\"], errors=\"coerce\").fillna(0.0)\n",
    "# df[\"threshold\"] = pd.to_numeric(df[\"threshold\"], errors=\"coerce\")\n",
    "\n",
    "# print(f\"✅ Loaded {len(df)} rows from {CSV_PATH.name}\")\n",
    "# print(\"Thresholds found:\", sorted(df[\"threshold\"].unique()))\n",
    "# df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "8cd599f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===== Cell 4 — Split into per-threshold DataFrames =====\n",
    "# dfs_by_t = {t: df[df[\"threshold\"] == t].copy() for t in THRESHOLDS}\n",
    "\n",
    "# for t in THRESHOLDS:\n",
    "#     print(f\"t={t}: {len(dfs_by_t[t])} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "62ba2cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===== Cell 5 — Compute metrics per threshold =====\n",
    "# metrics_rows = []\n",
    "\n",
    "# for t in THRESHOLDS:\n",
    "#     df_t = dfs_by_t[t]\n",
    "\n",
    "#     metrics_rows.append({\n",
    "#         \"threshold\": t,\n",
    "#         \"accuracy_at_t\": accuracy_at_threshold(df_t, t),\n",
    "#         \"coverage\": coverage(df_t, t),\n",
    "#         \"penalty_mean\": penalty_adjusted_mean(df_t, t),\n",
    "#         \"overconf_rate\": overconfidence_rate(df_t, t),\n",
    "#         \"answered_n\": int((df_t[\"confidence\"] > t).sum()),\n",
    "#         \"total_n\": len(df_t)\n",
    "#     })\n",
    "\n",
    "# metrics_df = pd.DataFrame(metrics_rows).sort_values(\"threshold\").reset_index(drop=True)\n",
    "# metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "e674e8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===== Cell 6 — 4×4 evaluation table =====\n",
    "# eval_table = (\n",
    "#     metrics_df[[\"threshold\", \"accuracy_at_t\", \"coverage\", \"penalty_mean\", \"overconf_rate\"]]\n",
    "#     .set_index(\"threshold\")\n",
    "#     .sort_index()\n",
    "# )\n",
    "\n",
    "# # Display\n",
    "# print(\"4×4 Evaluation Table:\")\n",
    "# display(eval_table)\n",
    "\n",
    "# # Save both detailed and compact tables\n",
    "# eval_table.to_csv(OUTPUT_PATH / \"mistral-mmlu-metric-eval.csv\")\n",
    "\n",
    "# print(f\"Saved results in: {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b87d790",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "8f7a5e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coverage_floor = 0.3\n",
    "\n",
    "# eligible = eval_table[eval_table[\"coverage\"] >= coverage_floor]\n",
    "# print(\"DEBUG — eligible rows:\")\n",
    "# display(eligible)\n",
    "\n",
    "# best_row = eligible.loc[eligible[\"accuracy_at_t\"].idxmax()]\n",
    "# t_star = best_row.name\n",
    "\n",
    "# print(f\"Selected t* = {t_star}\")\n",
    "# display(best_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "9309b56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===== Cell 2 – Compute binary-grading and always-abstain baselines =====\n",
    "# import numpy as np\n",
    "\n",
    "# # Load the original prediction CSV (same used to compute metrics_df)\n",
    "# df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# # Compute overall accuracy\n",
    "# binary_acc = np.mean(df[\"predicted_answer\"] == df[\"answer\"])\n",
    "\n",
    "# # Wrong rate (for overconfidence)\n",
    "# wrong_rate = 1 - binary_acc\n",
    "\n",
    "# # Binary baseline metrics (answers everything)\n",
    "# binary_row = {\n",
    "#     \"accuracy_at_t\": binary_acc,\n",
    "#     \"coverage\": 1.0,\n",
    "#     \"penalty_mean\": np.nan,  # you can fill with projected penalty if desired\n",
    "#     \"overconf_rate\": wrong_rate,\n",
    "# }\n",
    "\n",
    "# # Always-abstain baseline metrics\n",
    "# abstain_row = {\n",
    "#     \"accuracy_at_t\": 0.0,\n",
    "#     \"coverage\": 0.0,\n",
    "#     \"penalty_mean\": 0.0,\n",
    "#     \"overconf_rate\": 0.0,\n",
    "# }\n",
    "\n",
    "# print(\"Binary-grading baseline:\")\n",
    "# display(pd.DataFrame([binary_row], index=[\"Binary\"]))\n",
    "# print(\"Always-abstain baseline:\")\n",
    "# display(pd.DataFrame([abstain_row], index=[\"Abstain\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "97a529e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===== Cell 3 – Build final 3x4 headline table =====\n",
    "\n",
    "# headline_df = pd.DataFrame([\n",
    "#     best_row[[\"accuracy_at_t\", \"coverage\", \"penalty_mean\", \"overconf_rate\"]],\n",
    "#     pd.Series(binary_row),\n",
    "#     pd.Series(abstain_row)\n",
    "# ], index=[f\"Confidence-aware (t*={t_star})\", \"Binary grading\", \"Always abstain\"])\n",
    "\n",
    "# print(\"=== 3x4 Headline Evaluation Table ===\")\n",
    "# display(headline_df)\n",
    "# headline_df.to_csv(OUTPUT_PATH / \"mistral-mmlu-baseline-eval.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
