{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9de9656",
   "metadata": {},
   "source": [
    "## Imports, Setup, Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab2496e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Common thresholds (used for all model–dataset combinations)\n",
    "THRESHOLDS = [0.25, 0.5, 0.75, 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9750d8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 2 — Metric functions =====\n",
    "\n",
    "def decide(p: float, t: float) -> bool:\n",
    "    \"\"\"\n",
    "    Decision rule: returns True if the model 'answers' the question.\n",
    "    We assume the model only answers when its confidence p exceeds the threshold t.\n",
    "    \"\"\"\n",
    "    return p > t\n",
    "\n",
    "\n",
    "def penalty_score(pred: str, gold: str, p: float, t: float) -> float:\n",
    "    \"\"\"\n",
    "    Penalty-adjusted score (as defined in the paper/proposal):\n",
    "      - If p <= t  → the model abstains → score = 0.\n",
    "      - If p > t and prediction == gold → score = +1.\n",
    "      - If p > t and prediction != gold → score = - (p * t) / (1 - t)\n",
    "        (this is a confidence-proportional penalty for being overconfident).\n",
    "    \"\"\"\n",
    "    if p <= t:\n",
    "        return 0.0\n",
    "    if pred == gold:\n",
    "        return 1.0\n",
    "    return - (p * t) / (1.0 - t)\n",
    "\n",
    "\n",
    "#Metric 1\n",
    "def accuracy_at_threshold(df: pd.DataFrame, t: float) -> float:\n",
    "    \"\"\"\n",
    "    Accuracy@t:\n",
    "      Fraction of *answered* questions that are correct.\n",
    "      = (# correct with p>t) / (# answered with p>t)\n",
    "    If the model abstains on all (no p>t), returns 0.0.\n",
    "    \"\"\"\n",
    "    answered = df[\"confidence\"] > t\n",
    "    answered_n = answered.sum()\n",
    "    if answered_n == 0:\n",
    "        return 0.0\n",
    "    correct = (df[\"predicted_answer\"] == df[\"answer\"]) & answered\n",
    "    return float(correct.sum() / answered_n)\n",
    "\n",
    "#Metric 2\n",
    "def coverage(df: pd.DataFrame, t: float) -> float:\n",
    "    \"\"\"\n",
    "    Coverage:\n",
    "      Fraction of total questions that the model *answers*.\n",
    "      = (# p>t) / total\n",
    "    \"\"\"\n",
    "    if len(df) == 0:\n",
    "        return 0.0\n",
    "    return float((df[\"confidence\"] > t).sum() / len(df))\n",
    "\n",
    "#Metric 3\n",
    "def penalty_adjusted_mean(df: pd.DataFrame, t: float) -> float:\n",
    "    \"\"\"\n",
    "    Mean penalty-adjusted score across all rows (including abstains).\n",
    "    Abstentions contribute 0.\n",
    "    \"\"\"\n",
    "    scores = [\n",
    "        penalty_score(r.predicted_answer, r.answer, float(r.confidence), t)\n",
    "        for r in df.itertuples(index=False)\n",
    "    ]\n",
    "    return float(np.mean(scores)) if scores else 0.0\n",
    "\n",
    "#Metric 4\n",
    "def overconfidence_rate(df: pd.DataFrame, t: float) -> float:\n",
    "    \"\"\"\n",
    "    Overconfidence rate:\n",
    "      Fraction of questions where the model is *wrong* but still confident (p>t).\n",
    "      = (# wrong & p>t) / total\n",
    "    \"\"\"\n",
    "    if len(df) == 0:\n",
    "        return 0.0\n",
    "    mask = (df[\"predicted_answer\"] != df[\"answer\"]) & (df[\"confidence\"] > t)\n",
    "    return float(mask.sum() / len(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae96d5f1",
   "metadata": {},
   "source": [
    "## Qwen Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb006450",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = Path(\"../inference/outputs/qwen-mmlu.csv\")  # <-- change this for each run\n",
    "OUTPUT_PATH = Path(\"outputs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c99043c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 3400 rows from qwen-mmlu.csv\n",
      "Thresholds found: [np.float64(0.25), np.float64(0.5), np.float64(0.75), np.float64(0.9)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>threshold</th>\n",
       "      <th>question</th>\n",
       "      <th>choices</th>\n",
       "      <th>answer</th>\n",
       "      <th>predicted_answer</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>596</td>\n",
       "      <td>0.25</td>\n",
       "      <td>A manufacturer is currently selling 2000 units...</td>\n",
       "      <td>['$2.50' '$1.90' '$2.70' '$2.60' '$1.80' '$2.2...</td>\n",
       "      <td>F</td>\n",
       "      <td>IDK</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>814</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Fred Lowes is a typewriter salesman. He receiv...</td>\n",
       "      <td>['$210' '$200' '$225' '$175' '$195' '$150' '$2...</td>\n",
       "      <td>I</td>\n",
       "      <td>B</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>817</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Mary Redmond purchased a $28,500 home with 20%...</td>\n",
       "      <td>['$305' '$190' '$171' '$285.50' '$399' '$323' ...</td>\n",
       "      <td>F</td>\n",
       "      <td>IDK</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  threshold                                           question  \\\n",
       "0  596       0.25  A manufacturer is currently selling 2000 units...   \n",
       "1  814       0.25  Fred Lowes is a typewriter salesman. He receiv...   \n",
       "2  817       0.25  Mary Redmond purchased a $28,500 home with 20%...   \n",
       "\n",
       "                                             choices answer predicted_answer  \\\n",
       "0  ['$2.50' '$1.90' '$2.70' '$2.60' '$1.80' '$2.2...      F              IDK   \n",
       "1  ['$210' '$200' '$225' '$175' '$195' '$150' '$2...      I                B   \n",
       "2  ['$305' '$190' '$171' '$285.50' '$399' '$323' ...      F              IDK   \n",
       "\n",
       "   confidence  \n",
       "0    0.500000  \n",
       "1    0.833333  \n",
       "2    1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Normalize strings & types\n",
    "df[\"answer\"] = df[\"answer\"].astype(str).str.strip().str.upper()\n",
    "df[\"predicted_answer\"] = df[\"predicted_answer\"].astype(str).str.strip().str.upper()\n",
    "df[\"confidence\"] = pd.to_numeric(df[\"confidence\"], errors=\"coerce\").fillna(0.0)\n",
    "df[\"threshold\"] = pd.to_numeric(df[\"threshold\"], errors=\"coerce\")\n",
    "\n",
    "print(f\"✅ Loaded {len(df)} rows from {CSV_PATH.name}\")\n",
    "print(\"Thresholds found:\", sorted(df[\"threshold\"].unique()))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "059deb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=0.25: 850 rows\n",
      "t=0.5: 850 rows\n",
      "t=0.75: 850 rows\n",
      "t=0.9: 850 rows\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 4 — Split into per-threshold DataFrames =====\n",
    "dfs_by_t = {t: df[df[\"threshold\"] == t].copy() for t in THRESHOLDS}\n",
    "\n",
    "for t in THRESHOLDS:\n",
    "    print(f\"t={t}: {len(dfs_by_t[t])} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c25d465a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "      <th>answered_n</th>\n",
       "      <th>total_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.079710</td>\n",
       "      <td>0.974118</td>\n",
       "      <td>-0.170190</td>\n",
       "      <td>0.896471</td>\n",
       "      <td>828</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.094183</td>\n",
       "      <td>0.849412</td>\n",
       "      <td>-0.601471</td>\n",
       "      <td>0.769412</td>\n",
       "      <td>722</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.081481</td>\n",
       "      <td>0.635294</td>\n",
       "      <td>-1.636941</td>\n",
       "      <td>0.583529</td>\n",
       "      <td>540</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.081013</td>\n",
       "      <td>0.464706</td>\n",
       "      <td>-3.805882</td>\n",
       "      <td>0.427059</td>\n",
       "      <td>395</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   threshold  accuracy_at_t  coverage  penalty_mean  overconf_rate  \\\n",
       "0       0.25       0.079710  0.974118     -0.170190       0.896471   \n",
       "1       0.50       0.094183  0.849412     -0.601471       0.769412   \n",
       "2       0.75       0.081481  0.635294     -1.636941       0.583529   \n",
       "3       0.90       0.081013  0.464706     -3.805882       0.427059   \n",
       "\n",
       "   answered_n  total_n  \n",
       "0         828      850  \n",
       "1         722      850  \n",
       "2         540      850  \n",
       "3         395      850  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== Cell 5 — Compute metrics per threshold =====\n",
    "metrics_rows = []\n",
    "\n",
    "for t in THRESHOLDS:\n",
    "    df_t = dfs_by_t[t]\n",
    "\n",
    "    metrics_rows.append({\n",
    "        \"threshold\": t,\n",
    "        \"accuracy_at_t\": accuracy_at_threshold(df_t, t),\n",
    "        \"coverage\": coverage(df_t, t),\n",
    "        \"penalty_mean\": penalty_adjusted_mean(df_t, t),\n",
    "        \"overconf_rate\": overconfidence_rate(df_t, t),\n",
    "        \"answered_n\": int((df_t[\"confidence\"] > t).sum()),\n",
    "        \"total_n\": len(df_t)\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_rows).sort_values(\"threshold\").reset_index(drop=True)\n",
    "metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7360cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4×4 Evaluation Table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.079710</td>\n",
       "      <td>0.974118</td>\n",
       "      <td>-0.170190</td>\n",
       "      <td>0.896471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.094183</td>\n",
       "      <td>0.849412</td>\n",
       "      <td>-0.601471</td>\n",
       "      <td>0.769412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.081481</td>\n",
       "      <td>0.635294</td>\n",
       "      <td>-1.636941</td>\n",
       "      <td>0.583529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>0.081013</td>\n",
       "      <td>0.464706</td>\n",
       "      <td>-3.805882</td>\n",
       "      <td>0.427059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "threshold                                                      \n",
       "0.25            0.079710  0.974118     -0.170190       0.896471\n",
       "0.50            0.094183  0.849412     -0.601471       0.769412\n",
       "0.75            0.081481  0.635294     -1.636941       0.583529\n",
       "0.90            0.081013  0.464706     -3.805882       0.427059"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results in: outputs\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 6 — 4×4 evaluation table =====\n",
    "eval_table = (\n",
    "    metrics_df[[\"threshold\", \"accuracy_at_t\", \"coverage\", \"penalty_mean\", \"overconf_rate\"]]\n",
    "    .set_index(\"threshold\")\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# Display\n",
    "print(\"4×4 Evaluation Table:\")\n",
    "display(eval_table)\n",
    "\n",
    "# Save both detailed and compact tables\n",
    "eval_table.to_csv(OUTPUT_PATH / \"qwen-mmlu-metric-eval.csv\")\n",
    "\n",
    "print(f\"Saved results in: {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6e8a28",
   "metadata": {},
   "source": [
    "### Baseline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6757486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG — eligible rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.079710</td>\n",
       "      <td>0.974118</td>\n",
       "      <td>-0.170190</td>\n",
       "      <td>0.896471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.094183</td>\n",
       "      <td>0.849412</td>\n",
       "      <td>-0.601471</td>\n",
       "      <td>0.769412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.081481</td>\n",
       "      <td>0.635294</td>\n",
       "      <td>-1.636941</td>\n",
       "      <td>0.583529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>0.081013</td>\n",
       "      <td>0.464706</td>\n",
       "      <td>-3.805882</td>\n",
       "      <td>0.427059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "threshold                                                      \n",
       "0.25            0.079710  0.974118     -0.170190       0.896471\n",
       "0.50            0.094183  0.849412     -0.601471       0.769412\n",
       "0.75            0.081481  0.635294     -1.636941       0.583529\n",
       "0.90            0.081013  0.464706     -3.805882       0.427059"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected t* = 0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "accuracy_at_t    0.094183\n",
       "coverage         0.849412\n",
       "penalty_mean    -0.601471\n",
       "overconf_rate    0.769412\n",
       "Name: 0.5, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "coverage_floor = 0.3\n",
    "\n",
    "eligible = eval_table[eval_table[\"coverage\"] >= coverage_floor]\n",
    "print(\"DEBUG — eligible rows:\")\n",
    "display(eligible)\n",
    "\n",
    "best_row = eligible.loc[eligible[\"accuracy_at_t\"].idxmax()]\n",
    "t_star = best_row.name\n",
    "\n",
    "print(f\"Selected t* = {t_star}\")\n",
    "display(best_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e278d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary-grading baseline:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Binary</th>\n",
       "      <td>0.078824</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.921176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "Binary       0.078824       1.0           NaN       0.921176"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Always-abstain baseline:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Abstain</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "Abstain            0.0       0.0           0.0            0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Cell 2 – Compute binary-grading and always-abstain baselines =====\n",
    "import numpy as np\n",
    "\n",
    "# Load the original prediction CSV (same used to compute metrics_df)\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Compute overall accuracy\n",
    "binary_acc = np.mean(df[\"predicted_answer\"] == df[\"answer\"])\n",
    "\n",
    "# Wrong rate (for overconfidence)\n",
    "wrong_rate = 1 - binary_acc\n",
    "\n",
    "# Binary baseline metrics (answers everything)\n",
    "binary_row = {\n",
    "    \"accuracy_at_t\": binary_acc,\n",
    "    \"coverage\": 1.0,\n",
    "    \"penalty_mean\": np.nan,  # you can fill with projected penalty if desired\n",
    "    \"overconf_rate\": wrong_rate,\n",
    "}\n",
    "\n",
    "# Always-abstain baseline metrics\n",
    "abstain_row = {\n",
    "    \"accuracy_at_t\": 0.0,\n",
    "    \"coverage\": 0.0,\n",
    "    \"penalty_mean\": 0.0,\n",
    "    \"overconf_rate\": 0.0,\n",
    "}\n",
    "\n",
    "print(\"Binary-grading baseline:\")\n",
    "display(pd.DataFrame([binary_row], index=[\"Binary\"]))\n",
    "print(\"Always-abstain baseline:\")\n",
    "display(pd.DataFrame([abstain_row], index=[\"Abstain\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb6d0541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 3x4 Headline Evaluation Table ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Confidence-aware (t*=0.5)</th>\n",
       "      <td>0.094183</td>\n",
       "      <td>0.849412</td>\n",
       "      <td>-0.601471</td>\n",
       "      <td>0.769412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Binary grading</th>\n",
       "      <td>0.078824</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.921176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Always abstain</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           accuracy_at_t  coverage  penalty_mean  \\\n",
       "Confidence-aware (t*=0.5)       0.094183  0.849412     -0.601471   \n",
       "Binary grading                  0.078824  1.000000           NaN   \n",
       "Always abstain                  0.000000  0.000000      0.000000   \n",
       "\n",
       "                           overconf_rate  \n",
       "Confidence-aware (t*=0.5)       0.769412  \n",
       "Binary grading                  0.921176  \n",
       "Always abstain                  0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Cell 3 – Build final 3x4 headline table =====\n",
    "\n",
    "headline_df = pd.DataFrame([\n",
    "    best_row[[\"accuracy_at_t\", \"coverage\", \"penalty_mean\", \"overconf_rate\"]],\n",
    "    pd.Series(binary_row),\n",
    "    pd.Series(abstain_row)\n",
    "], index=[f\"Confidence-aware (t*={t_star})\", \"Binary grading\", \"Always abstain\"])\n",
    "\n",
    "print(\"=== 3x4 Headline Evaluation Table ===\")\n",
    "display(headline_df)\n",
    "headline_df.to_csv(OUTPUT_PATH / \"qwen-mmlu-baseline-eval.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bab1ca",
   "metadata": {},
   "source": [
    "## GPT Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2e305c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = Path(\"../inference/outputs/gpt-mmlu.csv\")  # <-- change this for each run\n",
    "OUTPUT_PATH = Path(\"outputs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f67a564c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 3400 rows from gpt-mmlu.csv\n",
      "Thresholds found: [np.float64(0.25), np.float64(0.5), np.float64(0.75), np.float64(0.9)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>threshold</th>\n",
       "      <th>question</th>\n",
       "      <th>choices</th>\n",
       "      <th>answer</th>\n",
       "      <th>predicted_answer</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>596</td>\n",
       "      <td>0.25</td>\n",
       "      <td>A manufacturer is currently selling 2000 units...</td>\n",
       "      <td>['$2.50' '$1.90' '$2.70' '$2.60' '$1.80' '$2.2...</td>\n",
       "      <td>F</td>\n",
       "      <td>D</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>814</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Fred Lowes is a typewriter salesman. He receiv...</td>\n",
       "      <td>['$210' '$200' '$225' '$175' '$195' '$150' '$2...</td>\n",
       "      <td>I</td>\n",
       "      <td>B</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>817</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Mary Redmond purchased a $28,500 home with 20%...</td>\n",
       "      <td>['$305' '$190' '$171' '$285.50' '$399' '$323' ...</td>\n",
       "      <td>F</td>\n",
       "      <td>C</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  threshold                                           question  \\\n",
       "0  596       0.25  A manufacturer is currently selling 2000 units...   \n",
       "1  814       0.25  Fred Lowes is a typewriter salesman. He receiv...   \n",
       "2  817       0.25  Mary Redmond purchased a $28,500 home with 20%...   \n",
       "\n",
       "                                             choices answer predicted_answer  \\\n",
       "0  ['$2.50' '$1.90' '$2.70' '$2.60' '$1.80' '$2.2...      F                D   \n",
       "1  ['$210' '$200' '$225' '$175' '$195' '$150' '$2...      I                B   \n",
       "2  ['$305' '$190' '$171' '$285.50' '$399' '$323' ...      F                C   \n",
       "\n",
       "   confidence  \n",
       "0    0.333333  \n",
       "1    0.666667  \n",
       "2    0.500000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Normalize strings & types\n",
    "df[\"answer\"] = df[\"answer\"].astype(str).str.strip().str.upper()\n",
    "df[\"predicted_answer\"] = df[\"predicted_answer\"].astype(str).str.strip().str.upper()\n",
    "df[\"confidence\"] = pd.to_numeric(df[\"confidence\"], errors=\"coerce\").fillna(0.0)\n",
    "df[\"threshold\"] = pd.to_numeric(df[\"threshold\"], errors=\"coerce\")\n",
    "\n",
    "print(f\"✅ Loaded {len(df)} rows from {CSV_PATH.name}\")\n",
    "print(\"Thresholds found:\", sorted(df[\"threshold\"].unique()))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9052885d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=0.25: 850 rows\n",
      "t=0.5: 850 rows\n",
      "t=0.75: 850 rows\n",
      "t=0.9: 850 rows\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 4 — Split into per-threshold DataFrames =====\n",
    "dfs_by_t = {t: df[df[\"threshold\"] == t].copy() for t in THRESHOLDS}\n",
    "\n",
    "for t in THRESHOLDS:\n",
    "    print(f\"t={t}: {len(dfs_by_t[t])} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "056e243e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "      <th>answered_n</th>\n",
       "      <th>total_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.197647</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.011542</td>\n",
       "      <td>0.802353</td>\n",
       "      <td>850</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.208517</td>\n",
       "      <td>0.801176</td>\n",
       "      <td>-0.386275</td>\n",
       "      <td>0.634118</td>\n",
       "      <td>681</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.248000</td>\n",
       "      <td>0.441176</td>\n",
       "      <td>-0.834118</td>\n",
       "      <td>0.331765</td>\n",
       "      <td>375</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   threshold  accuracy_at_t  coverage  penalty_mean  overconf_rate  \\\n",
       "0       0.25       0.197647  1.000000     -0.011542       0.802353   \n",
       "1       0.50       0.208517  0.801176     -0.386275       0.634118   \n",
       "2       0.75       0.248000  0.441176     -0.834118       0.331765   \n",
       "3       0.90       0.000000  0.000000      0.000000       0.000000   \n",
       "\n",
       "   answered_n  total_n  \n",
       "0         850      850  \n",
       "1         681      850  \n",
       "2         375      850  \n",
       "3           0      850  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== Cell 5 — Compute metrics per threshold =====\n",
    "metrics_rows = []\n",
    "\n",
    "for t in THRESHOLDS:\n",
    "    df_t = dfs_by_t[t]\n",
    "\n",
    "    metrics_rows.append({\n",
    "        \"threshold\": t,\n",
    "        \"accuracy_at_t\": accuracy_at_threshold(df_t, t),\n",
    "        \"coverage\": coverage(df_t, t),\n",
    "        \"penalty_mean\": penalty_adjusted_mean(df_t, t),\n",
    "        \"overconf_rate\": overconfidence_rate(df_t, t),\n",
    "        \"answered_n\": int((df_t[\"confidence\"] > t).sum()),\n",
    "        \"total_n\": len(df_t)\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_rows).sort_values(\"threshold\").reset_index(drop=True)\n",
    "metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd275867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4×4 Evaluation Table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.197647</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.011542</td>\n",
       "      <td>0.802353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.208517</td>\n",
       "      <td>0.801176</td>\n",
       "      <td>-0.386275</td>\n",
       "      <td>0.634118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.248000</td>\n",
       "      <td>0.441176</td>\n",
       "      <td>-0.834118</td>\n",
       "      <td>0.331765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "threshold                                                      \n",
       "0.25            0.197647  1.000000     -0.011542       0.802353\n",
       "0.50            0.208517  0.801176     -0.386275       0.634118\n",
       "0.75            0.248000  0.441176     -0.834118       0.331765\n",
       "0.90            0.000000  0.000000      0.000000       0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results in: outputs\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 6 — 4×4 evaluation table =====\n",
    "eval_table = (\n",
    "    metrics_df[[\"threshold\", \"accuracy_at_t\", \"coverage\", \"penalty_mean\", \"overconf_rate\"]]\n",
    "    .set_index(\"threshold\")\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# Display\n",
    "print(\"4×4 Evaluation Table:\")\n",
    "display(eval_table)\n",
    "\n",
    "# Save both detailed and compact tables\n",
    "eval_table.to_csv(OUTPUT_PATH / \"gpt-mmlu-metric-eval.csv\")\n",
    "\n",
    "print(f\"Saved results in: {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37f5925",
   "metadata": {},
   "source": [
    "### Baseline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "709cdde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG — eligible rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.197647</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.011542</td>\n",
       "      <td>0.802353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.208517</td>\n",
       "      <td>0.801176</td>\n",
       "      <td>-0.386275</td>\n",
       "      <td>0.634118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.248000</td>\n",
       "      <td>0.441176</td>\n",
       "      <td>-0.834118</td>\n",
       "      <td>0.331765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "threshold                                                      \n",
       "0.25            0.197647  1.000000     -0.011542       0.802353\n",
       "0.50            0.208517  0.801176     -0.386275       0.634118\n",
       "0.75            0.248000  0.441176     -0.834118       0.331765"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected t* = 0.75\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "accuracy_at_t    0.248000\n",
       "coverage         0.441176\n",
       "penalty_mean    -0.834118\n",
       "overconf_rate    0.331765\n",
       "Name: 0.75, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "coverage_floor = 0.3\n",
    "\n",
    "eligible = eval_table[eval_table[\"coverage\"] >= coverage_floor]\n",
    "print(\"DEBUG — eligible rows:\")\n",
    "display(eligible)\n",
    "\n",
    "best_row = eligible.loc[eligible[\"accuracy_at_t\"].idxmax()]\n",
    "t_star = best_row.name\n",
    "\n",
    "print(f\"Selected t* = {t_star}\")\n",
    "display(best_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "290126c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary-grading baseline:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Binary</th>\n",
       "      <td>0.134118</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.865882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "Binary       0.134118       1.0           NaN       0.865882"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Always-abstain baseline:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Abstain</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "Abstain            0.0       0.0           0.0            0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Cell 2 – Compute binary-grading and always-abstain baselines =====\n",
    "import numpy as np\n",
    "\n",
    "# Load the original prediction CSV (same used to compute metrics_df)\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Compute overall accuracy\n",
    "binary_acc = np.mean(df[\"predicted_answer\"] == df[\"answer\"])\n",
    "\n",
    "# Wrong rate (for overconfidence)\n",
    "wrong_rate = 1 - binary_acc\n",
    "\n",
    "# Binary baseline metrics (answers everything)\n",
    "binary_row = {\n",
    "    \"accuracy_at_t\": binary_acc,\n",
    "    \"coverage\": 1.0,\n",
    "    \"penalty_mean\": np.nan,  # you can fill with projected penalty if desired\n",
    "    \"overconf_rate\": wrong_rate,\n",
    "}\n",
    "\n",
    "# Always-abstain baseline metrics\n",
    "abstain_row = {\n",
    "    \"accuracy_at_t\": 0.0,\n",
    "    \"coverage\": 0.0,\n",
    "    \"penalty_mean\": 0.0,\n",
    "    \"overconf_rate\": 0.0,\n",
    "}\n",
    "\n",
    "print(\"Binary-grading baseline:\")\n",
    "display(pd.DataFrame([binary_row], index=[\"Binary\"]))\n",
    "print(\"Always-abstain baseline:\")\n",
    "display(pd.DataFrame([abstain_row], index=[\"Abstain\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8b39a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 3x4 Headline Evaluation Table ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Confidence-aware (t*=0.75)</th>\n",
       "      <td>0.248000</td>\n",
       "      <td>0.441176</td>\n",
       "      <td>-0.834118</td>\n",
       "      <td>0.331765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Binary grading</th>\n",
       "      <td>0.134118</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.865882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Always abstain</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            accuracy_at_t  coverage  penalty_mean  \\\n",
       "Confidence-aware (t*=0.75)       0.248000  0.441176     -0.834118   \n",
       "Binary grading                   0.134118  1.000000           NaN   \n",
       "Always abstain                   0.000000  0.000000      0.000000   \n",
       "\n",
       "                            overconf_rate  \n",
       "Confidence-aware (t*=0.75)       0.331765  \n",
       "Binary grading                   0.865882  \n",
       "Always abstain                   0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Cell 3 – Build final 3x4 headline table =====\n",
    "\n",
    "headline_df = pd.DataFrame([\n",
    "    best_row[[\"accuracy_at_t\", \"coverage\", \"penalty_mean\", \"overconf_rate\"]],\n",
    "    pd.Series(binary_row),\n",
    "    pd.Series(abstain_row)\n",
    "], index=[f\"Confidence-aware (t*={t_star})\", \"Binary grading\", \"Always abstain\"])\n",
    "\n",
    "print(\"=== 3x4 Headline Evaluation Table ===\")\n",
    "display(headline_df)\n",
    "headline_df.to_csv(OUTPUT_PATH / \"gpt-mmlu-baseline-eval.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99be7c33",
   "metadata": {},
   "source": [
    "## Claude Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0059b7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPQA - Qwen Evalution \n",
    "CSV_PATH = Path(\"../inference/outputs/claude-mmlu.csv\")  # <-- change this for each run\n",
    "OUTPUT_PATH = Path(\"outputs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56bd997c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 3400 rows from claude-mmlu.csv\n",
      "Thresholds found: [np.float64(0.25), np.float64(0.5), np.float64(0.75), np.float64(0.9)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>threshold</th>\n",
       "      <th>question</th>\n",
       "      <th>choices</th>\n",
       "      <th>answer</th>\n",
       "      <th>predicted_answer</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>596</td>\n",
       "      <td>0.25</td>\n",
       "      <td>A manufacturer is currently selling 2000 units...</td>\n",
       "      <td>['$2.50' '$1.90' '$2.70' '$2.60' '$1.80' '$2.2...</td>\n",
       "      <td>F</td>\n",
       "      <td>I</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>814</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Fred Lowes is a typewriter salesman. He receiv...</td>\n",
       "      <td>['$210' '$200' '$225' '$175' '$195' '$150' '$2...</td>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>817</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Mary Redmond purchased a $28,500 home with 20%...</td>\n",
       "      <td>['$305' '$190' '$171' '$285.50' '$399' '$323' ...</td>\n",
       "      <td>F</td>\n",
       "      <td>I</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  threshold                                           question  \\\n",
       "0  596       0.25  A manufacturer is currently selling 2000 units...   \n",
       "1  814       0.25  Fred Lowes is a typewriter salesman. He receiv...   \n",
       "2  817       0.25  Mary Redmond purchased a $28,500 home with 20%...   \n",
       "\n",
       "                                             choices answer predicted_answer  \\\n",
       "0  ['$2.50' '$1.90' '$2.70' '$2.60' '$1.80' '$2.2...      F                I   \n",
       "1  ['$210' '$200' '$225' '$175' '$195' '$150' '$2...      I                I   \n",
       "2  ['$305' '$190' '$171' '$285.50' '$399' '$323' ...      F                I   \n",
       "\n",
       "   confidence  \n",
       "0         1.0  \n",
       "1         1.0  \n",
       "2         1.0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Normalize strings & types\n",
    "df[\"answer\"] = df[\"answer\"].astype(str).str.strip().str.upper()\n",
    "df[\"predicted_answer\"] = df[\"predicted_answer\"].astype(str).str.strip().str.upper()\n",
    "df[\"confidence\"] = pd.to_numeric(df[\"confidence\"], errors=\"coerce\").fillna(0.0)\n",
    "df[\"threshold\"] = pd.to_numeric(df[\"threshold\"], errors=\"coerce\")\n",
    "\n",
    "print(f\"✅ Loaded {len(df)} rows from {CSV_PATH.name}\")\n",
    "print(\"Thresholds found:\", sorted(df[\"threshold\"].unique()))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc7688f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=0.25: 850 rows\n",
      "t=0.5: 850 rows\n",
      "t=0.75: 850 rows\n",
      "t=0.9: 850 rows\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 4 — Split into per-threshold DataFrames =====\n",
    "dfs_by_t = {t: df[df[\"threshold\"] == t].copy() for t in THRESHOLDS}\n",
    "\n",
    "for t in THRESHOLDS:\n",
    "    print(f\"t={t}: {len(dfs_by_t[t])} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c25f754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 5 — Compute metrics per threshold =====\n",
    "metrics_rows = []\n",
    "\n",
    "for t in THRESHOLDS:\n",
    "    df_t = dfs_by_t[t]\n",
    "\n",
    "    metrics_rows.append({\n",
    "        \"threshold\": t,\n",
    "        \"accuracy_at_t\": accuracy_at_threshold(df_t, t),\n",
    "        \"coverage\": coverage(df_t, t),\n",
    "        \"penalty_mean\": penalty_adjusted_mean(df_t, t),\n",
    "        \"overconf_rate\": overconfidence_rate(df_t, t),\n",
    "        \"answered_n\": int((df_t[\"confidence\"] > t).sum()),\n",
    "        \"total_n\": len(df_t)\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_rows).sort_values(\"threshold\").reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92a93fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4×4 Evaluation Table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.134454</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>-0.133105</td>\n",
       "      <td>0.848235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.137107</td>\n",
       "      <td>0.935294</td>\n",
       "      <td>-0.642902</td>\n",
       "      <td>0.807059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>-2.230353</td>\n",
       "      <td>0.794118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>0.110957</td>\n",
       "      <td>0.848235</td>\n",
       "      <td>-6.692941</td>\n",
       "      <td>0.754118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "threshold                                                      \n",
       "0.25            0.134454  0.980000     -0.133105       0.848235\n",
       "0.50            0.137107  0.935294     -0.642902       0.807059\n",
       "0.75            0.129032  0.911765     -2.230353       0.794118\n",
       "0.90            0.110957  0.848235     -6.692941       0.754118"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results in: outputs\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 6 — 4×4 evaluation table =====\n",
    "eval_table = (\n",
    "    metrics_df[[\"threshold\", \"accuracy_at_t\", \"coverage\", \"penalty_mean\", \"overconf_rate\"]]\n",
    "    .set_index(\"threshold\")\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# Display\n",
    "print(\"4×4 Evaluation Table:\")\n",
    "display(eval_table)\n",
    "\n",
    "# Save both detailed and compact tables\n",
    "eval_table.to_csv(OUTPUT_PATH / \"claude-mmlu-metric-eval.csv\")\n",
    "\n",
    "print(f\"Saved results in: {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1742c3",
   "metadata": {},
   "source": [
    "### Baseline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40cddaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG — eligible rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.134454</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>-0.133105</td>\n",
       "      <td>0.848235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.137107</td>\n",
       "      <td>0.935294</td>\n",
       "      <td>-0.642902</td>\n",
       "      <td>0.807059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>-2.230353</td>\n",
       "      <td>0.794118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>0.110957</td>\n",
       "      <td>0.848235</td>\n",
       "      <td>-6.692941</td>\n",
       "      <td>0.754118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "threshold                                                      \n",
       "0.25            0.134454  0.980000     -0.133105       0.848235\n",
       "0.50            0.137107  0.935294     -0.642902       0.807059\n",
       "0.75            0.129032  0.911765     -2.230353       0.794118\n",
       "0.90            0.110957  0.848235     -6.692941       0.754118"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected t* = 0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "accuracy_at_t    0.137107\n",
       "coverage         0.935294\n",
       "penalty_mean    -0.642902\n",
       "overconf_rate    0.807059\n",
       "Name: 0.5, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "coverage_floor = 0.3\n",
    "\n",
    "eligible = eval_table[eval_table[\"coverage\"] >= coverage_floor]\n",
    "print(\"DEBUG — eligible rows:\")\n",
    "display(eligible)\n",
    "\n",
    "best_row = eligible.loc[eligible[\"accuracy_at_t\"].idxmax()]\n",
    "t_star = best_row.name\n",
    "\n",
    "print(f\"Selected t* = {t_star}\")\n",
    "display(best_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9634128b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary-grading baseline:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Binary</th>\n",
       "      <td>0.127647</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.872353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "Binary       0.127647       1.0           NaN       0.872353"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Always-abstain baseline:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Abstain</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "Abstain            0.0       0.0           0.0            0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Cell 2 – Compute binary-grading and always-abstain baselines =====\n",
    "import numpy as np\n",
    "\n",
    "# Load the original prediction CSV (same used to compute metrics_df)\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Compute overall accuracy\n",
    "binary_acc = np.mean(df[\"predicted_answer\"] == df[\"answer\"])\n",
    "\n",
    "# Wrong rate (for overconfidence)\n",
    "wrong_rate = 1 - binary_acc\n",
    "\n",
    "# Binary baseline metrics (answers everything)\n",
    "binary_row = {\n",
    "    \"accuracy_at_t\": binary_acc,\n",
    "    \"coverage\": 1.0,\n",
    "    \"penalty_mean\": np.nan,  # you can fill with projected penalty if desired\n",
    "    \"overconf_rate\": wrong_rate,\n",
    "}\n",
    "\n",
    "# Always-abstain baseline metrics\n",
    "abstain_row = {\n",
    "    \"accuracy_at_t\": 0.0,\n",
    "    \"coverage\": 0.0,\n",
    "    \"penalty_mean\": 0.0,\n",
    "    \"overconf_rate\": 0.0,\n",
    "}\n",
    "\n",
    "print(\"Binary-grading baseline:\")\n",
    "display(pd.DataFrame([binary_row], index=[\"Binary\"]))\n",
    "print(\"Always-abstain baseline:\")\n",
    "display(pd.DataFrame([abstain_row], index=[\"Abstain\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "098a62f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 3x4 Headline Evaluation Table ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Confidence-aware (t*=0.5)</th>\n",
       "      <td>0.137107</td>\n",
       "      <td>0.935294</td>\n",
       "      <td>-0.642902</td>\n",
       "      <td>0.807059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Binary grading</th>\n",
       "      <td>0.127647</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.872353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Always abstain</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           accuracy_at_t  coverage  penalty_mean  \\\n",
       "Confidence-aware (t*=0.5)       0.137107  0.935294     -0.642902   \n",
       "Binary grading                  0.127647  1.000000           NaN   \n",
       "Always abstain                  0.000000  0.000000      0.000000   \n",
       "\n",
       "                           overconf_rate  \n",
       "Confidence-aware (t*=0.5)       0.807059  \n",
       "Binary grading                  0.872353  \n",
       "Always abstain                  0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Cell 3 – Build final 3x4 headline table =====\n",
    "\n",
    "headline_df = pd.DataFrame([\n",
    "    best_row[[\"accuracy_at_t\", \"coverage\", \"penalty_mean\", \"overconf_rate\"]],\n",
    "    pd.Series(binary_row),\n",
    "    pd.Series(abstain_row)\n",
    "], index=[f\"Confidence-aware (t*={t_star})\", \"Binary grading\", \"Always abstain\"])\n",
    "\n",
    "print(\"=== 3x4 Headline Evaluation Table ===\")\n",
    "display(headline_df)\n",
    "headline_df.to_csv(OUTPUT_PATH / \"claude-mmlu-baseline-eval.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe2d7bb",
   "metadata": {},
   "source": [
    "## LLama Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eaba8e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPQA - Qwen Evalution \n",
    "CSV_PATH = Path(\"../inference/outputs/llama-mmlu.csv\")  # <-- change this for each run\n",
    "OUTPUT_PATH = Path(\"outputs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b089aa3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'predicted_answer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/llm-eval/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'predicted_answer'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Normalize strings & types\u001b[39;00m\n\u001b[1;32m      4\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mupper()\n\u001b[0;32m----> 5\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredicted_answer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpredicted_answer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mupper()\n\u001b[1;32m      6\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfidence\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_numeric(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfidence\u001b[39m\u001b[38;5;124m\"\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0.0\u001b[39m)\n\u001b[1;32m      7\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreshold\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_numeric(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreshold\u001b[39m\u001b[38;5;124m\"\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/llm-eval/lib/python3.10/site-packages/pandas/core/frame.py:4113\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4113\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4115\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniforge3/envs/llm-eval/lib/python3.10/site-packages/pandas/core/indexes/base.py:3819\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3815\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3816\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3817\u001b[0m     ):\n\u001b[1;32m   3818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3821\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3822\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3823\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'predicted_answer'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Normalize strings & types\n",
    "df[\"answer\"] = df[\"answer\"].astype(str).str.strip().str.upper()\n",
    "df[\"predicted_answer\"] = df[\"predicted_answer\"].astype(str).str.strip().str.upper()\n",
    "df[\"confidence\"] = pd.to_numeric(df[\"confidence\"], errors=\"coerce\").fillna(0.0)\n",
    "df[\"threshold\"] = pd.to_numeric(df[\"threshold\"], errors=\"coerce\")\n",
    "\n",
    "print(f\"✅ Loaded {len(df)} rows from {CSV_PATH.name}\")\n",
    "print(\"Thresholds found:\", sorted(df[\"threshold\"].unique()))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f2bfbd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=0.25: 850 rows\n",
      "t=0.5: 850 rows\n",
      "t=0.75: 850 rows\n",
      "t=0.9: 850 rows\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 4 — Split into per-threshold DataFrames =====\n",
    "dfs_by_t = {t: df[df[\"threshold\"] == t].copy() for t in THRESHOLDS}\n",
    "\n",
    "for t in THRESHOLDS:\n",
    "    print(f\"t={t}: {len(dfs_by_t[t])} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc1fe3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 5 — Compute metrics per threshold =====\n",
    "metrics_rows = []\n",
    "\n",
    "for t in THRESHOLDS:\n",
    "    df_t = dfs_by_t[t]\n",
    "\n",
    "    metrics_rows.append({\n",
    "        \"threshold\": t,\n",
    "        \"accuracy_at_t\": accuracy_at_threshold(df_t, t),\n",
    "        \"coverage\": coverage(df_t, t),\n",
    "        \"penalty_mean\": penalty_adjusted_mean(df_t, t),\n",
    "        \"overconf_rate\": overconfidence_rate(df_t, t),\n",
    "        \"answered_n\": int((df_t[\"confidence\"] > t).sum()),\n",
    "        \"total_n\": len(df_t)\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_rows).sort_values(\"threshold\").reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e9cd275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4×4 Evaluation Table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "threshold                                                      \n",
       "0.25                 0.0       0.0           0.0            0.0\n",
       "0.50                 0.0       0.0           0.0            0.0\n",
       "0.75                 0.0       0.0           0.0            0.0\n",
       "0.90                 0.0       0.0           0.0            0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results in: outputs\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 6 — 4×4 evaluation table =====\n",
    "eval_table = (\n",
    "    metrics_df[[\"threshold\", \"accuracy_at_t\", \"coverage\", \"penalty_mean\", \"overconf_rate\"]]\n",
    "    .set_index(\"threshold\")\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# Display\n",
    "print(\"4×4 Evaluation Table:\")\n",
    "display(eval_table)\n",
    "\n",
    "# Save both detailed and compact tables\n",
    "eval_table.to_csv(OUTPUT_PATH / \"llama-mmlu-metric-eval.csv\")\n",
    "\n",
    "print(f\"Saved results in: {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cfe38a",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5bed551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG — eligible rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [accuracy_at_t, coverage, penalty_mean, overconf_rate]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "attempt to get argmax of an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDEBUG — eligible rows:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m display(eligible)\n\u001b[0;32m----> 7\u001b[0m best_row \u001b[38;5;241m=\u001b[39m eligible\u001b[38;5;241m.\u001b[39mloc[\u001b[43meligible\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maccuracy_at_t\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43midxmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m      8\u001b[0m t_star \u001b[38;5;241m=\u001b[39m best_row\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelected t* = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt_star\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/llm-eval/lib/python3.10/site-packages/pandas/core/series.py:2773\u001b[0m, in \u001b[0;36mSeries.idxmax\u001b[0;34m(self, axis, skipna, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2768\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n\u001b[1;32m   2769\u001b[0m     \u001b[38;5;66;03m# TODO(3.0): this catching/filtering can be removed\u001b[39;00m\n\u001b[1;32m   2770\u001b[0m     \u001b[38;5;66;03m# ignore warning produced by argmax since we will issue a different\u001b[39;00m\n\u001b[1;32m   2771\u001b[0m     \u001b[38;5;66;03m#  warning for argmax\u001b[39;00m\n\u001b[1;32m   2772\u001b[0m     warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2773\u001b[0m     i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2775\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2776\u001b[0m     \u001b[38;5;66;03m# GH#43587 give correct NA value for Index.\u001b[39;00m\n\u001b[1;32m   2777\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   2778\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe behavior of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.idxmax with all-NA \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2779\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues, or any-NA and skipna=False, is deprecated. In a future \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2782\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m   2783\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/llm-eval/lib/python3.10/site-packages/pandas/core/base.py:755\u001b[0m, in \u001b[0;36mIndexOpsMixin.argmax\u001b[0;34m(self, axis, skipna, *args, **kwargs)\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m delegate\u001b[38;5;241m.\u001b[39margmax()\n\u001b[1;32m    754\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 755\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mnanops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnanargmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelegate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    756\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    757\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    758\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe behavior of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.argmax/argmin \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    759\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith skipna=False and NAs, or with all-NAs is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    762\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    763\u001b[0m         )\n",
      "File \u001b[0;32m~/miniforge3/envs/llm-eval/lib/python3.10/site-packages/pandas/core/nanops.py:1148\u001b[0m, in \u001b[0;36mnanargmax\u001b[0;34m(values, axis, skipna, mask)\u001b[0m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;124;03m----------\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;124;03marray([2, 2, 1, 1])\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1147\u001b[0m values, mask \u001b[38;5;241m=\u001b[39m _get_values(values, \u001b[38;5;28;01mTrue\u001b[39;00m, fill_value_typ\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, mask\u001b[38;5;241m=\u001b[39mmask)\n\u001b[0;32m-> 1148\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[38;5;66;03m# error: Argument 1 to \"_maybe_arg_null_out\" has incompatible type \"Any |\u001b[39;00m\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;66;03m# signedinteger[Any]\"; expected \"ndarray[Any, Any]\"\u001b[39;00m\n\u001b[1;32m   1151\u001b[0m result \u001b[38;5;241m=\u001b[39m _maybe_arg_null_out(result, axis, mask, skipna)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: attempt to get argmax of an empty sequence"
     ]
    }
   ],
   "source": [
    "coverage_floor = 0.3\n",
    "\n",
    "eligible = eval_table[eval_table[\"coverage\"] >= coverage_floor]\n",
    "print(\"DEBUG — eligible rows:\")\n",
    "display(eligible)\n",
    "\n",
    "best_row = eligible.loc[eligible[\"accuracy_at_t\"].idxmax()]\n",
    "t_star = best_row.name\n",
    "\n",
    "print(f\"Selected t* = {t_star}\")\n",
    "display(best_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "02c2e9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 2 – Compute binary-grading and always-abstain baselines =====\n",
    "import numpy as np\n",
    "\n",
    "# Load the original prediction CSV (same used to compute metrics_df)\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Compute overall accuracy\n",
    "binary_acc = np.mean(df[\"predicted_answer\"] == df[\"answer\"])\n",
    "\n",
    "# Wrong rate (for overconfidence)\n",
    "wrong_rate = 1 - binary_acc\n",
    "\n",
    "# Binary baseline metrics (answers everything)\n",
    "binary_row = {\n",
    "    \"accuracy_at_t\": binary_acc,\n",
    "    \"coverage\": 1.0,\n",
    "    \"penalty_mean\": np.nan,  # you can fill with projected penalty if desired\n",
    "    \"overconf_rate\": wrong_rate,\n",
    "}\n",
    "\n",
    "# Always-abstain baseline metrics\n",
    "abstain_row = {\n",
    "    \"accuracy_at_t\": 0.0,\n",
    "    \"coverage\": 0.0,\n",
    "    \"penalty_mean\": 0.0,\n",
    "    \"overconf_rate\": 0.0,\n",
    "}\n",
    "\n",
    "print(\"Binary-grading baseline:\")\n",
    "display(pd.DataFrame([binary_row], index=[\"Binary\"]))\n",
    "print(\"Always-abstain baseline:\")\n",
    "display(pd.DataFrame([abstain_row], index=[\"Abstain\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "59a99052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 3 – Build final 3x4 headline table =====\n",
    "\n",
    "headline_df = pd.DataFrame([\n",
    "    best_row[[\"accuracy_at_t\", \"coverage\", \"penalty_mean\", \"overconf_rate\"]],\n",
    "    pd.Series(binary_row),\n",
    "    pd.Series(abstain_row)\n",
    "], index=[f\"Confidence-aware (t*={t_star})\", \"Binary grading\", \"Always abstain\"])\n",
    "\n",
    "print(\"=== 3x4 Headline Evaluation Table ===\")\n",
    "display(headline_df)\n",
    "headline_df.to_csv(OUTPUT_PATH / \"llama-mmlu-baseline-eval.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c18581",
   "metadata": {},
   "source": [
    "## Gemini Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4cae51bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPQA - Qwen Evalution \n",
    "CSV_PATH = Path(\"../inference/outputs/gemini-mmlu.csv\")  # <-- change this for each run\n",
    "OUTPUT_PATH = Path(\"outputs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c6288b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 3400 rows from gemini-mmlu.csv\n",
      "Thresholds found: [np.float64(0.25), np.float64(0.5), np.float64(0.75), np.float64(0.9)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>threshold</th>\n",
       "      <th>question</th>\n",
       "      <th>choices</th>\n",
       "      <th>answer</th>\n",
       "      <th>predicted_answer</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>596</td>\n",
       "      <td>0.25</td>\n",
       "      <td>A manufacturer is currently selling 2000 units...</td>\n",
       "      <td>['$2.50' '$1.90' '$2.70' '$2.60' '$1.80' '$2.2...</td>\n",
       "      <td>F</td>\n",
       "      <td>X</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>814</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Fred Lowes is a typewriter salesman. He receiv...</td>\n",
       "      <td>['$210' '$200' '$225' '$175' '$195' '$150' '$2...</td>\n",
       "      <td>I</td>\n",
       "      <td>S</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>817</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Mary Redmond purchased a $28,500 home with 20%...</td>\n",
       "      <td>['$305' '$190' '$171' '$285.50' '$399' '$323' ...</td>\n",
       "      <td>F</td>\n",
       "      <td>S</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  threshold                                           question  \\\n",
       "0  596       0.25  A manufacturer is currently selling 2000 units...   \n",
       "1  814       0.25  Fred Lowes is a typewriter salesman. He receiv...   \n",
       "2  817       0.25  Mary Redmond purchased a $28,500 home with 20%...   \n",
       "\n",
       "                                             choices answer predicted_answer  \\\n",
       "0  ['$2.50' '$1.90' '$2.70' '$2.60' '$1.80' '$2.2...      F                X   \n",
       "1  ['$210' '$200' '$225' '$175' '$195' '$150' '$2...      I                S   \n",
       "2  ['$305' '$190' '$171' '$285.50' '$399' '$323' ...      F                S   \n",
       "\n",
       "   confidence  \n",
       "0         1.0  \n",
       "1         1.0  \n",
       "2         1.0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Normalize strings & types\n",
    "df[\"answer\"] = df[\"answer\"].astype(str).str.strip().str.upper()\n",
    "df[\"predicted_answer\"] = df[\"predicted_answer\"].astype(str).str.strip().str.upper()\n",
    "df[\"confidence\"] = pd.to_numeric(df[\"confidence\"], errors=\"coerce\").fillna(0.0)\n",
    "df[\"threshold\"] = pd.to_numeric(df[\"threshold\"], errors=\"coerce\")\n",
    "\n",
    "print(f\"✅ Loaded {len(df)} rows from {CSV_PATH.name}\")\n",
    "print(\"Thresholds found:\", sorted(df[\"threshold\"].unique()))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f0828fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=0.25: 850 rows\n",
      "t=0.5: 850 rows\n",
      "t=0.75: 850 rows\n",
      "t=0.9: 850 rows\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 4 — Split into per-threshold DataFrames =====\n",
    "dfs_by_t = {t: df[df[\"threshold\"] == t].copy() for t in THRESHOLDS}\n",
    "\n",
    "for t in THRESHOLDS:\n",
    "    print(f\"t={t}: {len(dfs_by_t[t])} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d96f27c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "      <th>answered_n</th>\n",
       "      <th>total_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.123693</td>\n",
       "      <td>0.675294</td>\n",
       "      <td>-0.087301</td>\n",
       "      <td>0.591765</td>\n",
       "      <td>574</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.133231</td>\n",
       "      <td>0.768235</td>\n",
       "      <td>-0.515431</td>\n",
       "      <td>0.665882</td>\n",
       "      <td>653</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.138739</td>\n",
       "      <td>0.652941</td>\n",
       "      <td>-1.559059</td>\n",
       "      <td>0.562353</td>\n",
       "      <td>555</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.139831</td>\n",
       "      <td>0.555294</td>\n",
       "      <td>-4.221176</td>\n",
       "      <td>0.477647</td>\n",
       "      <td>472</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   threshold  accuracy_at_t  coverage  penalty_mean  overconf_rate  \\\n",
       "0       0.25       0.123693  0.675294     -0.087301       0.591765   \n",
       "1       0.50       0.133231  0.768235     -0.515431       0.665882   \n",
       "2       0.75       0.138739  0.652941     -1.559059       0.562353   \n",
       "3       0.90       0.139831  0.555294     -4.221176       0.477647   \n",
       "\n",
       "   answered_n  total_n  \n",
       "0         574      850  \n",
       "1         653      850  \n",
       "2         555      850  \n",
       "3         472      850  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== Cell 5 — Compute metrics per threshold =====\n",
    "metrics_rows = []\n",
    "\n",
    "for t in THRESHOLDS:\n",
    "    df_t = dfs_by_t[t]\n",
    "\n",
    "    metrics_rows.append({\n",
    "        \"threshold\": t,\n",
    "        \"accuracy_at_t\": accuracy_at_threshold(df_t, t),\n",
    "        \"coverage\": coverage(df_t, t),\n",
    "        \"penalty_mean\": penalty_adjusted_mean(df_t, t),\n",
    "        \"overconf_rate\": overconfidence_rate(df_t, t),\n",
    "        \"answered_n\": int((df_t[\"confidence\"] > t).sum()),\n",
    "        \"total_n\": len(df_t)\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_rows).sort_values(\"threshold\").reset_index(drop=True)\n",
    "metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "69297fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4×4 Evaluation Table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.123693</td>\n",
       "      <td>0.675294</td>\n",
       "      <td>-0.087301</td>\n",
       "      <td>0.591765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.133231</td>\n",
       "      <td>0.768235</td>\n",
       "      <td>-0.515431</td>\n",
       "      <td>0.665882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.138739</td>\n",
       "      <td>0.652941</td>\n",
       "      <td>-1.559059</td>\n",
       "      <td>0.562353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>0.139831</td>\n",
       "      <td>0.555294</td>\n",
       "      <td>-4.221176</td>\n",
       "      <td>0.477647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "threshold                                                      \n",
       "0.25            0.123693  0.675294     -0.087301       0.591765\n",
       "0.50            0.133231  0.768235     -0.515431       0.665882\n",
       "0.75            0.138739  0.652941     -1.559059       0.562353\n",
       "0.90            0.139831  0.555294     -4.221176       0.477647"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results in: outputs\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 6 — 4×4 evaluation table =====\n",
    "eval_table = (\n",
    "    metrics_df[[\"threshold\", \"accuracy_at_t\", \"coverage\", \"penalty_mean\", \"overconf_rate\"]]\n",
    "    .set_index(\"threshold\")\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# Display\n",
    "print(\"4×4 Evaluation Table:\")\n",
    "display(eval_table)\n",
    "\n",
    "# Save both detailed and compact tables\n",
    "eval_table.to_csv(OUTPUT_PATH / \"gemini-mmlu-metric-eval.csv\")\n",
    "\n",
    "print(f\"Saved results in: {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b35a66",
   "metadata": {},
   "source": [
    "### Baseline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bbaa912a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG — eligible rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.123693</td>\n",
       "      <td>0.675294</td>\n",
       "      <td>-0.087301</td>\n",
       "      <td>0.591765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.133231</td>\n",
       "      <td>0.768235</td>\n",
       "      <td>-0.515431</td>\n",
       "      <td>0.665882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.138739</td>\n",
       "      <td>0.652941</td>\n",
       "      <td>-1.559059</td>\n",
       "      <td>0.562353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>0.139831</td>\n",
       "      <td>0.555294</td>\n",
       "      <td>-4.221176</td>\n",
       "      <td>0.477647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "threshold                                                      \n",
       "0.25            0.123693  0.675294     -0.087301       0.591765\n",
       "0.50            0.133231  0.768235     -0.515431       0.665882\n",
       "0.75            0.138739  0.652941     -1.559059       0.562353\n",
       "0.90            0.139831  0.555294     -4.221176       0.477647"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected t* = 0.9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "accuracy_at_t    0.139831\n",
       "coverage         0.555294\n",
       "penalty_mean    -4.221176\n",
       "overconf_rate    0.477647\n",
       "Name: 0.9, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "coverage_floor = 0.3\n",
    "\n",
    "eligible = eval_table[eval_table[\"coverage\"] >= coverage_floor]\n",
    "print(\"DEBUG — eligible rows:\")\n",
    "display(eligible)\n",
    "\n",
    "best_row = eligible.loc[eligible[\"accuracy_at_t\"].idxmax()]\n",
    "t_star = best_row.name\n",
    "\n",
    "print(f\"Selected t* = {t_star}\")\n",
    "display(best_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8fd7728d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary-grading baseline:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Binary</th>\n",
       "      <td>0.105588</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.894412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "Binary       0.105588       1.0           NaN       0.894412"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Always-abstain baseline:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Abstain</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "Abstain            0.0       0.0           0.0            0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Cell 2 – Compute binary-grading and always-abstain baselines =====\n",
    "import numpy as np\n",
    "\n",
    "# Load the original prediction CSV (same used to compute metrics_df)\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Compute overall accuracy\n",
    "binary_acc = np.mean(df[\"predicted_answer\"] == df[\"answer\"])\n",
    "\n",
    "# Wrong rate (for overconfidence)\n",
    "wrong_rate = 1 - binary_acc\n",
    "\n",
    "# Binary baseline metrics (answers everything)\n",
    "binary_row = {\n",
    "    \"accuracy_at_t\": binary_acc,\n",
    "    \"coverage\": 1.0,\n",
    "    \"penalty_mean\": np.nan,  # you can fill with projected penalty if desired\n",
    "    \"overconf_rate\": wrong_rate,\n",
    "}\n",
    "\n",
    "# Always-abstain baseline metrics\n",
    "abstain_row = {\n",
    "    \"accuracy_at_t\": 0.0,\n",
    "    \"coverage\": 0.0,\n",
    "    \"penalty_mean\": 0.0,\n",
    "    \"overconf_rate\": 0.0,\n",
    "}\n",
    "\n",
    "print(\"Binary-grading baseline:\")\n",
    "display(pd.DataFrame([binary_row], index=[\"Binary\"]))\n",
    "print(\"Always-abstain baseline:\")\n",
    "display(pd.DataFrame([abstain_row], index=[\"Abstain\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "41bc13df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 3x4 Headline Evaluation Table ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Confidence-aware (t*=0.9)</th>\n",
       "      <td>0.139831</td>\n",
       "      <td>0.555294</td>\n",
       "      <td>-4.221176</td>\n",
       "      <td>0.477647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Binary grading</th>\n",
       "      <td>0.105588</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.894412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Always abstain</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           accuracy_at_t  coverage  penalty_mean  \\\n",
       "Confidence-aware (t*=0.9)       0.139831  0.555294     -4.221176   \n",
       "Binary grading                  0.105588  1.000000           NaN   \n",
       "Always abstain                  0.000000  0.000000      0.000000   \n",
       "\n",
       "                           overconf_rate  \n",
       "Confidence-aware (t*=0.9)       0.477647  \n",
       "Binary grading                  0.894412  \n",
       "Always abstain                  0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Cell 3 – Build final 3x4 headline table =====\n",
    "\n",
    "headline_df = pd.DataFrame([\n",
    "    best_row[[\"accuracy_at_t\", \"coverage\", \"penalty_mean\", \"overconf_rate\"]],\n",
    "    pd.Series(binary_row),\n",
    "    pd.Series(abstain_row)\n",
    "], index=[f\"Confidence-aware (t*={t_star})\", \"Binary grading\", \"Always abstain\"])\n",
    "\n",
    "print(\"=== 3x4 Headline Evaluation Table ===\")\n",
    "display(headline_df)\n",
    "headline_df.to_csv(OUTPUT_PATH / \"gemini-mmlu-baseline-eval.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a937e888",
   "metadata": {},
   "source": [
    "## Mistral Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "779f4cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPQA - Qwen Evalution \n",
    "CSV_PATH = Path(\"../inference/outputs/mistral-mmlu.csv\")  # <-- change this for each run\n",
    "OUTPUT_PATH = Path(\"outputs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eb63118c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 3400 rows from mistral-mmlu.csv\n",
      "Thresholds found: [np.float64(0.25), np.float64(0.5), np.float64(0.75), np.float64(0.9)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>threshold</th>\n",
       "      <th>question</th>\n",
       "      <th>choices</th>\n",
       "      <th>answer</th>\n",
       "      <th>predicted_answer</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>596</td>\n",
       "      <td>0.25</td>\n",
       "      <td>A manufacturer is currently selling 2000 units...</td>\n",
       "      <td>['$2.50' '$1.90' '$2.70' '$2.60' '$1.80' '$2.2...</td>\n",
       "      <td>F</td>\n",
       "      <td>IDK</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>814</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Fred Lowes is a typewriter salesman. He receiv...</td>\n",
       "      <td>['$210' '$200' '$225' '$175' '$195' '$150' '$2...</td>\n",
       "      <td>I</td>\n",
       "      <td>A</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>817</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Mary Redmond purchased a $28,500 home with 20%...</td>\n",
       "      <td>['$305' '$190' '$171' '$285.50' '$399' '$323' ...</td>\n",
       "      <td>F</td>\n",
       "      <td>IDK</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  threshold                                           question  \\\n",
       "0  596       0.25  A manufacturer is currently selling 2000 units...   \n",
       "1  814       0.25  Fred Lowes is a typewriter salesman. He receiv...   \n",
       "2  817       0.25  Mary Redmond purchased a $28,500 home with 20%...   \n",
       "\n",
       "                                             choices answer predicted_answer  \\\n",
       "0  ['$2.50' '$1.90' '$2.70' '$2.60' '$1.80' '$2.2...      F              IDK   \n",
       "1  ['$210' '$200' '$225' '$175' '$195' '$150' '$2...      I                A   \n",
       "2  ['$305' '$190' '$171' '$285.50' '$399' '$323' ...      F              IDK   \n",
       "\n",
       "   confidence  \n",
       "0    1.000000  \n",
       "1    1.000000  \n",
       "2    0.833333  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Normalize strings & types\n",
    "df[\"answer\"] = df[\"answer\"].astype(str).str.strip().str.upper()\n",
    "df[\"predicted_answer\"] = df[\"predicted_answer\"].astype(str).str.strip().str.upper()\n",
    "df[\"confidence\"] = pd.to_numeric(df[\"confidence\"], errors=\"coerce\").fillna(0.0)\n",
    "df[\"threshold\"] = pd.to_numeric(df[\"threshold\"], errors=\"coerce\")\n",
    "\n",
    "print(f\"✅ Loaded {len(df)} rows from {CSV_PATH.name}\")\n",
    "print(\"Thresholds found:\", sorted(df[\"threshold\"].unique()))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8cd599f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=0.25: 850 rows\n",
      "t=0.5: 850 rows\n",
      "t=0.75: 850 rows\n",
      "t=0.9: 850 rows\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 4 — Split into per-threshold DataFrames =====\n",
    "dfs_by_t = {t: df[df[\"threshold\"] == t].copy() for t in THRESHOLDS}\n",
    "\n",
    "for t in THRESHOLDS:\n",
    "    print(f\"t={t}: {len(dfs_by_t[t])} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "62ba2cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "      <th>answered_n</th>\n",
       "      <th>total_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.089928</td>\n",
       "      <td>0.981176</td>\n",
       "      <td>-0.157065</td>\n",
       "      <td>0.892941</td>\n",
       "      <td>834</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.094840</td>\n",
       "      <td>0.843529</td>\n",
       "      <td>-0.607843</td>\n",
       "      <td>0.763529</td>\n",
       "      <td>717</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.100352</td>\n",
       "      <td>0.668235</td>\n",
       "      <td>-1.656353</td>\n",
       "      <td>0.601176</td>\n",
       "      <td>568</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.103604</td>\n",
       "      <td>0.522353</td>\n",
       "      <td>-4.160000</td>\n",
       "      <td>0.468235</td>\n",
       "      <td>444</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   threshold  accuracy_at_t  coverage  penalty_mean  overconf_rate  \\\n",
       "0       0.25       0.089928  0.981176     -0.157065       0.892941   \n",
       "1       0.50       0.094840  0.843529     -0.607843       0.763529   \n",
       "2       0.75       0.100352  0.668235     -1.656353       0.601176   \n",
       "3       0.90       0.103604  0.522353     -4.160000       0.468235   \n",
       "\n",
       "   answered_n  total_n  \n",
       "0         834      850  \n",
       "1         717      850  \n",
       "2         568      850  \n",
       "3         444      850  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== Cell 5 — Compute metrics per threshold =====\n",
    "metrics_rows = []\n",
    "\n",
    "for t in THRESHOLDS:\n",
    "    df_t = dfs_by_t[t]\n",
    "\n",
    "    metrics_rows.append({\n",
    "        \"threshold\": t,\n",
    "        \"accuracy_at_t\": accuracy_at_threshold(df_t, t),\n",
    "        \"coverage\": coverage(df_t, t),\n",
    "        \"penalty_mean\": penalty_adjusted_mean(df_t, t),\n",
    "        \"overconf_rate\": overconfidence_rate(df_t, t),\n",
    "        \"answered_n\": int((df_t[\"confidence\"] > t).sum()),\n",
    "        \"total_n\": len(df_t)\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_rows).sort_values(\"threshold\").reset_index(drop=True)\n",
    "metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e674e8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4×4 Evaluation Table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.089928</td>\n",
       "      <td>0.981176</td>\n",
       "      <td>-0.157065</td>\n",
       "      <td>0.892941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.094840</td>\n",
       "      <td>0.843529</td>\n",
       "      <td>-0.607843</td>\n",
       "      <td>0.763529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.100352</td>\n",
       "      <td>0.668235</td>\n",
       "      <td>-1.656353</td>\n",
       "      <td>0.601176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>0.103604</td>\n",
       "      <td>0.522353</td>\n",
       "      <td>-4.160000</td>\n",
       "      <td>0.468235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "threshold                                                      \n",
       "0.25            0.089928  0.981176     -0.157065       0.892941\n",
       "0.50            0.094840  0.843529     -0.607843       0.763529\n",
       "0.75            0.100352  0.668235     -1.656353       0.601176\n",
       "0.90            0.103604  0.522353     -4.160000       0.468235"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results in: outputs\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 6 — 4×4 evaluation table =====\n",
    "eval_table = (\n",
    "    metrics_df[[\"threshold\", \"accuracy_at_t\", \"coverage\", \"penalty_mean\", \"overconf_rate\"]]\n",
    "    .set_index(\"threshold\")\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# Display\n",
    "print(\"4×4 Evaluation Table:\")\n",
    "display(eval_table)\n",
    "\n",
    "# Save both detailed and compact tables\n",
    "eval_table.to_csv(OUTPUT_PATH / \"mistral-mmlu-metric-eval.csv\")\n",
    "\n",
    "print(f\"Saved results in: {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b87d790",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8f7a5e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG — eligible rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.089928</td>\n",
       "      <td>0.981176</td>\n",
       "      <td>-0.157065</td>\n",
       "      <td>0.892941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.094840</td>\n",
       "      <td>0.843529</td>\n",
       "      <td>-0.607843</td>\n",
       "      <td>0.763529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.100352</td>\n",
       "      <td>0.668235</td>\n",
       "      <td>-1.656353</td>\n",
       "      <td>0.601176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>0.103604</td>\n",
       "      <td>0.522353</td>\n",
       "      <td>-4.160000</td>\n",
       "      <td>0.468235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "threshold                                                      \n",
       "0.25            0.089928  0.981176     -0.157065       0.892941\n",
       "0.50            0.094840  0.843529     -0.607843       0.763529\n",
       "0.75            0.100352  0.668235     -1.656353       0.601176\n",
       "0.90            0.103604  0.522353     -4.160000       0.468235"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected t* = 0.9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "accuracy_at_t    0.103604\n",
       "coverage         0.522353\n",
       "penalty_mean    -4.160000\n",
       "overconf_rate    0.468235\n",
       "Name: 0.9, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "coverage_floor = 0.3\n",
    "\n",
    "eligible = eval_table[eval_table[\"coverage\"] >= coverage_floor]\n",
    "print(\"DEBUG — eligible rows:\")\n",
    "display(eligible)\n",
    "\n",
    "best_row = eligible.loc[eligible[\"accuracy_at_t\"].idxmax()]\n",
    "t_star = best_row.name\n",
    "\n",
    "print(f\"Selected t* = {t_star}\")\n",
    "display(best_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9309b56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary-grading baseline:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Binary</th>\n",
       "      <td>0.087353</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.912647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "Binary       0.087353       1.0           NaN       0.912647"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Always-abstain baseline:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Abstain</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "Abstain            0.0       0.0           0.0            0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Cell 2 – Compute binary-grading and always-abstain baselines =====\n",
    "import numpy as np\n",
    "\n",
    "# Load the original prediction CSV (same used to compute metrics_df)\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Compute overall accuracy\n",
    "binary_acc = np.mean(df[\"predicted_answer\"] == df[\"answer\"])\n",
    "\n",
    "# Wrong rate (for overconfidence)\n",
    "wrong_rate = 1 - binary_acc\n",
    "\n",
    "# Binary baseline metrics (answers everything)\n",
    "binary_row = {\n",
    "    \"accuracy_at_t\": binary_acc,\n",
    "    \"coverage\": 1.0,\n",
    "    \"penalty_mean\": np.nan,  # you can fill with projected penalty if desired\n",
    "    \"overconf_rate\": wrong_rate,\n",
    "}\n",
    "\n",
    "# Always-abstain baseline metrics\n",
    "abstain_row = {\n",
    "    \"accuracy_at_t\": 0.0,\n",
    "    \"coverage\": 0.0,\n",
    "    \"penalty_mean\": 0.0,\n",
    "    \"overconf_rate\": 0.0,\n",
    "}\n",
    "\n",
    "print(\"Binary-grading baseline:\")\n",
    "display(pd.DataFrame([binary_row], index=[\"Binary\"]))\n",
    "print(\"Always-abstain baseline:\")\n",
    "display(pd.DataFrame([abstain_row], index=[\"Abstain\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "97a529e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 3x4 Headline Evaluation Table ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Confidence-aware (t*=0.9)</th>\n",
       "      <td>0.103604</td>\n",
       "      <td>0.522353</td>\n",
       "      <td>-4.16</td>\n",
       "      <td>0.468235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Binary grading</th>\n",
       "      <td>0.087353</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.912647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Always abstain</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           accuracy_at_t  coverage  penalty_mean  \\\n",
       "Confidence-aware (t*=0.9)       0.103604  0.522353         -4.16   \n",
       "Binary grading                  0.087353  1.000000           NaN   \n",
       "Always abstain                  0.000000  0.000000          0.00   \n",
       "\n",
       "                           overconf_rate  \n",
       "Confidence-aware (t*=0.9)       0.468235  \n",
       "Binary grading                  0.912647  \n",
       "Always abstain                  0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Cell 3 – Build final 3x4 headline table =====\n",
    "\n",
    "headline_df = pd.DataFrame([\n",
    "    best_row[[\"accuracy_at_t\", \"coverage\", \"penalty_mean\", \"overconf_rate\"]],\n",
    "    pd.Series(binary_row),\n",
    "    pd.Series(abstain_row)\n",
    "], index=[f\"Confidence-aware (t*={t_star})\", \"Binary grading\", \"Always abstain\"])\n",
    "\n",
    "print(\"=== 3x4 Headline Evaluation Table ===\")\n",
    "display(headline_df)\n",
    "headline_df.to_csv(OUTPUT_PATH / \"mistral-mmlu-baseline-eval.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
