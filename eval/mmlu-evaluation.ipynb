{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9de9656",
   "metadata": {},
   "source": [
    "## Imports, Setup, Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "ab2496e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Common thresholds (used for all model–dataset combinations)\n",
    "THRESHOLDS = [0.25, 0.5, 0.75, 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "9750d8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 2 — Metric functions =====\n",
    "\n",
    "def decide(p: float, t: float) -> bool:\n",
    "    \"\"\"\n",
    "    Decision rule: returns True if the model 'answers' the question.\n",
    "    We assume the model only answers when its confidence p exceeds the threshold t.\n",
    "    \"\"\"\n",
    "    return p > t\n",
    "\n",
    "\n",
    "def penalty_score(pred: str, gold: str, p: float, t: float) -> float:\n",
    "    \"\"\"\n",
    "    Penalty-adjusted score (as defined in the paper/proposal):\n",
    "      - If p <= t  → the model abstains → score = 0.\n",
    "      - If p > t and prediction == gold → score = +1.\n",
    "      - If p > t and prediction != gold → score = - (p * t) / (1 - t)\n",
    "        (this is a confidence-proportional penalty for being overconfident).\n",
    "    \"\"\"\n",
    "    if p <= t:\n",
    "        return 0.0\n",
    "    if pred == gold:\n",
    "        return 1.0\n",
    "    return round(- (p * t) / (1.0 - t),2)\n",
    "\n",
    "\n",
    "#Metric 1\n",
    "def accuracy_at_threshold(df: pd.DataFrame, t: float) -> float:\n",
    "    \"\"\"\n",
    "    Accuracy@t:\n",
    "      Fraction of *answered* questions that are correct.\n",
    "      = (# correct with p>t) / (# answered with p>t)\n",
    "    If the model abstains on all (no p>t), returns 0.0.\n",
    "    \"\"\"\n",
    "    answered = df[\"confidence\"] > t\n",
    "    answered_n = answered.sum()\n",
    "    if answered_n == 0:\n",
    "        return 0.0\n",
    "    correct = (df[\"predicted_answer\"] == df[\"answer\"]) & answered\n",
    "    return round(100*float(correct.sum() / answered_n),2)\n",
    "\n",
    "#Metric 2\n",
    "def coverage(df: pd.DataFrame, t: float) -> float:\n",
    "    \"\"\"\n",
    "    Coverage:\n",
    "      Fraction of total questions that the model *answers*.\n",
    "      = (# p>t) / total\n",
    "    \"\"\"\n",
    "    if len(df) == 0:\n",
    "        return 0.0\n",
    "    return round(100*float((df[\"confidence\"] > t).sum() / len(df)),2)\n",
    "\n",
    "#Metric 3\n",
    "def penalty_adjusted_mean(df: pd.DataFrame, t: float) -> float:\n",
    "    \"\"\"\n",
    "    Mean penalty-adjusted score across all rows (including abstains).\n",
    "    Abstentions contribute 0.\n",
    "    \"\"\"\n",
    "    scores = [\n",
    "        penalty_score(r.predicted_answer, r.answer, float(r.confidence), t)\n",
    "        for r in df.itertuples(index=False)\n",
    "    ]\n",
    "    return round(float(np.mean(scores)) if scores else 0.0,2)\n",
    "\n",
    "#Metric 4\n",
    "def overconfidence_rate(df: pd.DataFrame, t: float) -> float:\n",
    "    \"\"\"\n",
    "    Overconfidence rate:\n",
    "      Fraction of questions where the model is *wrong* but still confident (p>t).\n",
    "      = (# wrong & p>t) / total\n",
    "    \"\"\"\n",
    "    if len(df) == 0:\n",
    "        return 0.0\n",
    "    mask = (df[\"predicted_answer\"] != df[\"answer\"]) & (df[\"confidence\"] > t)\n",
    "    return round(100*float(mask.sum() / len(df)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae96d5f1",
   "metadata": {},
   "source": [
    "## Qwen Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "cb006450",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = Path(\"../inference/outputs/qwen-mmlu.csv\")  # <-- change this for each run\n",
    "OUTPUT_PATH = Path(\"outputs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "5c99043c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3400 rows from qwen-mmlu.csv\n",
      "Thresholds found: [np.float64(0.25), np.float64(0.5), np.float64(0.75), np.float64(0.9)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>threshold</th>\n",
       "      <th>question</th>\n",
       "      <th>choices</th>\n",
       "      <th>answer</th>\n",
       "      <th>predicted_answer</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>596</td>\n",
       "      <td>0.25</td>\n",
       "      <td>A manufacturer is currently selling 2000 units...</td>\n",
       "      <td>['$2.50' '$1.90' '$2.70' '$2.60' '$1.80' '$2.2...</td>\n",
       "      <td>F</td>\n",
       "      <td>IDK</td>\n",
       "      <td>50.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>814</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Fred Lowes is a typewriter salesman. He receiv...</td>\n",
       "      <td>['$210' '$200' '$225' '$175' '$195' '$150' '$2...</td>\n",
       "      <td>I</td>\n",
       "      <td>B</td>\n",
       "      <td>83.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>817</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Mary Redmond purchased a $28,500 home with 20%...</td>\n",
       "      <td>['$305' '$190' '$171' '$285.50' '$399' '$323' ...</td>\n",
       "      <td>F</td>\n",
       "      <td>IDK</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  threshold                                           question  \\\n",
       "0  596       0.25  A manufacturer is currently selling 2000 units...   \n",
       "1  814       0.25  Fred Lowes is a typewriter salesman. He receiv...   \n",
       "2  817       0.25  Mary Redmond purchased a $28,500 home with 20%...   \n",
       "\n",
       "                                             choices answer predicted_answer  \\\n",
       "0  ['$2.50' '$1.90' '$2.70' '$2.60' '$1.80' '$2.2...      F              IDK   \n",
       "1  ['$210' '$200' '$225' '$175' '$195' '$150' '$2...      I                B   \n",
       "2  ['$305' '$190' '$171' '$285.50' '$399' '$323' ...      F              IDK   \n",
       "\n",
       "   confidence  \n",
       "0       50.00  \n",
       "1       83.33  \n",
       "2      100.00  "
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Normalize strings & types\n",
    "df[\"answer\"] = df[\"answer\"].astype(str).str.strip().str.upper()\n",
    "df[\"predicted_answer\"] = df[\"predicted_answer\"].astype(str).str.strip().str.upper()\n",
    "df[\"confidence\"] = round(100*pd.to_numeric(df[\"confidence\"], errors=\"coerce\").fillna(0.0),2)\n",
    "df[\"threshold\"] = pd.to_numeric(df[\"threshold\"], errors=\"coerce\")\n",
    "\n",
    "print(f\"Loaded {len(df)} rows from {CSV_PATH.name}\")\n",
    "print(\"Thresholds found:\", sorted(df[\"threshold\"].unique()))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "059deb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=0.25: 850 rows\n",
      "t=0.5: 850 rows\n",
      "t=0.75: 850 rows\n",
      "t=0.9: 850 rows\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 4 — Split into per-threshold DataFrames =====\n",
    "dfs_by_t = {t: df[df[\"threshold\"] == t].copy() for t in THRESHOLDS}\n",
    "\n",
    "for t in THRESHOLDS:\n",
    "    print(f\"t={t}: {len(dfs_by_t[t])} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "c25d465a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "      <th>answered_n</th>\n",
       "      <th>total_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.25</td>\n",
       "      <td>7.97</td>\n",
       "      <td>97.41</td>\n",
       "      <td>-24.70</td>\n",
       "      <td>90</td>\n",
       "      <td>828</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.50</td>\n",
       "      <td>9.03</td>\n",
       "      <td>97.76</td>\n",
       "      <td>-73.96</td>\n",
       "      <td>89</td>\n",
       "      <td>831</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.75</td>\n",
       "      <td>7.13</td>\n",
       "      <td>97.29</td>\n",
       "      <td>-226.38</td>\n",
       "      <td>90</td>\n",
       "      <td>827</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.90</td>\n",
       "      <td>8.22</td>\n",
       "      <td>97.29</td>\n",
       "      <td>-667.16</td>\n",
       "      <td>89</td>\n",
       "      <td>827</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   threshold  accuracy_at_t  coverage  penalty_mean  overconf_rate  \\\n",
       "0       0.25           7.97     97.41        -24.70             90   \n",
       "1       0.50           9.03     97.76        -73.96             89   \n",
       "2       0.75           7.13     97.29       -226.38             90   \n",
       "3       0.90           8.22     97.29       -667.16             89   \n",
       "\n",
       "   answered_n  total_n  \n",
       "0         828      850  \n",
       "1         831      850  \n",
       "2         827      850  \n",
       "3         827      850  "
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== Cell 5 — Compute metrics per threshold =====\n",
    "metrics_rows = []\n",
    "\n",
    "for t in THRESHOLDS:\n",
    "    df_t = dfs_by_t[t]\n",
    "\n",
    "    metrics_rows.append({\n",
    "        \"threshold\": t,\n",
    "        \"accuracy_at_t\": accuracy_at_threshold(df_t, t),\n",
    "        \"coverage\": coverage(df_t, t),\n",
    "        \"penalty_mean\": penalty_adjusted_mean(df_t, t),\n",
    "        \"overconf_rate\": overconfidence_rate(df_t, t),\n",
    "        \"answered_n\": int((df_t[\"confidence\"] > t).sum()),\n",
    "        \"total_n\": len(df_t)\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_rows).sort_values(\"threshold\").reset_index(drop=True)\n",
    "metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "a7360cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4×4 Evaluation Table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>7.97</td>\n",
       "      <td>97.41</td>\n",
       "      <td>-24.70</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>9.03</td>\n",
       "      <td>97.76</td>\n",
       "      <td>-73.96</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>7.13</td>\n",
       "      <td>97.29</td>\n",
       "      <td>-226.38</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>8.22</td>\n",
       "      <td>97.29</td>\n",
       "      <td>-667.16</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "threshold                                                      \n",
       "0.25                7.97     97.41        -24.70             90\n",
       "0.50                9.03     97.76        -73.96             89\n",
       "0.75                7.13     97.29       -226.38             90\n",
       "0.90                8.22     97.29       -667.16             89"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results in: outputs\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 6 — 4×4 evaluation table =====\n",
    "eval_table = (\n",
    "    metrics_df[[\"threshold\", \"accuracy_at_t\", \"coverage\", \"penalty_mean\", \"overconf_rate\"]]\n",
    "    .set_index(\"threshold\")\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# Display\n",
    "print(\"4×4 Evaluation Table:\")\n",
    "display(eval_table)\n",
    "\n",
    "# Save both detailed and compact tables\n",
    "eval_table.to_csv(OUTPUT_PATH / \"qwen-mmlu-metric-eval.csv\")\n",
    "\n",
    "print(f\"Saved results in: {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6e8a28",
   "metadata": {},
   "source": [
    "### Baseline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "a6757486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG — eligible rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>7.97</td>\n",
       "      <td>97.41</td>\n",
       "      <td>-24.70</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>9.03</td>\n",
       "      <td>97.76</td>\n",
       "      <td>-73.96</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>7.13</td>\n",
       "      <td>97.29</td>\n",
       "      <td>-226.38</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>8.22</td>\n",
       "      <td>97.29</td>\n",
       "      <td>-667.16</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "threshold                                                      \n",
       "0.25                7.97     97.41        -24.70             90\n",
       "0.50                9.03     97.76        -73.96             89\n",
       "0.75                7.13     97.29       -226.38             90\n",
       "0.90                8.22     97.29       -667.16             89"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected t* = 0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "accuracy_at_t     9.03\n",
       "coverage         97.76\n",
       "penalty_mean    -73.96\n",
       "overconf_rate    89.00\n",
       "Name: 0.5, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "coverage_floor = 0.3\n",
    "\n",
    "eligible = eval_table[eval_table[\"coverage\"] >= coverage_floor]\n",
    "print(\"DEBUG — eligible rows:\")\n",
    "display(eligible)\n",
    "\n",
    "best_row = eligible.loc[eligible[\"accuracy_at_t\"].idxmax()]\n",
    "t_star = best_row.name\n",
    "\n",
    "print(f\"Selected t* = {t_star}\")\n",
    "display(best_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "4e278d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary-grading baseline:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Binary</th>\n",
       "      <td>7.88</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92.12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "Binary           7.88       1.0           NaN          92.12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Always-abstain baseline:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Abstain</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "Abstain            0.0       0.0           0.0            0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Cell 2 – Compute binary-grading and always-abstain baselines =====\n",
    "import numpy as np\n",
    "\n",
    "# Load the original prediction CSV (same used to compute metrics_df)\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Compute overall accuracy\n",
    "binary_acc = np.mean(df[\"predicted_answer\"] == df[\"answer\"])\n",
    "\n",
    "# Wrong rate (for overconfidence)\n",
    "wrong_rate = 1 - binary_acc\n",
    "\n",
    "# Binary baseline metrics (answers everything)\n",
    "binary_row = {\n",
    "    \"accuracy_at_t\": round(100*binary_acc,2),\n",
    "    \"coverage\": 1.0,\n",
    "    \"penalty_mean\": np.nan,  # you can fill with projected penalty if desired\n",
    "    \"overconf_rate\": round(100*wrong_rate,2),\n",
    "}\n",
    "\n",
    "# Always-abstain baseline metrics\n",
    "abstain_row = {\n",
    "    \"accuracy_at_t\": 0.0,\n",
    "    \"coverage\": 0.0,\n",
    "    \"penalty_mean\": 0.0,\n",
    "    \"overconf_rate\": 0.0,\n",
    "}\n",
    "\n",
    "print(\"Binary-grading baseline:\")\n",
    "display(pd.DataFrame([binary_row], index=[\"Binary\"]))\n",
    "print(\"Always-abstain baseline:\")\n",
    "display(pd.DataFrame([abstain_row], index=[\"Abstain\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "eb6d0541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 3x4 Headline Evaluation Table ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Confidence-aware (t*=0.5)</th>\n",
       "      <td>9.03</td>\n",
       "      <td>97.76</td>\n",
       "      <td>-73.96</td>\n",
       "      <td>89.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Binary grading</th>\n",
       "      <td>7.88</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Always abstain</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           accuracy_at_t  coverage  penalty_mean  \\\n",
       "Confidence-aware (t*=0.5)           9.03     97.76        -73.96   \n",
       "Binary grading                      7.88      1.00           NaN   \n",
       "Always abstain                      0.00      0.00          0.00   \n",
       "\n",
       "                           overconf_rate  \n",
       "Confidence-aware (t*=0.5)          89.00  \n",
       "Binary grading                     92.12  \n",
       "Always abstain                      0.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Cell 3 – Build final 3x4 headline table =====\n",
    "\n",
    "headline_df = pd.DataFrame([\n",
    "    best_row[[\"accuracy_at_t\", \"coverage\", \"penalty_mean\", \"overconf_rate\"]],\n",
    "    pd.Series(binary_row),\n",
    "    pd.Series(abstain_row)\n",
    "], index=[f\"Confidence-aware (t*={t_star})\", \"Binary grading\", \"Always abstain\"])\n",
    "\n",
    "print(\"=== 3x4 Headline Evaluation Table ===\")\n",
    "display(headline_df)\n",
    "headline_df.to_csv(OUTPUT_PATH / \"qwen-mmlu-baseline-eval.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bab1ca",
   "metadata": {},
   "source": [
    "## GPT Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "d2e305c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = Path(\"../inference/outputs/gpt-mmlu.csv\")  # <-- change this for each run\n",
    "OUTPUT_PATH = Path(\"outputs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "f67a564c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 3400 rows from gpt-mmlu.csv\n",
      "Thresholds found: [np.float64(0.25), np.float64(0.5), np.float64(0.75), np.float64(0.9)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>threshold</th>\n",
       "      <th>question</th>\n",
       "      <th>choices</th>\n",
       "      <th>answer</th>\n",
       "      <th>predicted_answer</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>596</td>\n",
       "      <td>0.25</td>\n",
       "      <td>A manufacturer is currently selling 2000 units...</td>\n",
       "      <td>['$2.50' '$1.90' '$2.70' '$2.60' '$1.80' '$2.2...</td>\n",
       "      <td>F</td>\n",
       "      <td>D</td>\n",
       "      <td>33.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>814</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Fred Lowes is a typewriter salesman. He receiv...</td>\n",
       "      <td>['$210' '$200' '$225' '$175' '$195' '$150' '$2...</td>\n",
       "      <td>I</td>\n",
       "      <td>B</td>\n",
       "      <td>66.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>817</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Mary Redmond purchased a $28,500 home with 20%...</td>\n",
       "      <td>['$305' '$190' '$171' '$285.50' '$399' '$323' ...</td>\n",
       "      <td>F</td>\n",
       "      <td>C</td>\n",
       "      <td>50.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  threshold                                           question  \\\n",
       "0  596       0.25  A manufacturer is currently selling 2000 units...   \n",
       "1  814       0.25  Fred Lowes is a typewriter salesman. He receiv...   \n",
       "2  817       0.25  Mary Redmond purchased a $28,500 home with 20%...   \n",
       "\n",
       "                                             choices answer predicted_answer  \\\n",
       "0  ['$2.50' '$1.90' '$2.70' '$2.60' '$1.80' '$2.2...      F                D   \n",
       "1  ['$210' '$200' '$225' '$175' '$195' '$150' '$2...      I                B   \n",
       "2  ['$305' '$190' '$171' '$285.50' '$399' '$323' ...      F                C   \n",
       "\n",
       "   confidence  \n",
       "0       33.33  \n",
       "1       66.67  \n",
       "2       50.00  "
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Normalize strings & types\n",
    "df[\"answer\"] = df[\"answer\"].astype(str).str.strip().str.upper()\n",
    "df[\"predicted_answer\"] = df[\"predicted_answer\"].astype(str).str.strip().str.upper()\n",
    "df[\"confidence\"] = round(100*pd.to_numeric(df[\"confidence\"], errors=\"coerce\").fillna(0.0),2)\n",
    "df[\"threshold\"] = pd.to_numeric(df[\"threshold\"], errors=\"coerce\")\n",
    "\n",
    "print(f\"✅ Loaded {len(df)} rows from {CSV_PATH.name}\")\n",
    "print(\"Thresholds found:\", sorted(df[\"threshold\"].unique()))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "9052885d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=0.25: 850 rows\n",
      "t=0.5: 850 rows\n",
      "t=0.75: 850 rows\n",
      "t=0.9: 850 rows\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 4 — Split into per-threshold DataFrames =====\n",
    "dfs_by_t = {t: df[df[\"threshold\"] == t].copy() for t in THRESHOLDS}\n",
    "\n",
    "for t in THRESHOLDS:\n",
    "    print(f\"t={t}: {len(dfs_by_t[t])} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "056e243e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "      <th>answered_n</th>\n",
       "      <th>total_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.25</td>\n",
       "      <td>19.76</td>\n",
       "      <td>100.00</td>\n",
       "      <td>-20.72</td>\n",
       "      <td>80</td>\n",
       "      <td>850</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.50</td>\n",
       "      <td>19.18</td>\n",
       "      <td>100.00</td>\n",
       "      <td>-63.26</td>\n",
       "      <td>81</td>\n",
       "      <td>850</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.75</td>\n",
       "      <td>20.39</td>\n",
       "      <td>72.12</td>\n",
       "      <td>-134.84</td>\n",
       "      <td>57</td>\n",
       "      <td>613</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   threshold  accuracy_at_t  coverage  penalty_mean  overconf_rate  \\\n",
       "0       0.25          19.76    100.00        -20.72             80   \n",
       "1       0.50          19.18    100.00        -63.26             81   \n",
       "2       0.75          20.39     72.12       -134.84             57   \n",
       "3       0.90           0.00      0.00          0.00              0   \n",
       "\n",
       "   answered_n  total_n  \n",
       "0         850      850  \n",
       "1         850      850  \n",
       "2         613      850  \n",
       "3           0      850  "
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== Cell 5 — Compute metrics per threshold =====\n",
    "metrics_rows = []\n",
    "\n",
    "for t in THRESHOLDS:\n",
    "    df_t = dfs_by_t[t]\n",
    "\n",
    "    metrics_rows.append({\n",
    "        \"threshold\": t,\n",
    "        \"accuracy_at_t\": accuracy_at_threshold(df_t, t),\n",
    "        \"coverage\": coverage(df_t, t),\n",
    "        \"penalty_mean\": penalty_adjusted_mean(df_t, t),\n",
    "        \"overconf_rate\": overconfidence_rate(df_t, t),\n",
    "        \"answered_n\": int((df_t[\"confidence\"] > t).sum()),\n",
    "        \"total_n\": len(df_t)\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_rows).sort_values(\"threshold\").reset_index(drop=True)\n",
    "metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "fd275867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4×4 Evaluation Table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>19.76</td>\n",
       "      <td>100.00</td>\n",
       "      <td>-20.72</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>19.18</td>\n",
       "      <td>100.00</td>\n",
       "      <td>-63.26</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>20.39</td>\n",
       "      <td>72.12</td>\n",
       "      <td>-134.84</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "threshold                                                      \n",
       "0.25               19.76    100.00        -20.72             80\n",
       "0.50               19.18    100.00        -63.26             81\n",
       "0.75               20.39     72.12       -134.84             57\n",
       "0.90                0.00      0.00          0.00              0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results in: outputs\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 6 — 4×4 evaluation table =====\n",
    "eval_table = (\n",
    "    metrics_df[[\"threshold\", \"accuracy_at_t\", \"coverage\", \"penalty_mean\", \"overconf_rate\"]]\n",
    "    .set_index(\"threshold\")\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# Display\n",
    "print(\"4×4 Evaluation Table:\")\n",
    "display(eval_table)\n",
    "\n",
    "# Save both detailed and compact tables\n",
    "eval_table.to_csv(OUTPUT_PATH / \"gpt-mmlu-metric-eval.csv\")\n",
    "\n",
    "print(f\"Saved results in: {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37f5925",
   "metadata": {},
   "source": [
    "### Baseline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "709cdde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG — eligible rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>19.76</td>\n",
       "      <td>100.00</td>\n",
       "      <td>-20.72</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>19.18</td>\n",
       "      <td>100.00</td>\n",
       "      <td>-63.26</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>20.39</td>\n",
       "      <td>72.12</td>\n",
       "      <td>-134.84</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "threshold                                                      \n",
       "0.25               19.76    100.00        -20.72             80\n",
       "0.50               19.18    100.00        -63.26             81\n",
       "0.75               20.39     72.12       -134.84             57"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected t* = 0.75\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "accuracy_at_t     20.39\n",
       "coverage          72.12\n",
       "penalty_mean    -134.84\n",
       "overconf_rate     57.00\n",
       "Name: 0.75, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "coverage_floor = 0.3\n",
    "\n",
    "eligible = eval_table[eval_table[\"coverage\"] >= coverage_floor]\n",
    "print(\"DEBUG — eligible rows:\")\n",
    "display(eligible)\n",
    "\n",
    "best_row = eligible.loc[eligible[\"accuracy_at_t\"].idxmax()]\n",
    "t_star = best_row.name\n",
    "\n",
    "print(f\"Selected t* = {t_star}\")\n",
    "display(best_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "290126c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary-grading baseline:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Binary</th>\n",
       "      <td>13.41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86.59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "Binary          13.41       1.0           NaN          86.59"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Always-abstain baseline:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Abstain</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "Abstain            0.0       0.0           0.0            0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Cell 2 – Compute binary-grading and always-abstain baselines =====\n",
    "import numpy as np\n",
    "\n",
    "# Load the original prediction CSV (same used to compute metrics_df)\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Compute overall accuracy\n",
    "binary_acc = np.mean(df[\"predicted_answer\"] == df[\"answer\"])\n",
    "\n",
    "# Wrong rate (for overconfidence)\n",
    "wrong_rate = 1 - binary_acc\n",
    "\n",
    "# Binary baseline metrics (answers everything)\n",
    "binary_row = {\n",
    "    \"accuracy_at_t\": round(100*binary_acc,2),\n",
    "    \"coverage\": 1.0,\n",
    "    \"penalty_mean\": np.nan,  # you can fill with projected penalty if desired\n",
    "    \"overconf_rate\": round(100*wrong_rate,2),\n",
    "}\n",
    "\n",
    "# Always-abstain baseline metrics\n",
    "abstain_row = {\n",
    "    \"accuracy_at_t\": 0.0,\n",
    "    \"coverage\": 0.0,\n",
    "    \"penalty_mean\": 0.0,\n",
    "    \"overconf_rate\": 0.0,\n",
    "}\n",
    "\n",
    "print(\"Binary-grading baseline:\")\n",
    "display(pd.DataFrame([binary_row], index=[\"Binary\"]))\n",
    "print(\"Always-abstain baseline:\")\n",
    "display(pd.DataFrame([abstain_row], index=[\"Abstain\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "f8b39a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 3x4 Headline Evaluation Table ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Confidence-aware (t*=0.75)</th>\n",
       "      <td>20.39</td>\n",
       "      <td>72.12</td>\n",
       "      <td>-134.84</td>\n",
       "      <td>57.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Binary grading</th>\n",
       "      <td>13.41</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Always abstain</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            accuracy_at_t  coverage  penalty_mean  \\\n",
       "Confidence-aware (t*=0.75)          20.39     72.12       -134.84   \n",
       "Binary grading                      13.41      1.00           NaN   \n",
       "Always abstain                       0.00      0.00          0.00   \n",
       "\n",
       "                            overconf_rate  \n",
       "Confidence-aware (t*=0.75)          57.00  \n",
       "Binary grading                      86.59  \n",
       "Always abstain                       0.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Cell 3 – Build final 3x4 headline table =====\n",
    "\n",
    "headline_df = pd.DataFrame([\n",
    "    best_row[[\"accuracy_at_t\", \"coverage\", \"penalty_mean\", \"overconf_rate\"]],\n",
    "    pd.Series(binary_row),\n",
    "    pd.Series(abstain_row)\n",
    "], index=[f\"Confidence-aware (t*={t_star})\", \"Binary grading\", \"Always abstain\"])\n",
    "\n",
    "print(\"=== 3x4 Headline Evaluation Table ===\")\n",
    "display(headline_df)\n",
    "headline_df.to_csv(OUTPUT_PATH / \"gpt-mmlu-baseline-eval.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99be7c33",
   "metadata": {},
   "source": [
    "## Claude Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "0059b7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPQA - Qwen Evalution \n",
    "CSV_PATH = Path(\"../inference/outputs/claude-mmlu.csv\")  # <-- change this for each run\n",
    "OUTPUT_PATH = Path(\"outputs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "56bd997c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 3400 rows from claude-mmlu.csv\n",
      "Thresholds found: [np.float64(0.25), np.float64(0.5), np.float64(0.75), np.float64(0.9)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>threshold</th>\n",
       "      <th>question</th>\n",
       "      <th>choices</th>\n",
       "      <th>answer</th>\n",
       "      <th>predicted_answer</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>596</td>\n",
       "      <td>0.25</td>\n",
       "      <td>A manufacturer is currently selling 2000 units...</td>\n",
       "      <td>['$2.50' '$1.90' '$2.70' '$2.60' '$1.80' '$2.2...</td>\n",
       "      <td>F</td>\n",
       "      <td>I</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>814</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Fred Lowes is a typewriter salesman. He receiv...</td>\n",
       "      <td>['$210' '$200' '$225' '$175' '$195' '$150' '$2...</td>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>817</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Mary Redmond purchased a $28,500 home with 20%...</td>\n",
       "      <td>['$305' '$190' '$171' '$285.50' '$399' '$323' ...</td>\n",
       "      <td>F</td>\n",
       "      <td>I</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  threshold                                           question  \\\n",
       "0  596       0.25  A manufacturer is currently selling 2000 units...   \n",
       "1  814       0.25  Fred Lowes is a typewriter salesman. He receiv...   \n",
       "2  817       0.25  Mary Redmond purchased a $28,500 home with 20%...   \n",
       "\n",
       "                                             choices answer predicted_answer  \\\n",
       "0  ['$2.50' '$1.90' '$2.70' '$2.60' '$1.80' '$2.2...      F                I   \n",
       "1  ['$210' '$200' '$225' '$175' '$195' '$150' '$2...      I                I   \n",
       "2  ['$305' '$190' '$171' '$285.50' '$399' '$323' ...      F                I   \n",
       "\n",
       "   confidence  \n",
       "0       100.0  \n",
       "1       100.0  \n",
       "2       100.0  "
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Normalize strings & types\n",
    "df[\"answer\"] = df[\"answer\"].astype(str).str.strip().str.upper()\n",
    "df[\"predicted_answer\"] = df[\"predicted_answer\"].astype(str).str.strip().str.upper()\n",
    "df[\"confidence\"] = round(100*pd.to_numeric(df[\"confidence\"], errors=\"coerce\").fillna(0.0),2)\n",
    "df[\"threshold\"] = pd.to_numeric(df[\"threshold\"], errors=\"coerce\")\n",
    "\n",
    "print(f\"✅ Loaded {len(df)} rows from {CSV_PATH.name}\")\n",
    "print(\"Thresholds found:\", sorted(df[\"threshold\"].unique()))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "bc7688f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=0.25: 850 rows\n",
      "t=0.5: 850 rows\n",
      "t=0.75: 850 rows\n",
      "t=0.9: 850 rows\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 4 — Split into per-threshold DataFrames =====\n",
    "dfs_by_t = {t: df[df[\"threshold\"] == t].copy() for t in THRESHOLDS}\n",
    "\n",
    "for t in THRESHOLDS:\n",
    "    print(f\"t={t}: {len(dfs_by_t[t])} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "9c25f754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 5 — Compute metrics per threshold =====\n",
    "metrics_rows = []\n",
    "\n",
    "for t in THRESHOLDS:\n",
    "    df_t = dfs_by_t[t]\n",
    "\n",
    "    metrics_rows.append({\n",
    "        \"threshold\": t,\n",
    "        \"accuracy_at_t\": accuracy_at_threshold(df_t, t),\n",
    "        \"coverage\": coverage(df_t, t),\n",
    "        \"penalty_mean\": penalty_adjusted_mean(df_t, t),\n",
    "        \"overconf_rate\": overconfidence_rate(df_t, t),\n",
    "        \"answered_n\": int((df_t[\"confidence\"] > t).sum()),\n",
    "        \"total_n\": len(df_t)\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_rows).sort_values(\"threshold\").reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "92a93fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4×4 Evaluation Table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>13.45</td>\n",
       "      <td>98.00</td>\n",
       "      <td>-26.35</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>13.82</td>\n",
       "      <td>97.88</td>\n",
       "      <td>-78.79</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>12.84</td>\n",
       "      <td>98.94</td>\n",
       "      <td>-246.70</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>11.76</td>\n",
       "      <td>99.06</td>\n",
       "      <td>-757.54</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "threshold                                                      \n",
       "0.25               13.45     98.00        -26.35             85\n",
       "0.50               13.82     97.88        -78.79             84\n",
       "0.75               12.84     98.94       -246.70             86\n",
       "0.90               11.76     99.06       -757.54             87"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results in: outputs\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 6 — 4×4 evaluation table =====\n",
    "eval_table = (\n",
    "    metrics_df[[\"threshold\", \"accuracy_at_t\", \"coverage\", \"penalty_mean\", \"overconf_rate\"]]\n",
    "    .set_index(\"threshold\")\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# Display\n",
    "print(\"4×4 Evaluation Table:\")\n",
    "display(eval_table)\n",
    "\n",
    "# Save both detailed and compact tables\n",
    "eval_table.to_csv(OUTPUT_PATH / \"claude-mmlu-metric-eval.csv\")\n",
    "\n",
    "print(f\"Saved results in: {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1742c3",
   "metadata": {},
   "source": [
    "### Baseline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "40cddaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG — eligible rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>13.45</td>\n",
       "      <td>98.00</td>\n",
       "      <td>-26.35</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>13.82</td>\n",
       "      <td>97.88</td>\n",
       "      <td>-78.79</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>12.84</td>\n",
       "      <td>98.94</td>\n",
       "      <td>-246.70</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>11.76</td>\n",
       "      <td>99.06</td>\n",
       "      <td>-757.54</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "threshold                                                      \n",
       "0.25               13.45     98.00        -26.35             85\n",
       "0.50               13.82     97.88        -78.79             84\n",
       "0.75               12.84     98.94       -246.70             86\n",
       "0.90               11.76     99.06       -757.54             87"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected t* = 0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "accuracy_at_t    13.82\n",
       "coverage         97.88\n",
       "penalty_mean    -78.79\n",
       "overconf_rate    84.00\n",
       "Name: 0.5, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "coverage_floor = 0.3\n",
    "\n",
    "eligible = eval_table[eval_table[\"coverage\"] >= coverage_floor]\n",
    "print(\"DEBUG — eligible rows:\")\n",
    "display(eligible)\n",
    "\n",
    "best_row = eligible.loc[eligible[\"accuracy_at_t\"].idxmax()]\n",
    "t_star = best_row.name\n",
    "\n",
    "print(f\"Selected t* = {t_star}\")\n",
    "display(best_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "9634128b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary-grading baseline:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Binary</th>\n",
       "      <td>12.76</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>87.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "Binary          12.76       1.0           NaN          87.24"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Always-abstain baseline:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Abstain</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "Abstain            0.0       0.0           0.0            0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Cell 2 – Compute binary-grading and always-abstain baselines =====\n",
    "import numpy as np\n",
    "\n",
    "# Load the original prediction CSV (same used to compute metrics_df)\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Compute overall accuracy\n",
    "binary_acc = np.mean(df[\"predicted_answer\"] == df[\"answer\"])\n",
    "\n",
    "# Wrong rate (for overconfidence)\n",
    "wrong_rate = 1 - binary_acc\n",
    "\n",
    "# Binary baseline metrics (answers everything)\n",
    "binary_row = {\n",
    "    \"accuracy_at_t\": round(100*binary_acc,2),\n",
    "    \"coverage\": 1.0,\n",
    "    \"penalty_mean\": np.nan,  # you can fill with projected penalty if desired\n",
    "    \"overconf_rate\": round(100*wrong_rate,2),\n",
    "}\n",
    "\n",
    "# Always-abstain baseline metrics\n",
    "abstain_row = {\n",
    "    \"accuracy_at_t\": 0.0,\n",
    "    \"coverage\": 0.0,\n",
    "    \"penalty_mean\": 0.0,\n",
    "    \"overconf_rate\": 0.0,\n",
    "}\n",
    "\n",
    "print(\"Binary-grading baseline:\")\n",
    "display(pd.DataFrame([binary_row], index=[\"Binary\"]))\n",
    "print(\"Always-abstain baseline:\")\n",
    "display(pd.DataFrame([abstain_row], index=[\"Abstain\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "098a62f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 3x4 Headline Evaluation Table ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Confidence-aware (t*=0.5)</th>\n",
       "      <td>13.82</td>\n",
       "      <td>97.88</td>\n",
       "      <td>-78.79</td>\n",
       "      <td>84.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Binary grading</th>\n",
       "      <td>12.76</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>87.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Always abstain</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           accuracy_at_t  coverage  penalty_mean  \\\n",
       "Confidence-aware (t*=0.5)          13.82     97.88        -78.79   \n",
       "Binary grading                     12.76      1.00           NaN   \n",
       "Always abstain                      0.00      0.00          0.00   \n",
       "\n",
       "                           overconf_rate  \n",
       "Confidence-aware (t*=0.5)          84.00  \n",
       "Binary grading                     87.24  \n",
       "Always abstain                      0.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Cell 3 – Build final 3x4 headline table =====\n",
    "\n",
    "headline_df = pd.DataFrame([\n",
    "    best_row[[\"accuracy_at_t\", \"coverage\", \"penalty_mean\", \"overconf_rate\"]],\n",
    "    pd.Series(binary_row),\n",
    "    pd.Series(abstain_row)\n",
    "], index=[f\"Confidence-aware (t*={t_star})\", \"Binary grading\", \"Always abstain\"])\n",
    "\n",
    "print(\"=== 3x4 Headline Evaluation Table ===\")\n",
    "display(headline_df)\n",
    "headline_df.to_csv(OUTPUT_PATH / \"claude-mmlu-baseline-eval.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe2d7bb",
   "metadata": {},
   "source": [
    "## LLama Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "eaba8e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPQA - Qwen Evalution \n",
    "CSV_PATH = Path(\"../inference/outputs/llama-mmlu.csv\")  # <-- change this for each run\n",
    "OUTPUT_PATH = Path(\"outputs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "1b089aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 3400 rows from llama-mmlu.csv\n",
      "Thresholds found: [np.float64(0.25), np.float64(0.5), np.float64(0.75), np.float64(0.9)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>threshold</th>\n",
       "      <th>question</th>\n",
       "      <th>choices</th>\n",
       "      <th>answer</th>\n",
       "      <th>predicted_answer</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>596</td>\n",
       "      <td>0.25</td>\n",
       "      <td>A manufacturer is currently selling 2000 units...</td>\n",
       "      <td>['$2.50' '$1.90' '$2.70' '$2.60' '$1.80' '$2.2...</td>\n",
       "      <td>F</td>\n",
       "      <td>E</td>\n",
       "      <td>50.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>814</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Fred Lowes is a typewriter salesman. He receiv...</td>\n",
       "      <td>['$210' '$200' '$225' '$175' '$195' '$150' '$2...</td>\n",
       "      <td>I</td>\n",
       "      <td>E</td>\n",
       "      <td>33.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>817</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Mary Redmond purchased a $28,500 home with 20%...</td>\n",
       "      <td>['$305' '$190' '$171' '$285.50' '$399' '$323' ...</td>\n",
       "      <td>F</td>\n",
       "      <td>A</td>\n",
       "      <td>66.67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  threshold                                           question  \\\n",
       "0  596       0.25  A manufacturer is currently selling 2000 units...   \n",
       "1  814       0.25  Fred Lowes is a typewriter salesman. He receiv...   \n",
       "2  817       0.25  Mary Redmond purchased a $28,500 home with 20%...   \n",
       "\n",
       "                                             choices answer predicted_answer  \\\n",
       "0  ['$2.50' '$1.90' '$2.70' '$2.60' '$1.80' '$2.2...      F                E   \n",
       "1  ['$210' '$200' '$225' '$175' '$195' '$150' '$2...      I                E   \n",
       "2  ['$305' '$190' '$171' '$285.50' '$399' '$323' ...      F                A   \n",
       "\n",
       "   confidence  \n",
       "0       50.00  \n",
       "1       33.33  \n",
       "2       66.67  "
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Normalize strings & types\n",
    "df[\"answer\"] = df[\"answer\"].astype(str).str.strip().str.upper()\n",
    "df[\"predicted_answer\"] = df[\"predicted_answer\"].astype(str).str.strip().str.upper()\n",
    "df[\"confidence\"] = round(100*pd.to_numeric(df[\"confidence\"], errors=\"coerce\").fillna(0.0),2)\n",
    "df[\"threshold\"] = pd.to_numeric(df[\"threshold\"], errors=\"coerce\")\n",
    "\n",
    "print(f\"✅ Loaded {len(df)} rows from {CSV_PATH.name}\")\n",
    "print(\"Thresholds found:\", sorted(df[\"threshold\"].unique()))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "f2bfbd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=0.25: 850 rows\n",
      "t=0.5: 850 rows\n",
      "t=0.75: 850 rows\n",
      "t=0.9: 850 rows\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 4 — Split into per-threshold DataFrames =====\n",
    "dfs_by_t = {t: df[df[\"threshold\"] == t].copy() for t in THRESHOLDS}\n",
    "\n",
    "for t in THRESHOLDS:\n",
    "    print(f\"t={t}: {len(dfs_by_t[t])} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "bc1fe3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 5 — Compute metrics per threshold =====\n",
    "metrics_rows = []\n",
    "\n",
    "for t in THRESHOLDS:\n",
    "    df_t = dfs_by_t[t]\n",
    "\n",
    "    metrics_rows.append({\n",
    "        \"threshold\": t,\n",
    "        \"accuracy_at_t\": accuracy_at_threshold(df_t, t),\n",
    "        \"coverage\": coverage(df_t, t),\n",
    "        \"penalty_mean\": penalty_adjusted_mean(df_t, t),\n",
    "        \"overconf_rate\": overconfidence_rate(df_t, t),\n",
    "        \"answered_n\": int((df_t[\"confidence\"] > t).sum()),\n",
    "        \"total_n\": len(df_t)\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_rows).sort_values(\"threshold\").reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "1e9cd275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4×4 Evaluation Table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>9.33</td>\n",
       "      <td>89.53</td>\n",
       "      <td>-20.62</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>7.01</td>\n",
       "      <td>87.29</td>\n",
       "      <td>-59.87</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>9.90</td>\n",
       "      <td>90.35</td>\n",
       "      <td>-184.05</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>6.76</td>\n",
       "      <td>88.71</td>\n",
       "      <td>-546.70</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "threshold                                                      \n",
       "0.25                9.33     89.53        -20.62             81\n",
       "0.50                7.01     87.29        -59.87             81\n",
       "0.75                9.90     90.35       -184.05             81\n",
       "0.90                6.76     88.71       -546.70             83"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results in: outputs\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 6 — 4×4 evaluation table =====\n",
    "eval_table = (\n",
    "    metrics_df[[\"threshold\", \"accuracy_at_t\", \"coverage\", \"penalty_mean\", \"overconf_rate\"]]\n",
    "    .set_index(\"threshold\")\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# Display\n",
    "print(\"4×4 Evaluation Table:\")\n",
    "display(eval_table)\n",
    "\n",
    "# Save both detailed and compact tables\n",
    "eval_table.to_csv(OUTPUT_PATH / \"llama-mmlu-metric-eval.csv\")\n",
    "\n",
    "print(f\"Saved results in: {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cfe38a",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "f5bed551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG — eligible rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>9.33</td>\n",
       "      <td>89.53</td>\n",
       "      <td>-20.62</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>7.01</td>\n",
       "      <td>87.29</td>\n",
       "      <td>-59.87</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>9.90</td>\n",
       "      <td>90.35</td>\n",
       "      <td>-184.05</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>6.76</td>\n",
       "      <td>88.71</td>\n",
       "      <td>-546.70</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "threshold                                                      \n",
       "0.25                9.33     89.53        -20.62             81\n",
       "0.50                7.01     87.29        -59.87             81\n",
       "0.75                9.90     90.35       -184.05             81\n",
       "0.90                6.76     88.71       -546.70             83"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected t* = 0.75\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "accuracy_at_t      9.90\n",
       "coverage          90.35\n",
       "penalty_mean    -184.05\n",
       "overconf_rate     81.00\n",
       "Name: 0.75, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "coverage_floor = 0.3\n",
    "\n",
    "eligible = eval_table[eval_table[\"coverage\"] >= coverage_floor]\n",
    "print(\"DEBUG — eligible rows:\")\n",
    "display(eligible)\n",
    "\n",
    "best_row = eligible.loc[eligible[\"accuracy_at_t\"].idxmax()]\n",
    "t_star = best_row.name\n",
    "\n",
    "print(f\"Selected t* = {t_star}\")\n",
    "display(best_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "02c2e9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary-grading baseline:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Binary</th>\n",
       "      <td>7.35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "Binary           7.35       1.0           NaN          92.65"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Always-abstain baseline:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Abstain</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "Abstain            0.0       0.0           0.0            0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Cell 2 – Compute binary-grading and always-abstain baselines =====\n",
    "import numpy as np\n",
    "\n",
    "# Load the original prediction CSV (same used to compute metrics_df)\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Compute overall accuracy\n",
    "binary_acc = np.mean(df[\"predicted_answer\"] == df[\"answer\"])\n",
    "\n",
    "# Wrong rate (for overconfidence)\n",
    "wrong_rate = 1 - binary_acc\n",
    "\n",
    "# Binary baseline metrics (answers everything)\n",
    "binary_row = {\n",
    "    \"accuracy_at_t\": round(100*binary_acc,2),\n",
    "    \"coverage\": 1.0,\n",
    "    \"penalty_mean\": np.nan,  # you can fill with projected penalty if desired\n",
    "    \"overconf_rate\": round(100*wrong_rate,2),\n",
    "}\n",
    "\n",
    "# Always-abstain baseline metrics\n",
    "abstain_row = {\n",
    "    \"accuracy_at_t\": 0.0,\n",
    "    \"coverage\": 0.0,\n",
    "    \"penalty_mean\": 0.0,\n",
    "    \"overconf_rate\": 0.0,\n",
    "}\n",
    "\n",
    "print(\"Binary-grading baseline:\")\n",
    "display(pd.DataFrame([binary_row], index=[\"Binary\"]))\n",
    "print(\"Always-abstain baseline:\")\n",
    "display(pd.DataFrame([abstain_row], index=[\"Abstain\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "59a99052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 3x4 Headline Evaluation Table ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Confidence-aware (t*=0.75)</th>\n",
       "      <td>9.90</td>\n",
       "      <td>90.35</td>\n",
       "      <td>-184.05</td>\n",
       "      <td>81.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Binary grading</th>\n",
       "      <td>7.35</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Always abstain</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            accuracy_at_t  coverage  penalty_mean  \\\n",
       "Confidence-aware (t*=0.75)           9.90     90.35       -184.05   \n",
       "Binary grading                       7.35      1.00           NaN   \n",
       "Always abstain                       0.00      0.00          0.00   \n",
       "\n",
       "                            overconf_rate  \n",
       "Confidence-aware (t*=0.75)          81.00  \n",
       "Binary grading                      92.65  \n",
       "Always abstain                       0.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Cell 3 – Build final 3x4 headline table =====\n",
    "\n",
    "headline_df = pd.DataFrame([\n",
    "    best_row[[\"accuracy_at_t\", \"coverage\", \"penalty_mean\", \"overconf_rate\"]],\n",
    "    pd.Series(binary_row),\n",
    "    pd.Series(abstain_row)\n",
    "], index=[f\"Confidence-aware (t*={t_star})\", \"Binary grading\", \"Always abstain\"])\n",
    "\n",
    "print(\"=== 3x4 Headline Evaluation Table ===\")\n",
    "display(headline_df)\n",
    "headline_df.to_csv(OUTPUT_PATH / \"llama-mmlu-baseline-eval.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c18581",
   "metadata": {},
   "source": [
    "## Gemini Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "4cae51bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPQA - Qwen Evalution \n",
    "CSV_PATH = Path(\"../inference/outputs/gemini-mmlu.csv\")  # <-- change this for each run\n",
    "OUTPUT_PATH = Path(\"outputs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "5c6288b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 3400 rows from gemini-mmlu.csv\n",
      "Thresholds found: [np.float64(0.25), np.float64(0.5), np.float64(0.75), np.float64(0.9)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>threshold</th>\n",
       "      <th>question</th>\n",
       "      <th>choices</th>\n",
       "      <th>answer</th>\n",
       "      <th>predicted_answer</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>596</td>\n",
       "      <td>0.25</td>\n",
       "      <td>A manufacturer is currently selling 2000 units...</td>\n",
       "      <td>['$2.50' '$1.90' '$2.70' '$2.60' '$1.80' '$2.2...</td>\n",
       "      <td>F</td>\n",
       "      <td>X</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>814</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Fred Lowes is a typewriter salesman. He receiv...</td>\n",
       "      <td>['$210' '$200' '$225' '$175' '$195' '$150' '$2...</td>\n",
       "      <td>I</td>\n",
       "      <td>S</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>817</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Mary Redmond purchased a $28,500 home with 20%...</td>\n",
       "      <td>['$305' '$190' '$171' '$285.50' '$399' '$323' ...</td>\n",
       "      <td>F</td>\n",
       "      <td>S</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  threshold                                           question  \\\n",
       "0  596       0.25  A manufacturer is currently selling 2000 units...   \n",
       "1  814       0.25  Fred Lowes is a typewriter salesman. He receiv...   \n",
       "2  817       0.25  Mary Redmond purchased a $28,500 home with 20%...   \n",
       "\n",
       "                                             choices answer predicted_answer  \\\n",
       "0  ['$2.50' '$1.90' '$2.70' '$2.60' '$1.80' '$2.2...      F                X   \n",
       "1  ['$210' '$200' '$225' '$175' '$195' '$150' '$2...      I                S   \n",
       "2  ['$305' '$190' '$171' '$285.50' '$399' '$323' ...      F                S   \n",
       "\n",
       "   confidence  \n",
       "0       100.0  \n",
       "1       100.0  \n",
       "2       100.0  "
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Normalize strings & types\n",
    "df[\"answer\"] = df[\"answer\"].astype(str).str.strip().str.upper()\n",
    "df[\"predicted_answer\"] = df[\"predicted_answer\"].astype(str).str.strip().str.upper()\n",
    "df[\"confidence\"] = round(100*pd.to_numeric(df[\"confidence\"], errors=\"coerce\").fillna(0.0),2)\n",
    "df[\"threshold\"] = pd.to_numeric(df[\"threshold\"], errors=\"coerce\")\n",
    "\n",
    "print(f\"✅ Loaded {len(df)} rows from {CSV_PATH.name}\")\n",
    "print(\"Thresholds found:\", sorted(df[\"threshold\"].unique()))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "f0828fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=0.25: 850 rows\n",
      "t=0.5: 850 rows\n",
      "t=0.75: 850 rows\n",
      "t=0.9: 850 rows\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 4 — Split into per-threshold DataFrames =====\n",
    "dfs_by_t = {t: df[df[\"threshold\"] == t].copy() for t in THRESHOLDS}\n",
    "\n",
    "for t in THRESHOLDS:\n",
    "    print(f\"t={t}: {len(dfs_by_t[t])} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "1d96f27c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "      <th>answered_n</th>\n",
       "      <th>total_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.25</td>\n",
       "      <td>12.35</td>\n",
       "      <td>67.65</td>\n",
       "      <td>-17.01</td>\n",
       "      <td>59</td>\n",
       "      <td>575</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.50</td>\n",
       "      <td>12.73</td>\n",
       "      <td>90.59</td>\n",
       "      <td>-67.47</td>\n",
       "      <td>79</td>\n",
       "      <td>770</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.75</td>\n",
       "      <td>11.95</td>\n",
       "      <td>90.59</td>\n",
       "      <td>-205.99</td>\n",
       "      <td>80</td>\n",
       "      <td>770</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.90</td>\n",
       "      <td>12.73</td>\n",
       "      <td>90.59</td>\n",
       "      <td>-605.76</td>\n",
       "      <td>79</td>\n",
       "      <td>770</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   threshold  accuracy_at_t  coverage  penalty_mean  overconf_rate  \\\n",
       "0       0.25          12.35     67.65        -17.01             59   \n",
       "1       0.50          12.73     90.59        -67.47             79   \n",
       "2       0.75          11.95     90.59       -205.99             80   \n",
       "3       0.90          12.73     90.59       -605.76             79   \n",
       "\n",
       "   answered_n  total_n  \n",
       "0         575      850  \n",
       "1         770      850  \n",
       "2         770      850  \n",
       "3         770      850  "
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== Cell 5 — Compute metrics per threshold =====\n",
    "metrics_rows = []\n",
    "\n",
    "for t in THRESHOLDS:\n",
    "    df_t = dfs_by_t[t]\n",
    "\n",
    "    metrics_rows.append({\n",
    "        \"threshold\": t,\n",
    "        \"accuracy_at_t\": accuracy_at_threshold(df_t, t),\n",
    "        \"coverage\": coverage(df_t, t),\n",
    "        \"penalty_mean\": penalty_adjusted_mean(df_t, t),\n",
    "        \"overconf_rate\": overconfidence_rate(df_t, t),\n",
    "        \"answered_n\": int((df_t[\"confidence\"] > t).sum()),\n",
    "        \"total_n\": len(df_t)\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_rows).sort_values(\"threshold\").reset_index(drop=True)\n",
    "metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "69297fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4×4 Evaluation Table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>12.35</td>\n",
       "      <td>67.65</td>\n",
       "      <td>-17.01</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>12.73</td>\n",
       "      <td>90.59</td>\n",
       "      <td>-67.47</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>11.95</td>\n",
       "      <td>90.59</td>\n",
       "      <td>-205.99</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>12.73</td>\n",
       "      <td>90.59</td>\n",
       "      <td>-605.76</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "threshold                                                      \n",
       "0.25               12.35     67.65        -17.01             59\n",
       "0.50               12.73     90.59        -67.47             79\n",
       "0.75               11.95     90.59       -205.99             80\n",
       "0.90               12.73     90.59       -605.76             79"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results in: outputs\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 6 — 4×4 evaluation table =====\n",
    "eval_table = (\n",
    "    metrics_df[[\"threshold\", \"accuracy_at_t\", \"coverage\", \"penalty_mean\", \"overconf_rate\"]]\n",
    "    .set_index(\"threshold\")\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# Display\n",
    "print(\"4×4 Evaluation Table:\")\n",
    "display(eval_table)\n",
    "\n",
    "# Save both detailed and compact tables\n",
    "eval_table.to_csv(OUTPUT_PATH / \"gemini-mmlu-metric-eval.csv\")\n",
    "\n",
    "print(f\"Saved results in: {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b35a66",
   "metadata": {},
   "source": [
    "### Baseline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "bbaa912a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG — eligible rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>12.35</td>\n",
       "      <td>67.65</td>\n",
       "      <td>-17.01</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>12.73</td>\n",
       "      <td>90.59</td>\n",
       "      <td>-67.47</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>11.95</td>\n",
       "      <td>90.59</td>\n",
       "      <td>-205.99</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>12.73</td>\n",
       "      <td>90.59</td>\n",
       "      <td>-605.76</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "threshold                                                      \n",
       "0.25               12.35     67.65        -17.01             59\n",
       "0.50               12.73     90.59        -67.47             79\n",
       "0.75               11.95     90.59       -205.99             80\n",
       "0.90               12.73     90.59       -605.76             79"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected t* = 0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "accuracy_at_t    12.73\n",
       "coverage         90.59\n",
       "penalty_mean    -67.47\n",
       "overconf_rate    79.00\n",
       "Name: 0.5, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "coverage_floor = 0.3\n",
    "\n",
    "eligible = eval_table[eval_table[\"coverage\"] >= coverage_floor]\n",
    "print(\"DEBUG — eligible rows:\")\n",
    "display(eligible)\n",
    "\n",
    "best_row = eligible.loc[eligible[\"accuracy_at_t\"].idxmax()]\n",
    "t_star = best_row.name\n",
    "\n",
    "print(f\"Selected t* = {t_star}\")\n",
    "display(best_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "8fd7728d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary-grading baseline:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Binary</th>\n",
       "      <td>10.56</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>89.44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "Binary          10.56       1.0           NaN          89.44"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Always-abstain baseline:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Abstain</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "Abstain            0.0       0.0           0.0            0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Cell 2 – Compute binary-grading and always-abstain baselines =====\n",
    "import numpy as np\n",
    "\n",
    "# Load the original prediction CSV (same used to compute metrics_df)\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Compute overall accuracy\n",
    "binary_acc = np.mean(df[\"predicted_answer\"] == df[\"answer\"])\n",
    "\n",
    "# Wrong rate (for overconfidence)\n",
    "wrong_rate = 1 - binary_acc\n",
    "\n",
    "# Binary baseline metrics (answers everything)\n",
    "binary_row = {\n",
    "    \"accuracy_at_t\": round(100*binary_acc,2),\n",
    "    \"coverage\": 1.0,\n",
    "    \"penalty_mean\": np.nan,  # you can fill with projected penalty if desired\n",
    "    \"overconf_rate\": round(100*wrong_rate,2),\n",
    "}\n",
    "\n",
    "# Always-abstain baseline metrics\n",
    "abstain_row = {\n",
    "    \"accuracy_at_t\": 0.0,\n",
    "    \"coverage\": 0.0,\n",
    "    \"penalty_mean\": 0.0,\n",
    "    \"overconf_rate\": 0.0,\n",
    "}\n",
    "\n",
    "print(\"Binary-grading baseline:\")\n",
    "display(pd.DataFrame([binary_row], index=[\"Binary\"]))\n",
    "print(\"Always-abstain baseline:\")\n",
    "display(pd.DataFrame([abstain_row], index=[\"Abstain\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "41bc13df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 3x4 Headline Evaluation Table ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Confidence-aware (t*=0.5)</th>\n",
       "      <td>12.73</td>\n",
       "      <td>90.59</td>\n",
       "      <td>-67.47</td>\n",
       "      <td>79.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Binary grading</th>\n",
       "      <td>10.56</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>89.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Always abstain</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           accuracy_at_t  coverage  penalty_mean  \\\n",
       "Confidence-aware (t*=0.5)          12.73     90.59        -67.47   \n",
       "Binary grading                     10.56      1.00           NaN   \n",
       "Always abstain                      0.00      0.00          0.00   \n",
       "\n",
       "                           overconf_rate  \n",
       "Confidence-aware (t*=0.5)          79.00  \n",
       "Binary grading                     89.44  \n",
       "Always abstain                      0.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Cell 3 – Build final 3x4 headline table =====\n",
    "\n",
    "headline_df = pd.DataFrame([\n",
    "    best_row[[\"accuracy_at_t\", \"coverage\", \"penalty_mean\", \"overconf_rate\"]],\n",
    "    pd.Series(binary_row),\n",
    "    pd.Series(abstain_row)\n",
    "], index=[f\"Confidence-aware (t*={t_star})\", \"Binary grading\", \"Always abstain\"])\n",
    "\n",
    "print(\"=== 3x4 Headline Evaluation Table ===\")\n",
    "display(headline_df)\n",
    "headline_df.to_csv(OUTPUT_PATH / \"gemini-mmlu-baseline-eval.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a937e888",
   "metadata": {},
   "source": [
    "## Mistral Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "779f4cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPQA - Qwen Evalution \n",
    "CSV_PATH = Path(\"../inference/outputs/mistral-mmlu.csv\")  # <-- change this for each run\n",
    "OUTPUT_PATH = Path(\"outputs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "eb63118c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 3400 rows from mistral-mmlu.csv\n",
      "Thresholds found: [np.float64(0.25), np.float64(0.5), np.float64(0.75), np.float64(0.9)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>threshold</th>\n",
       "      <th>question</th>\n",
       "      <th>choices</th>\n",
       "      <th>answer</th>\n",
       "      <th>predicted_answer</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>596</td>\n",
       "      <td>0.25</td>\n",
       "      <td>A manufacturer is currently selling 2000 units...</td>\n",
       "      <td>['$2.50' '$1.90' '$2.70' '$2.60' '$1.80' '$2.2...</td>\n",
       "      <td>F</td>\n",
       "      <td>IDK</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>814</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Fred Lowes is a typewriter salesman. He receiv...</td>\n",
       "      <td>['$210' '$200' '$225' '$175' '$195' '$150' '$2...</td>\n",
       "      <td>I</td>\n",
       "      <td>A</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>817</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Mary Redmond purchased a $28,500 home with 20%...</td>\n",
       "      <td>['$305' '$190' '$171' '$285.50' '$399' '$323' ...</td>\n",
       "      <td>F</td>\n",
       "      <td>IDK</td>\n",
       "      <td>83.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  threshold                                           question  \\\n",
       "0  596       0.25  A manufacturer is currently selling 2000 units...   \n",
       "1  814       0.25  Fred Lowes is a typewriter salesman. He receiv...   \n",
       "2  817       0.25  Mary Redmond purchased a $28,500 home with 20%...   \n",
       "\n",
       "                                             choices answer predicted_answer  \\\n",
       "0  ['$2.50' '$1.90' '$2.70' '$2.60' '$1.80' '$2.2...      F              IDK   \n",
       "1  ['$210' '$200' '$225' '$175' '$195' '$150' '$2...      I                A   \n",
       "2  ['$305' '$190' '$171' '$285.50' '$399' '$323' ...      F              IDK   \n",
       "\n",
       "   confidence  \n",
       "0      100.00  \n",
       "1      100.00  \n",
       "2       83.33  "
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Normalize strings & types\n",
    "df[\"answer\"] = df[\"answer\"].astype(str).str.strip().str.upper()\n",
    "df[\"predicted_answer\"] = df[\"predicted_answer\"].astype(str).str.strip().str.upper()\n",
    "df[\"confidence\"] = round(100*pd.to_numeric(df[\"confidence\"], errors=\"coerce\").fillna(0.0),2)\n",
    "df[\"threshold\"] = pd.to_numeric(df[\"threshold\"], errors=\"coerce\")\n",
    "\n",
    "print(f\"✅ Loaded {len(df)} rows from {CSV_PATH.name}\")\n",
    "print(\"Thresholds found:\", sorted(df[\"threshold\"].unique()))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "8cd599f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=0.25: 850 rows\n",
      "t=0.5: 850 rows\n",
      "t=0.75: 850 rows\n",
      "t=0.9: 850 rows\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 4 — Split into per-threshold DataFrames =====\n",
    "dfs_by_t = {t: df[df[\"threshold\"] == t].copy() for t in THRESHOLDS}\n",
    "\n",
    "for t in THRESHOLDS:\n",
    "    print(f\"t={t}: {len(dfs_by_t[t])} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "62ba2cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "      <th>answered_n</th>\n",
       "      <th>total_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.25</td>\n",
       "      <td>8.99</td>\n",
       "      <td>98.12</td>\n",
       "      <td>-24.44</td>\n",
       "      <td>89</td>\n",
       "      <td>834</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.50</td>\n",
       "      <td>8.86</td>\n",
       "      <td>98.24</td>\n",
       "      <td>-74.80</td>\n",
       "      <td>90</td>\n",
       "      <td>835</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.75</td>\n",
       "      <td>8.86</td>\n",
       "      <td>98.24</td>\n",
       "      <td>-222.95</td>\n",
       "      <td>90</td>\n",
       "      <td>835</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.90</td>\n",
       "      <td>8.82</td>\n",
       "      <td>98.71</td>\n",
       "      <td>-682.45</td>\n",
       "      <td>90</td>\n",
       "      <td>839</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   threshold  accuracy_at_t  coverage  penalty_mean  overconf_rate  \\\n",
       "0       0.25           8.99     98.12        -24.44             89   \n",
       "1       0.50           8.86     98.24        -74.80             90   \n",
       "2       0.75           8.86     98.24       -222.95             90   \n",
       "3       0.90           8.82     98.71       -682.45             90   \n",
       "\n",
       "   answered_n  total_n  \n",
       "0         834      850  \n",
       "1         835      850  \n",
       "2         835      850  \n",
       "3         839      850  "
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== Cell 5 — Compute metrics per threshold =====\n",
    "metrics_rows = []\n",
    "\n",
    "for t in THRESHOLDS:\n",
    "    df_t = dfs_by_t[t]\n",
    "\n",
    "    metrics_rows.append({\n",
    "        \"threshold\": t,\n",
    "        \"accuracy_at_t\": accuracy_at_threshold(df_t, t),\n",
    "        \"coverage\": coverage(df_t, t),\n",
    "        \"penalty_mean\": penalty_adjusted_mean(df_t, t),\n",
    "        \"overconf_rate\": overconfidence_rate(df_t, t),\n",
    "        \"answered_n\": int((df_t[\"confidence\"] > t).sum()),\n",
    "        \"total_n\": len(df_t)\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_rows).sort_values(\"threshold\").reset_index(drop=True)\n",
    "metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "e674e8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4×4 Evaluation Table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>8.99</td>\n",
       "      <td>98.12</td>\n",
       "      <td>-24.44</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>8.86</td>\n",
       "      <td>98.24</td>\n",
       "      <td>-74.80</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>8.86</td>\n",
       "      <td>98.24</td>\n",
       "      <td>-222.95</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>8.82</td>\n",
       "      <td>98.71</td>\n",
       "      <td>-682.45</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "threshold                                                      \n",
       "0.25                8.99     98.12        -24.44             89\n",
       "0.50                8.86     98.24        -74.80             90\n",
       "0.75                8.86     98.24       -222.95             90\n",
       "0.90                8.82     98.71       -682.45             90"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results in: outputs\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 6 — 4×4 evaluation table =====\n",
    "eval_table = (\n",
    "    metrics_df[[\"threshold\", \"accuracy_at_t\", \"coverage\", \"penalty_mean\", \"overconf_rate\"]]\n",
    "    .set_index(\"threshold\")\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# Display\n",
    "print(\"4×4 Evaluation Table:\")\n",
    "display(eval_table)\n",
    "\n",
    "# Save both detailed and compact tables\n",
    "eval_table.to_csv(OUTPUT_PATH / \"mistral-mmlu-metric-eval.csv\")\n",
    "\n",
    "print(f\"Saved results in: {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b87d790",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "8f7a5e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG — eligible rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>8.99</td>\n",
       "      <td>98.12</td>\n",
       "      <td>-24.44</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>8.86</td>\n",
       "      <td>98.24</td>\n",
       "      <td>-74.80</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>8.86</td>\n",
       "      <td>98.24</td>\n",
       "      <td>-222.95</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>8.82</td>\n",
       "      <td>98.71</td>\n",
       "      <td>-682.45</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "threshold                                                      \n",
       "0.25                8.99     98.12        -24.44             89\n",
       "0.50                8.86     98.24        -74.80             90\n",
       "0.75                8.86     98.24       -222.95             90\n",
       "0.90                8.82     98.71       -682.45             90"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected t* = 0.25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "accuracy_at_t     8.99\n",
       "coverage         98.12\n",
       "penalty_mean    -24.44\n",
       "overconf_rate    89.00\n",
       "Name: 0.25, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "coverage_floor = 0.3\n",
    "\n",
    "eligible = eval_table[eval_table[\"coverage\"] >= coverage_floor]\n",
    "print(\"DEBUG — eligible rows:\")\n",
    "display(eligible)\n",
    "\n",
    "best_row = eligible.loc[eligible[\"accuracy_at_t\"].idxmax()]\n",
    "t_star = best_row.name\n",
    "\n",
    "print(f\"Selected t* = {t_star}\")\n",
    "display(best_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "9309b56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary-grading baseline:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Binary</th>\n",
       "      <td>8.74</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>91.26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "Binary           8.74       1.0           NaN          91.26"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Always-abstain baseline:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Abstain</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "Abstain            0.0       0.0           0.0            0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Cell 2 – Compute binary-grading and always-abstain baselines =====\n",
    "import numpy as np\n",
    "\n",
    "# Load the original prediction CSV (same used to compute metrics_df)\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Compute overall accuracy\n",
    "binary_acc = np.mean(df[\"predicted_answer\"] == df[\"answer\"])\n",
    "\n",
    "# Wrong rate (for overconfidence)\n",
    "wrong_rate = 1 - binary_acc\n",
    "\n",
    "# Binary baseline metrics (answers everything)\n",
    "binary_row = {\n",
    "    \"accuracy_at_t\": round(100*binary_acc,2),\n",
    "    \"coverage\": 1.0,\n",
    "    \"penalty_mean\": np.nan,  # you can fill with projected penalty if desired\n",
    "    \"overconf_rate\": round(100*wrong_rate,2),\n",
    "}\n",
    "\n",
    "# Always-abstain baseline metrics\n",
    "abstain_row = {\n",
    "    \"accuracy_at_t\": 0.0,\n",
    "    \"coverage\": 0.0,\n",
    "    \"penalty_mean\": 0.0,\n",
    "    \"overconf_rate\": 0.0,\n",
    "}\n",
    "\n",
    "print(\"Binary-grading baseline:\")\n",
    "display(pd.DataFrame([binary_row], index=[\"Binary\"]))\n",
    "print(\"Always-abstain baseline:\")\n",
    "display(pd.DataFrame([abstain_row], index=[\"Abstain\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "97a529e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 3x4 Headline Evaluation Table ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Confidence-aware (t*=0.25)</th>\n",
       "      <td>8.99</td>\n",
       "      <td>98.12</td>\n",
       "      <td>-24.44</td>\n",
       "      <td>89.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Binary grading</th>\n",
       "      <td>8.74</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>91.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Always abstain</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            accuracy_at_t  coverage  penalty_mean  \\\n",
       "Confidence-aware (t*=0.25)           8.99     98.12        -24.44   \n",
       "Binary grading                       8.74      1.00           NaN   \n",
       "Always abstain                       0.00      0.00          0.00   \n",
       "\n",
       "                            overconf_rate  \n",
       "Confidence-aware (t*=0.25)          89.00  \n",
       "Binary grading                      91.26  \n",
       "Always abstain                       0.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Cell 3 – Build final 3x4 headline table =====\n",
    "\n",
    "headline_df = pd.DataFrame([\n",
    "    best_row[[\"accuracy_at_t\", \"coverage\", \"penalty_mean\", \"overconf_rate\"]],\n",
    "    pd.Series(binary_row),\n",
    "    pd.Series(abstain_row)\n",
    "], index=[f\"Confidence-aware (t*={t_star})\", \"Binary grading\", \"Always abstain\"])\n",
    "\n",
    "print(\"=== 3x4 Headline Evaluation Table ===\")\n",
    "display(headline_df)\n",
    "headline_df.to_csv(OUTPUT_PATH / \"mistral-mmlu-baseline-eval.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
