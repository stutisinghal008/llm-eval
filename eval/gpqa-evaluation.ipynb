{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22de04fa",
   "metadata": {},
   "source": [
    "## Imports, Setup, Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "39cd8f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Common thresholds (used for all model–dataset combinations)\n",
    "THRESHOLDS = [0.25, 0.5, 0.75, 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e562bdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 2 — Metric functions =====\n",
    "\n",
    "def decide(p: float, t: float) -> bool:\n",
    "    \"\"\"\n",
    "    Decision rule: returns True if the model 'answers' the question.\n",
    "    We assume the model only answers when its confidence p exceeds the threshold t.\n",
    "    \"\"\"\n",
    "    return p > t\n",
    "\n",
    "\n",
    "def penalty_score(pred: str, gold: str, p: float, t: float) -> float:\n",
    "    \"\"\"\n",
    "    Penalty-adjusted score (as defined in the paper/proposal):\n",
    "      - If p <= t  → the model abstains → score = 0.\n",
    "      - If p > t and prediction == gold → score = +1.\n",
    "      - If p > t and prediction != gold → score = - (p * t) / (1 - t)\n",
    "        (this is a confidence-proportional penalty for being overconfident).\n",
    "    \"\"\"\n",
    "    if p <= t:\n",
    "        return 0.0\n",
    "    if pred == gold:\n",
    "        return 1.0\n",
    "    return - (p * t) / (1.0 - t)\n",
    "\n",
    "\n",
    "#Metric 1\n",
    "def accuracy_at_threshold(df: pd.DataFrame, t: float) -> float:\n",
    "    \"\"\"\n",
    "    Accuracy@t:\n",
    "      Fraction of *answered* questions that are correct.\n",
    "      = (# correct with p>t) / (# answered with p>t)\n",
    "    If the model abstains on all (no p>t), returns 0.0.\n",
    "    \"\"\"\n",
    "    answered = df[\"confidence\"] > t\n",
    "    answered_n = answered.sum()\n",
    "    if answered_n == 0:\n",
    "        return 0.0\n",
    "    correct = (df[\"predicted_answer\"] == df[\"answer\"]) & answered\n",
    "    return float(correct.sum() / answered_n)\n",
    "\n",
    "#Metric 2\n",
    "def coverage(df: pd.DataFrame, t: float) -> float:\n",
    "    \"\"\"\n",
    "    Coverage:\n",
    "      Fraction of total questions that the model *answers*.\n",
    "      = (# p>t) / total\n",
    "    \"\"\"\n",
    "    if len(df) == 0:\n",
    "        return 0.0\n",
    "    return float((df[\"confidence\"] > t).sum() / len(df))\n",
    "\n",
    "#Metric 3\n",
    "def penalty_adjusted_mean(df: pd.DataFrame, t: float) -> float:\n",
    "    \"\"\"\n",
    "    Mean penalty-adjusted score across all rows (including abstains).\n",
    "    Abstentions contribute 0.\n",
    "    \"\"\"\n",
    "    scores = [\n",
    "        penalty_score(r.predicted_answer, r.answer, float(r.confidence), t)\n",
    "        for r in df.itertuples(index=False)\n",
    "    ]\n",
    "    return float(np.mean(scores)) if scores else 0.0\n",
    "\n",
    "#Metric 4\n",
    "def overconfidence_rate(df: pd.DataFrame, t: float) -> float:\n",
    "    \"\"\"\n",
    "    Overconfidence rate:\n",
    "      Fraction of questions where the model is *wrong* but still confident (p>t).\n",
    "      = (# wrong & p>t) / total\n",
    "    \"\"\"\n",
    "    if len(df) == 0:\n",
    "        return 0.0\n",
    "    mask = (df[\"predicted_answer\"] != df[\"answer\"]) & (df[\"confidence\"] > t)\n",
    "    return float(mask.sum() / len(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8856ef",
   "metadata": {},
   "source": [
    "## Qwen Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "05093db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = Path(\"../inference/outputs/qwen-gpqa.csv\")  # <-- change this for each run\n",
    "OUTPUT_PATH = Path(\"outputs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "45874a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1020 rows from qwen-gpqa.csv\n",
      "Thresholds found: [np.float64(0.25), np.float64(0.5), np.float64(0.75), np.float64(0.9)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>threshold</th>\n",
       "      <th>question</th>\n",
       "      <th>choices</th>\n",
       "      <th>answer</th>\n",
       "      <th>predicted_answer</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>203</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Identify the correct sequence of reagents for ...</td>\n",
       "      <td>['1. NaH; CH3CH2Br 2. H2SO4, HNO3 3. Fe-HCl 4....</td>\n",
       "      <td>D</td>\n",
       "      <td>IDK</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>266</td>\n",
       "      <td>0.25</td>\n",
       "      <td>There is a C-NOT gate where the condition is t...</td>\n",
       "      <td>['U_{C-NOT}\\\\left|\\\\psi\\\\right\\\\rangle =\\\\alph...</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>152</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Two stars are being studied. It has been obser...</td>\n",
       "      <td>['ln(2) = [ (T_1 - T_2) / (T1*T2)]', 'ln(2) = ...</td>\n",
       "      <td>A</td>\n",
       "      <td>IDK</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  threshold                                           question  \\\n",
       "0  203       0.25  Identify the correct sequence of reagents for ...   \n",
       "1  266       0.25  There is a C-NOT gate where the condition is t...   \n",
       "2  152       0.25  Two stars are being studied. It has been obser...   \n",
       "\n",
       "                                             choices answer predicted_answer  \\\n",
       "0  ['1. NaH; CH3CH2Br 2. H2SO4, HNO3 3. Fe-HCl 4....      D              IDK   \n",
       "1  ['U_{C-NOT}\\\\left|\\\\psi\\\\right\\\\rangle =\\\\alph...      A                B   \n",
       "2  ['ln(2) = [ (T_1 - T_2) / (T1*T2)]', 'ln(2) = ...      A              IDK   \n",
       "\n",
       "   confidence  \n",
       "0    0.666667  \n",
       "1    1.000000  \n",
       "2    1.000000  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Normalize strings & types\n",
    "df[\"answer\"] = df[\"answer\"].astype(str).str.strip().str.upper()\n",
    "df[\"predicted_answer\"] = df[\"predicted_answer\"].astype(str).str.strip().str.upper()\n",
    "df[\"confidence\"] = pd.to_numeric(df[\"confidence\"], errors=\"coerce\").fillna(0.0)\n",
    "df[\"threshold\"] = pd.to_numeric(df[\"threshold\"], errors=\"coerce\")\n",
    "\n",
    "print(f\"✅ Loaded {len(df)} rows from {CSV_PATH.name}\")\n",
    "print(\"Thresholds found:\", sorted(df[\"threshold\"].unique()))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "72eb218d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=0.25: 255 rows\n",
      "t=0.5: 255 rows\n",
      "t=0.75: 255 rows\n",
      "t=0.9: 255 rows\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 4 — Split into per-threshold DataFrames =====\n",
    "dfs_by_t = {t: df[df[\"threshold\"] == t].copy() for t in THRESHOLDS}\n",
    "\n",
    "for t in THRESHOLDS:\n",
    "    print(f\"t={t}: {len(dfs_by_t[t])} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d60390b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "      <th>answered_n</th>\n",
       "      <th>total_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.057377</td>\n",
       "      <td>0.956863</td>\n",
       "      <td>-0.228758</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>244</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.043860</td>\n",
       "      <td>0.894118</td>\n",
       "      <td>-0.784314</td>\n",
       "      <td>0.854902</td>\n",
       "      <td>228</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.046296</td>\n",
       "      <td>0.847059</td>\n",
       "      <td>-2.358824</td>\n",
       "      <td>0.807843</td>\n",
       "      <td>216</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.045685</td>\n",
       "      <td>0.772549</td>\n",
       "      <td>-6.600000</td>\n",
       "      <td>0.737255</td>\n",
       "      <td>197</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   threshold  accuracy_at_t  coverage  penalty_mean  overconf_rate  \\\n",
       "0       0.25       0.057377  0.956863     -0.228758       0.901961   \n",
       "1       0.50       0.043860  0.894118     -0.784314       0.854902   \n",
       "2       0.75       0.046296  0.847059     -2.358824       0.807843   \n",
       "3       0.90       0.045685  0.772549     -6.600000       0.737255   \n",
       "\n",
       "   answered_n  total_n  \n",
       "0         244      255  \n",
       "1         228      255  \n",
       "2         216      255  \n",
       "3         197      255  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== Cell 5 — Compute metrics per threshold =====\n",
    "metrics_rows = []\n",
    "\n",
    "for t in THRESHOLDS:\n",
    "    df_t = dfs_by_t[t]\n",
    "\n",
    "    metrics_rows.append({\n",
    "        \"threshold\": t,\n",
    "        \"accuracy_at_t\": accuracy_at_threshold(df_t, t),\n",
    "        \"coverage\": coverage(df_t, t),\n",
    "        \"penalty_mean\": penalty_adjusted_mean(df_t, t),\n",
    "        \"overconf_rate\": overconfidence_rate(df_t, t),\n",
    "        \"answered_n\": int((df_t[\"confidence\"] > t).sum()),\n",
    "        \"total_n\": len(df_t)\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_rows).sort_values(\"threshold\").reset_index(drop=True)\n",
    "metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "73099b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4×4 Evaluation Table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.057377</td>\n",
       "      <td>0.956863</td>\n",
       "      <td>-0.228758</td>\n",
       "      <td>0.901961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.043860</td>\n",
       "      <td>0.894118</td>\n",
       "      <td>-0.784314</td>\n",
       "      <td>0.854902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.046296</td>\n",
       "      <td>0.847059</td>\n",
       "      <td>-2.358824</td>\n",
       "      <td>0.807843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>0.045685</td>\n",
       "      <td>0.772549</td>\n",
       "      <td>-6.600000</td>\n",
       "      <td>0.737255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "threshold                                                      \n",
       "0.25            0.057377  0.956863     -0.228758       0.901961\n",
       "0.50            0.043860  0.894118     -0.784314       0.854902\n",
       "0.75            0.046296  0.847059     -2.358824       0.807843\n",
       "0.90            0.045685  0.772549     -6.600000       0.737255"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results in: outputs\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 6 — 4×4 evaluation table =====\n",
    "eval_table = (\n",
    "    metrics_df[[\"threshold\", \"accuracy_at_t\", \"coverage\", \"penalty_mean\", \"overconf_rate\"]]\n",
    "    .set_index(\"threshold\")\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# Display\n",
    "print(\"4×4 Evaluation Table:\")\n",
    "display(eval_table)\n",
    "\n",
    "# Save both detailed and compact tables\n",
    "eval_table.to_csv(OUTPUT_PATH / \"qwen-gpqa-metric-eval.csv\")\n",
    "\n",
    "print(f\"Saved results in: {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefbf7a9",
   "metadata": {},
   "source": [
    "## Baseline Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "441e6909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG — eligible rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.057377</td>\n",
       "      <td>0.956863</td>\n",
       "      <td>-0.228758</td>\n",
       "      <td>0.901961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.043860</td>\n",
       "      <td>0.894118</td>\n",
       "      <td>-0.784314</td>\n",
       "      <td>0.854902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.046296</td>\n",
       "      <td>0.847059</td>\n",
       "      <td>-2.358824</td>\n",
       "      <td>0.807843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>0.045685</td>\n",
       "      <td>0.772549</td>\n",
       "      <td>-6.600000</td>\n",
       "      <td>0.737255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "threshold                                                      \n",
       "0.25            0.057377  0.956863     -0.228758       0.901961\n",
       "0.50            0.043860  0.894118     -0.784314       0.854902\n",
       "0.75            0.046296  0.847059     -2.358824       0.807843\n",
       "0.90            0.045685  0.772549     -6.600000       0.737255"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected t* = 0.25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "accuracy_at_t    0.057377\n",
       "coverage         0.956863\n",
       "penalty_mean    -0.228758\n",
       "overconf_rate    0.901961\n",
       "Name: 0.25, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "coverage_floor = 0.3\n",
    "\n",
    "eligible = eval_table[eval_table[\"coverage\"] >= coverage_floor]\n",
    "print(\"DEBUG — eligible rows:\")\n",
    "display(eligible)\n",
    "\n",
    "best_row = eligible.loc[eligible[\"accuracy_at_t\"].idxmax()]\n",
    "t_star = best_row.name\n",
    "\n",
    "print(f\"Selected t* = {t_star}\")\n",
    "display(best_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "369ec9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary-grading baseline:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Binary</th>\n",
       "      <td>0.051961</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.948039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "Binary       0.051961       1.0           NaN       0.948039"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Always-abstain baseline:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Abstain</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "Abstain            0.0       0.0           0.0            0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Cell 2 – Compute binary-grading and always-abstain baselines =====\n",
    "import numpy as np\n",
    "\n",
    "# Load the original prediction CSV (same used to compute metrics_df)\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Compute overall accuracy\n",
    "binary_acc = np.mean(df[\"predicted_answer\"] == df[\"answer\"])\n",
    "\n",
    "# Wrong rate (for overconfidence)\n",
    "wrong_rate = 1 - binary_acc\n",
    "\n",
    "# Binary baseline metrics (answers everything)\n",
    "binary_row = {\n",
    "    \"accuracy_at_t\": binary_acc,\n",
    "    \"coverage\": 1.0,\n",
    "    \"penalty_mean\": np.nan,  # you can fill with projected penalty if desired\n",
    "    \"overconf_rate\": wrong_rate,\n",
    "}\n",
    "\n",
    "# Always-abstain baseline metrics\n",
    "abstain_row = {\n",
    "    \"accuracy_at_t\": 0.0,\n",
    "    \"coverage\": 0.0,\n",
    "    \"penalty_mean\": 0.0,\n",
    "    \"overconf_rate\": 0.0,\n",
    "}\n",
    "\n",
    "print(\"Binary-grading baseline:\")\n",
    "display(pd.DataFrame([binary_row], index=[\"Binary\"]))\n",
    "print(\"Always-abstain baseline:\")\n",
    "display(pd.DataFrame([abstain_row], index=[\"Abstain\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "618b417a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 3x4 Headline Evaluation Table ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Confidence-aware (t*=0.25)</th>\n",
       "      <td>0.057377</td>\n",
       "      <td>0.956863</td>\n",
       "      <td>-0.228758</td>\n",
       "      <td>0.901961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Binary grading</th>\n",
       "      <td>0.051961</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.948039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Always abstain</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            accuracy_at_t  coverage  penalty_mean  \\\n",
       "Confidence-aware (t*=0.25)       0.057377  0.956863     -0.228758   \n",
       "Binary grading                   0.051961  1.000000           NaN   \n",
       "Always abstain                   0.000000  0.000000      0.000000   \n",
       "\n",
       "                            overconf_rate  \n",
       "Confidence-aware (t*=0.25)       0.901961  \n",
       "Binary grading                   0.948039  \n",
       "Always abstain                   0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Cell 3 – Build final 3x4 headline table =====\n",
    "\n",
    "headline_df = pd.DataFrame([\n",
    "    best_row[[\"accuracy_at_t\", \"coverage\", \"penalty_mean\", \"overconf_rate\"]],\n",
    "    pd.Series(binary_row),\n",
    "    pd.Series(abstain_row)\n",
    "], index=[f\"Confidence-aware (t*={t_star})\", \"Binary grading\", \"Always abstain\"])\n",
    "\n",
    "print(\"=== 3x4 Headline Evaluation Table ===\")\n",
    "display(headline_df)\n",
    "headline_df.to_csv(OUTPUT_PATH / \"qwen-gpqa-baseline-eval.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf67fdc",
   "metadata": {},
   "source": [
    "## GPT Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "61d50c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = Path(\"../inference/outputs/gpt-gpqa.csv\")  # <-- change this for each run\n",
    "OUTPUT_PATH = Path(\"outputs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7b39f5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1020 rows from gpt-gpqa.csv\n",
      "Thresholds found: [np.float64(0.25), np.float64(0.5), np.float64(0.75), np.float64(0.9)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>threshold</th>\n",
       "      <th>question</th>\n",
       "      <th>choices</th>\n",
       "      <th>answer</th>\n",
       "      <th>predicted_answer</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>203</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Identify the correct sequence of reagents for ...</td>\n",
       "      <td>['1. NaH; CH3CH2Br 2. H2SO4, HNO3 3. Fe-HCl 4....</td>\n",
       "      <td>D</td>\n",
       "      <td>B</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>266</td>\n",
       "      <td>0.25</td>\n",
       "      <td>There is a C-NOT gate where the condition is t...</td>\n",
       "      <td>['U_{C-NOT}\\\\left|\\\\psi\\\\right\\\\rangle =\\\\alph...</td>\n",
       "      <td>A</td>\n",
       "      <td>D</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>152</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Two stars are being studied. It has been obser...</td>\n",
       "      <td>['ln(2) = [ (T_1 - T_2) / (T1*T2)]', 'ln(2) = ...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  threshold                                           question  \\\n",
       "0  203       0.25  Identify the correct sequence of reagents for ...   \n",
       "1  266       0.25  There is a C-NOT gate where the condition is t...   \n",
       "2  152       0.25  Two stars are being studied. It has been obser...   \n",
       "\n",
       "                                             choices answer predicted_answer  \\\n",
       "0  ['1. NaH; CH3CH2Br 2. H2SO4, HNO3 3. Fe-HCl 4....      D                B   \n",
       "1  ['U_{C-NOT}\\\\left|\\\\psi\\\\right\\\\rangle =\\\\alph...      A                D   \n",
       "2  ['ln(2) = [ (T_1 - T_2) / (T1*T2)]', 'ln(2) = ...      A                A   \n",
       "\n",
       "   confidence  \n",
       "0    0.666667  \n",
       "1    0.666667  \n",
       "2    1.000000  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Normalize strings & types\n",
    "df[\"answer\"] = df[\"answer\"].astype(str).str.strip().str.upper()\n",
    "df[\"predicted_answer\"] = df[\"predicted_answer\"].astype(str).str.strip().str.upper()\n",
    "df[\"confidence\"] = pd.to_numeric(df[\"confidence\"], errors=\"coerce\").fillna(0.0)\n",
    "df[\"threshold\"] = pd.to_numeric(df[\"threshold\"], errors=\"coerce\")\n",
    "\n",
    "print(f\"✅ Loaded {len(df)} rows from {CSV_PATH.name}\")\n",
    "print(\"Thresholds found:\", sorted(df[\"threshold\"].unique()))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "21ffea09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=0.25: 255 rows\n",
      "t=0.5: 255 rows\n",
      "t=0.75: 255 rows\n",
      "t=0.9: 255 rows\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 4 — Split into per-threshold DataFrames =====\n",
    "dfs_by_t = {t: df[df[\"threshold\"] == t].copy() for t in THRESHOLDS}\n",
    "\n",
    "for t in THRESHOLDS:\n",
    "    print(f\"t={t}: {len(dfs_by_t[t])} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "426ec340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 5 — Compute metrics per threshold =====\n",
    "metrics_rows = []\n",
    "\n",
    "for t in THRESHOLDS:\n",
    "    df_t = dfs_by_t[t]\n",
    "\n",
    "    metrics_rows.append({\n",
    "        \"threshold\": t,\n",
    "        \"accuracy_at_t\": accuracy_at_threshold(df_t, t),\n",
    "        \"coverage\": coverage(df_t, t),\n",
    "        \"penalty_mean\": penalty_adjusted_mean(df_t, t),\n",
    "        \"overconf_rate\": overconfidence_rate(df_t, t),\n",
    "        \"answered_n\": int((df_t[\"confidence\"] > t).sum()),\n",
    "        \"total_n\": len(df_t)\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_rows).sort_values(\"threshold\").reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c062912a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4×4 Evaluation Table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.334646</td>\n",
       "      <td>0.996078</td>\n",
       "      <td>0.147756</td>\n",
       "      <td>0.662745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.348624</td>\n",
       "      <td>0.854902</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.556863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.338798</td>\n",
       "      <td>0.717647</td>\n",
       "      <td>-1.111765</td>\n",
       "      <td>0.474510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>0.401460</td>\n",
       "      <td>0.537255</td>\n",
       "      <td>-2.678431</td>\n",
       "      <td>0.321569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "threshold                                                      \n",
       "0.25            0.334646  0.996078      0.147756       0.662745\n",
       "0.50            0.348624  0.854902     -0.200000       0.556863\n",
       "0.75            0.338798  0.717647     -1.111765       0.474510\n",
       "0.90            0.401460  0.537255     -2.678431       0.321569"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results in: outputs\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 6 — 4×4 evaluation table =====\n",
    "eval_table = (\n",
    "    metrics_df[[\"threshold\", \"accuracy_at_t\", \"coverage\", \"penalty_mean\", \"overconf_rate\"]]\n",
    "    .set_index(\"threshold\")\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# Display\n",
    "print(\"4×4 Evaluation Table:\")\n",
    "display(eval_table)\n",
    "\n",
    "# Save both detailed and compact tables\n",
    "eval_table.to_csv(OUTPUT_PATH / \"gpt-gpqa-metric-eval.csv\")\n",
    "\n",
    "print(f\"Saved results in: {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf85e3cb",
   "metadata": {},
   "source": [
    "### Baseline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0d837711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG — eligible rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.334646</td>\n",
       "      <td>0.996078</td>\n",
       "      <td>0.147756</td>\n",
       "      <td>0.662745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.348624</td>\n",
       "      <td>0.854902</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.556863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.338798</td>\n",
       "      <td>0.717647</td>\n",
       "      <td>-1.111765</td>\n",
       "      <td>0.474510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>0.401460</td>\n",
       "      <td>0.537255</td>\n",
       "      <td>-2.678431</td>\n",
       "      <td>0.321569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "threshold                                                      \n",
       "0.25            0.334646  0.996078      0.147756       0.662745\n",
       "0.50            0.348624  0.854902     -0.200000       0.556863\n",
       "0.75            0.338798  0.717647     -1.111765       0.474510\n",
       "0.90            0.401460  0.537255     -2.678431       0.321569"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected t* = 0.9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "accuracy_at_t    0.401460\n",
       "coverage         0.537255\n",
       "penalty_mean    -2.678431\n",
       "overconf_rate    0.321569\n",
       "Name: 0.9, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "coverage_floor = 0.3\n",
    "\n",
    "eligible = eval_table[eval_table[\"coverage\"] >= coverage_floor]\n",
    "print(\"DEBUG — eligible rows:\")\n",
    "display(eligible)\n",
    "\n",
    "best_row = eligible.loc[eligible[\"accuracy_at_t\"].idxmax()]\n",
    "t_star = best_row.name\n",
    "\n",
    "print(f\"Selected t* = {t_star}\")\n",
    "display(best_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ebad8958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary-grading baseline:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Binary</th>\n",
       "      <td>0.319608</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.680392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "Binary       0.319608       1.0           NaN       0.680392"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Always-abstain baseline:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Abstain</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "Abstain            0.0       0.0           0.0            0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Cell 2 – Compute binary-grading and always-abstain baselines =====\n",
    "import numpy as np\n",
    "\n",
    "# Load the original prediction CSV (same used to compute metrics_df)\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Compute overall accuracy\n",
    "binary_acc = np.mean(df[\"predicted_answer\"] == df[\"answer\"])\n",
    "\n",
    "# Wrong rate (for overconfidence)\n",
    "wrong_rate = 1 - binary_acc\n",
    "\n",
    "# Binary baseline metrics (answers everything)\n",
    "binary_row = {\n",
    "    \"accuracy_at_t\": binary_acc,\n",
    "    \"coverage\": 1.0,\n",
    "    \"penalty_mean\": np.nan,  # you can fill with projected penalty if desired\n",
    "    \"overconf_rate\": wrong_rate,\n",
    "}\n",
    "\n",
    "# Always-abstain baseline metrics\n",
    "abstain_row = {\n",
    "    \"accuracy_at_t\": 0.0,\n",
    "    \"coverage\": 0.0,\n",
    "    \"penalty_mean\": 0.0,\n",
    "    \"overconf_rate\": 0.0,\n",
    "}\n",
    "\n",
    "print(\"Binary-grading baseline:\")\n",
    "display(pd.DataFrame([binary_row], index=[\"Binary\"]))\n",
    "print(\"Always-abstain baseline:\")\n",
    "display(pd.DataFrame([abstain_row], index=[\"Abstain\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "fb5273cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 3x4 Headline Evaluation Table ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Confidence-aware (t*=0.9)</th>\n",
       "      <td>0.401460</td>\n",
       "      <td>0.537255</td>\n",
       "      <td>-2.678431</td>\n",
       "      <td>0.321569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Binary grading</th>\n",
       "      <td>0.319608</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.680392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Always abstain</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           accuracy_at_t  coverage  penalty_mean  \\\n",
       "Confidence-aware (t*=0.9)       0.401460  0.537255     -2.678431   \n",
       "Binary grading                  0.319608  1.000000           NaN   \n",
       "Always abstain                  0.000000  0.000000      0.000000   \n",
       "\n",
       "                           overconf_rate  \n",
       "Confidence-aware (t*=0.9)       0.321569  \n",
       "Binary grading                  0.680392  \n",
       "Always abstain                  0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Cell 3 – Build final 3x4 headline table =====\n",
    "\n",
    "headline_df = pd.DataFrame([\n",
    "    best_row[[\"accuracy_at_t\", \"coverage\", \"penalty_mean\", \"overconf_rate\"]],\n",
    "    pd.Series(binary_row),\n",
    "    pd.Series(abstain_row)\n",
    "], index=[f\"Confidence-aware (t*={t_star})\", \"Binary grading\", \"Always abstain\"])\n",
    "\n",
    "print(\"=== 3x4 Headline Evaluation Table ===\")\n",
    "display(headline_df)\n",
    "headline_df.to_csv(OUTPUT_PATH / \"gpt-gpqa-baseline-eval.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2776445b",
   "metadata": {},
   "source": [
    "## Claude Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "53eaaf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = Path(\"../inference/outputs/claude-gpqa.csv\")  # <-- change this for each run\n",
    "OUTPUT_PATH = Path(\"outputs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "62e20a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1020 rows from claude-gpqa.csv\n",
      "Thresholds found: [np.float64(0.25), np.float64(0.5), np.float64(0.75), np.float64(0.9)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>threshold</th>\n",
       "      <th>question</th>\n",
       "      <th>choices</th>\n",
       "      <th>answer</th>\n",
       "      <th>predicted_answer</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>203</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Identify the correct sequence of reagents for ...</td>\n",
       "      <td>['1. NaH; CH3CH2Br 2. H2SO4, HNO3 3. Fe-HCl 4....</td>\n",
       "      <td>D</td>\n",
       "      <td>I</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>266</td>\n",
       "      <td>0.25</td>\n",
       "      <td>There is a C-NOT gate where the condition is t...</td>\n",
       "      <td>['U_{C-NOT}\\\\left|\\\\psi\\\\right\\\\rangle =\\\\alph...</td>\n",
       "      <td>A</td>\n",
       "      <td>I</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>152</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Two stars are being studied. It has been obser...</td>\n",
       "      <td>['ln(2) = [ (T_1 - T_2) / (T1*T2)]', 'ln(2) = ...</td>\n",
       "      <td>A</td>\n",
       "      <td>I</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  threshold                                           question  \\\n",
       "0  203       0.25  Identify the correct sequence of reagents for ...   \n",
       "1  266       0.25  There is a C-NOT gate where the condition is t...   \n",
       "2  152       0.25  Two stars are being studied. It has been obser...   \n",
       "\n",
       "                                             choices answer predicted_answer  \\\n",
       "0  ['1. NaH; CH3CH2Br 2. H2SO4, HNO3 3. Fe-HCl 4....      D                I   \n",
       "1  ['U_{C-NOT}\\\\left|\\\\psi\\\\right\\\\rangle =\\\\alph...      A                I   \n",
       "2  ['ln(2) = [ (T_1 - T_2) / (T1*T2)]', 'ln(2) = ...      A                I   \n",
       "\n",
       "   confidence  \n",
       "0         1.0  \n",
       "1         1.0  \n",
       "2         1.0  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Normalize strings & types\n",
    "df[\"answer\"] = df[\"answer\"].astype(str).str.strip().str.upper()\n",
    "df[\"predicted_answer\"] = df[\"predicted_answer\"].astype(str).str.strip().str.upper()\n",
    "df[\"confidence\"] = pd.to_numeric(df[\"confidence\"], errors=\"coerce\").fillna(0.0)\n",
    "df[\"threshold\"] = pd.to_numeric(df[\"threshold\"], errors=\"coerce\")\n",
    "\n",
    "print(f\"✅ Loaded {len(df)} rows from {CSV_PATH.name}\")\n",
    "print(\"Thresholds found:\", sorted(df[\"threshold\"].unique()))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a06ed9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=0.25: 255 rows\n",
      "t=0.5: 255 rows\n",
      "t=0.75: 255 rows\n",
      "t=0.9: 255 rows\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 4 — Split into per-threshold DataFrames =====\n",
    "dfs_by_t = {t: df[df[\"threshold\"] == t].copy() for t in THRESHOLDS}\n",
    "\n",
    "for t in THRESHOLDS:\n",
    "    print(f\"t={t}: {len(dfs_by_t[t])} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "999a2a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 5 — Compute metrics per threshold =====\n",
    "metrics_rows = []\n",
    "\n",
    "for t in THRESHOLDS:\n",
    "    df_t = dfs_by_t[t]\n",
    "\n",
    "    metrics_rows.append({\n",
    "        \"threshold\": t,\n",
    "        \"accuracy_at_t\": accuracy_at_threshold(df_t, t),\n",
    "        \"coverage\": coverage(df_t, t),\n",
    "        \"penalty_mean\": penalty_adjusted_mean(df_t, t),\n",
    "        \"overconf_rate\": overconfidence_rate(df_t, t),\n",
    "        \"answered_n\": int((df_t[\"confidence\"] > t).sum()),\n",
    "        \"total_n\": len(df_t)\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_rows).sort_values(\"threshold\").reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "551c3c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4×4 Evaluation Table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.034335</td>\n",
       "      <td>0.913725</td>\n",
       "      <td>-0.259804</td>\n",
       "      <td>0.882353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.025751</td>\n",
       "      <td>0.913725</td>\n",
       "      <td>-0.855817</td>\n",
       "      <td>0.890196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.029046</td>\n",
       "      <td>0.945098</td>\n",
       "      <td>-2.721176</td>\n",
       "      <td>0.917647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>0.021097</td>\n",
       "      <td>0.929412</td>\n",
       "      <td>-8.168627</td>\n",
       "      <td>0.909804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "threshold                                                      \n",
       "0.25            0.034335  0.913725     -0.259804       0.882353\n",
       "0.50            0.025751  0.913725     -0.855817       0.890196\n",
       "0.75            0.029046  0.945098     -2.721176       0.917647\n",
       "0.90            0.021097  0.929412     -8.168627       0.909804"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results in: outputs\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 6 — 4×4 evaluation table =====\n",
    "eval_table = (\n",
    "    metrics_df[[\"threshold\", \"accuracy_at_t\", \"coverage\", \"penalty_mean\", \"overconf_rate\"]]\n",
    "    .set_index(\"threshold\")\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# Display\n",
    "print(\"4×4 Evaluation Table:\")\n",
    "display(eval_table)\n",
    "\n",
    "# Save both detailed and compact tables\n",
    "eval_table.to_csv(OUTPUT_PATH / \"claude-gpqa-metric-eval.csv\")\n",
    "\n",
    "print(f\"Saved results in: {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c847243d",
   "metadata": {},
   "source": [
    "### Baseline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "aa6ae463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG — eligible rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.034335</td>\n",
       "      <td>0.913725</td>\n",
       "      <td>-0.259804</td>\n",
       "      <td>0.882353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.025751</td>\n",
       "      <td>0.913725</td>\n",
       "      <td>-0.855817</td>\n",
       "      <td>0.890196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.029046</td>\n",
       "      <td>0.945098</td>\n",
       "      <td>-2.721176</td>\n",
       "      <td>0.917647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>0.021097</td>\n",
       "      <td>0.929412</td>\n",
       "      <td>-8.168627</td>\n",
       "      <td>0.909804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "threshold                                                      \n",
       "0.25            0.034335  0.913725     -0.259804       0.882353\n",
       "0.50            0.025751  0.913725     -0.855817       0.890196\n",
       "0.75            0.029046  0.945098     -2.721176       0.917647\n",
       "0.90            0.021097  0.929412     -8.168627       0.909804"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected t* = 0.25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "accuracy_at_t    0.034335\n",
       "coverage         0.913725\n",
       "penalty_mean    -0.259804\n",
       "overconf_rate    0.882353\n",
       "Name: 0.25, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "coverage_floor = 0.3\n",
    "\n",
    "eligible = eval_table[eval_table[\"coverage\"] >= coverage_floor]\n",
    "print(\"DEBUG — eligible rows:\")\n",
    "display(eligible)\n",
    "\n",
    "best_row = eligible.loc[eligible[\"accuracy_at_t\"].idxmax()]\n",
    "t_star = best_row.name\n",
    "\n",
    "print(f\"Selected t* = {t_star}\")\n",
    "display(best_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b48c6559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary-grading baseline:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Binary</th>\n",
       "      <td>0.027451</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.972549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "Binary       0.027451       1.0           NaN       0.972549"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Always-abstain baseline:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Abstain</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "Abstain            0.0       0.0           0.0            0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Cell 2 – Compute binary-grading and always-abstain baselines =====\n",
    "import numpy as np\n",
    "\n",
    "# Load the original prediction CSV (same used to compute metrics_df)\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Compute overall accuracy\n",
    "binary_acc = np.mean(df[\"predicted_answer\"] == df[\"answer\"])\n",
    "\n",
    "# Wrong rate (for overconfidence)\n",
    "wrong_rate = 1 - binary_acc\n",
    "\n",
    "# Binary baseline metrics (answers everything)\n",
    "binary_row = {\n",
    "    \"accuracy_at_t\": binary_acc,\n",
    "    \"coverage\": 1.0,\n",
    "    \"penalty_mean\": np.nan,  # you can fill with projected penalty if desired\n",
    "    \"overconf_rate\": wrong_rate,\n",
    "}\n",
    "\n",
    "# Always-abstain baseline metrics\n",
    "abstain_row = {\n",
    "    \"accuracy_at_t\": 0.0,\n",
    "    \"coverage\": 0.0,\n",
    "    \"penalty_mean\": 0.0,\n",
    "    \"overconf_rate\": 0.0,\n",
    "}\n",
    "\n",
    "print(\"Binary-grading baseline:\")\n",
    "display(pd.DataFrame([binary_row], index=[\"Binary\"]))\n",
    "print(\"Always-abstain baseline:\")\n",
    "display(pd.DataFrame([abstain_row], index=[\"Abstain\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f2214bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 3x4 Headline Evaluation Table ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Confidence-aware (t*=0.25)</th>\n",
       "      <td>0.034335</td>\n",
       "      <td>0.913725</td>\n",
       "      <td>-0.259804</td>\n",
       "      <td>0.882353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Binary grading</th>\n",
       "      <td>0.027451</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.972549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Always abstain</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            accuracy_at_t  coverage  penalty_mean  \\\n",
       "Confidence-aware (t*=0.25)       0.034335  0.913725     -0.259804   \n",
       "Binary grading                   0.027451  1.000000           NaN   \n",
       "Always abstain                   0.000000  0.000000      0.000000   \n",
       "\n",
       "                            overconf_rate  \n",
       "Confidence-aware (t*=0.25)       0.882353  \n",
       "Binary grading                   0.972549  \n",
       "Always abstain                   0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Cell 3 – Build final 3x4 headline table =====\n",
    "\n",
    "headline_df = pd.DataFrame([\n",
    "    best_row[[\"accuracy_at_t\", \"coverage\", \"penalty_mean\", \"overconf_rate\"]],\n",
    "    pd.Series(binary_row),\n",
    "    pd.Series(abstain_row)\n",
    "], index=[f\"Confidence-aware (t*={t_star})\", \"Binary grading\", \"Always abstain\"])\n",
    "\n",
    "print(\"=== 3x4 Headline Evaluation Table ===\")\n",
    "display(headline_df)\n",
    "headline_df.to_csv(OUTPUT_PATH / \"claude-gpqa-baseline-eval.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51da187c",
   "metadata": {},
   "source": [
    "## LLama Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "41dbf33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = Path(\"../inference/outputs/llama-gpqa.csv\")  # <-- change this for each run\n",
    "OUTPUT_PATH = Path(\"outputs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3cd3d2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1020 rows from llama-gpqa.csv\n",
      "Thresholds found: [np.float64(0.25), np.float64(0.5), np.float64(0.75), np.float64(0.9)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>threshold</th>\n",
       "      <th>question</th>\n",
       "      <th>choices</th>\n",
       "      <th>answer</th>\n",
       "      <th>predicted_answer</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>203</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Identify the correct sequence of reagents for ...</td>\n",
       "      <td>['1. NaH; CH3CH2Br 2. H2SO4, HNO3 3. Fe-HCl 4....</td>\n",
       "      <td>D</td>\n",
       "      <td>T</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>266</td>\n",
       "      <td>0.25</td>\n",
       "      <td>There is a C-NOT gate where the condition is t...</td>\n",
       "      <td>['U_{C-NOT}\\\\left|\\\\psi\\\\right\\\\rangle =\\\\alph...</td>\n",
       "      <td>A</td>\n",
       "      <td>IDK</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>152</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Two stars are being studied. It has been obser...</td>\n",
       "      <td>['ln(2) = [ (T_1 - T_2) / (T1*T2)]', 'ln(2) = ...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  threshold                                           question  \\\n",
       "0  203       0.25  Identify the correct sequence of reagents for ...   \n",
       "1  266       0.25  There is a C-NOT gate where the condition is t...   \n",
       "2  152       0.25  Two stars are being studied. It has been obser...   \n",
       "\n",
       "                                             choices answer predicted_answer  \\\n",
       "0  ['1. NaH; CH3CH2Br 2. H2SO4, HNO3 3. Fe-HCl 4....      D                T   \n",
       "1  ['U_{C-NOT}\\\\left|\\\\psi\\\\right\\\\rangle =\\\\alph...      A              IDK   \n",
       "2  ['ln(2) = [ (T_1 - T_2) / (T1*T2)]', 'ln(2) = ...      A                A   \n",
       "\n",
       "   confidence  \n",
       "0    1.000000  \n",
       "1    0.000000  \n",
       "2    0.666667  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Normalize strings & types\n",
    "df[\"answer\"] = df[\"answer\"].astype(str).str.strip().str.upper()\n",
    "df[\"predicted_answer\"] = df[\"predicted_answer\"].astype(str).str.strip().str.upper()\n",
    "df[\"confidence\"] = pd.to_numeric(df[\"confidence\"], errors=\"coerce\").fillna(0.0)\n",
    "df[\"threshold\"] = pd.to_numeric(df[\"threshold\"], errors=\"coerce\")\n",
    "\n",
    "print(f\"✅ Loaded {len(df)} rows from {CSV_PATH.name}\")\n",
    "print(\"Thresholds found:\", sorted(df[\"threshold\"].unique()))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "1b5141ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=0.25: 255 rows\n",
      "t=0.5: 255 rows\n",
      "t=0.75: 255 rows\n",
      "t=0.9: 255 rows\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 4 — Split into per-threshold DataFrames =====\n",
    "dfs_by_t = {t: df[df[\"threshold\"] == t].copy() for t in THRESHOLDS}\n",
    "\n",
    "for t in THRESHOLDS:\n",
    "    print(f\"t={t}: {len(dfs_by_t[t])} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b14e2a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 5 — Compute metrics per threshold =====\n",
    "metrics_rows = []\n",
    "\n",
    "for t in THRESHOLDS:\n",
    "    df_t = dfs_by_t[t]\n",
    "\n",
    "    metrics_rows.append({\n",
    "        \"threshold\": t,\n",
    "        \"accuracy_at_t\": accuracy_at_threshold(df_t, t),\n",
    "        \"coverage\": coverage(df_t, t),\n",
    "        \"penalty_mean\": penalty_adjusted_mean(df_t, t),\n",
    "        \"overconf_rate\": overconfidence_rate(df_t, t),\n",
    "        \"answered_n\": int((df_t[\"confidence\"] > t).sum()),\n",
    "        \"total_n\": len(df_t)\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_rows).sort_values(\"threshold\").reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "22b616ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4×4 Evaluation Table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.108280</td>\n",
       "      <td>0.615686</td>\n",
       "      <td>-0.092048</td>\n",
       "      <td>0.549020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.106061</td>\n",
       "      <td>0.517647</td>\n",
       "      <td>-0.394771</td>\n",
       "      <td>0.462745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.131579</td>\n",
       "      <td>0.447059</td>\n",
       "      <td>-1.105882</td>\n",
       "      <td>0.388235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>0.151786</td>\n",
       "      <td>0.439216</td>\n",
       "      <td>-3.286275</td>\n",
       "      <td>0.372549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "threshold                                                      \n",
       "0.25            0.108280  0.615686     -0.092048       0.549020\n",
       "0.50            0.106061  0.517647     -0.394771       0.462745\n",
       "0.75            0.131579  0.447059     -1.105882       0.388235\n",
       "0.90            0.151786  0.439216     -3.286275       0.372549"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results in: outputs\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 6 — 4×4 evaluation table =====\n",
    "eval_table = (\n",
    "    metrics_df[[\"threshold\", \"accuracy_at_t\", \"coverage\", \"penalty_mean\", \"overconf_rate\"]]\n",
    "    .set_index(\"threshold\")\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# Display\n",
    "print(\"4×4 Evaluation Table:\")\n",
    "display(eval_table)\n",
    "\n",
    "# Save both detailed and compact tables\n",
    "eval_table.to_csv(OUTPUT_PATH / \"llama-gpqa-metric-eval.csv\")\n",
    "\n",
    "print(f\"Saved results in: {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0e6127",
   "metadata": {},
   "source": [
    "### Baseline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "6c46e1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG — eligible rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.108280</td>\n",
       "      <td>0.615686</td>\n",
       "      <td>-0.092048</td>\n",
       "      <td>0.549020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.106061</td>\n",
       "      <td>0.517647</td>\n",
       "      <td>-0.394771</td>\n",
       "      <td>0.462745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.131579</td>\n",
       "      <td>0.447059</td>\n",
       "      <td>-1.105882</td>\n",
       "      <td>0.388235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>0.151786</td>\n",
       "      <td>0.439216</td>\n",
       "      <td>-3.286275</td>\n",
       "      <td>0.372549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "threshold                                                      \n",
       "0.25            0.108280  0.615686     -0.092048       0.549020\n",
       "0.50            0.106061  0.517647     -0.394771       0.462745\n",
       "0.75            0.131579  0.447059     -1.105882       0.388235\n",
       "0.90            0.151786  0.439216     -3.286275       0.372549"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected t* = 0.9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "accuracy_at_t    0.151786\n",
       "coverage         0.439216\n",
       "penalty_mean    -3.286275\n",
       "overconf_rate    0.372549\n",
       "Name: 0.9, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "coverage_floor = 0.3\n",
    "\n",
    "eligible = eval_table[eval_table[\"coverage\"] >= coverage_floor]\n",
    "print(\"DEBUG — eligible rows:\")\n",
    "display(eligible)\n",
    "\n",
    "best_row = eligible.loc[eligible[\"accuracy_at_t\"].idxmax()]\n",
    "t_star = best_row.name\n",
    "\n",
    "print(f\"Selected t* = {t_star}\")\n",
    "display(best_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "530c3ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary-grading baseline:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Binary</th>\n",
       "      <td>0.072549</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.927451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "Binary       0.072549       1.0           NaN       0.927451"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Always-abstain baseline:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Abstain</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "Abstain            0.0       0.0           0.0            0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Cell 2 – Compute binary-grading and always-abstain baselines =====\n",
    "import numpy as np\n",
    "\n",
    "# Load the original prediction CSV (same used to compute metrics_df)\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Compute overall accuracy\n",
    "binary_acc = np.mean(df[\"predicted_answer\"] == df[\"answer\"])\n",
    "\n",
    "# Wrong rate (for overconfidence)\n",
    "wrong_rate = 1 - binary_acc\n",
    "\n",
    "# Binary baseline metrics (answers everything)\n",
    "binary_row = {\n",
    "    \"accuracy_at_t\": binary_acc,\n",
    "    \"coverage\": 1.0,\n",
    "    \"penalty_mean\": np.nan,  # you can fill with projected penalty if desired\n",
    "    \"overconf_rate\": wrong_rate,\n",
    "}\n",
    "\n",
    "# Always-abstain baseline metrics\n",
    "abstain_row = {\n",
    "    \"accuracy_at_t\": 0.0,\n",
    "    \"coverage\": 0.0,\n",
    "    \"penalty_mean\": 0.0,\n",
    "    \"overconf_rate\": 0.0,\n",
    "}\n",
    "\n",
    "print(\"Binary-grading baseline:\")\n",
    "display(pd.DataFrame([binary_row], index=[\"Binary\"]))\n",
    "print(\"Always-abstain baseline:\")\n",
    "display(pd.DataFrame([abstain_row], index=[\"Abstain\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "530da1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 3x4 Headline Evaluation Table ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Confidence-aware (t*=0.9)</th>\n",
       "      <td>0.151786</td>\n",
       "      <td>0.439216</td>\n",
       "      <td>-3.286275</td>\n",
       "      <td>0.372549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Binary grading</th>\n",
       "      <td>0.072549</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.927451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Always abstain</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           accuracy_at_t  coverage  penalty_mean  \\\n",
       "Confidence-aware (t*=0.9)       0.151786  0.439216     -3.286275   \n",
       "Binary grading                  0.072549  1.000000           NaN   \n",
       "Always abstain                  0.000000  0.000000      0.000000   \n",
       "\n",
       "                           overconf_rate  \n",
       "Confidence-aware (t*=0.9)       0.372549  \n",
       "Binary grading                  0.927451  \n",
       "Always abstain                  0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Cell 3 – Build final 3x4 headline table =====\n",
    "\n",
    "headline_df = pd.DataFrame([\n",
    "    best_row[[\"accuracy_at_t\", \"coverage\", \"penalty_mean\", \"overconf_rate\"]],\n",
    "    pd.Series(binary_row),\n",
    "    pd.Series(abstain_row)\n",
    "], index=[f\"Confidence-aware (t*={t_star})\", \"Binary grading\", \"Always abstain\"])\n",
    "\n",
    "print(\"=== 3x4 Headline Evaluation Table ===\")\n",
    "display(headline_df)\n",
    "headline_df.to_csv(OUTPUT_PATH / \"llama-gpqa-baseline-eval.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c929f4c4",
   "metadata": {},
   "source": [
    "## Gemini Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "e011634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = Path(\"../inference/outputs/gemini-gpqa.csv\")  # <-- change this for each run\n",
    "OUTPUT_PATH = Path(\"outputs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "99771df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1020 rows from gemini-gpqa.csv\n",
      "Thresholds found: [np.float64(0.25), np.float64(0.5), np.float64(0.75), np.float64(0.9)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>threshold</th>\n",
       "      <th>question</th>\n",
       "      <th>choices</th>\n",
       "      <th>answer</th>\n",
       "      <th>predicted_answer</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>203</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Identify the correct sequence of reagents for ...</td>\n",
       "      <td>['1. NaH; CH3CH2Br 2. H2SO4, HNO3 3. Fe-HCl 4....</td>\n",
       "      <td>D</td>\n",
       "      <td>IDK</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>266</td>\n",
       "      <td>0.25</td>\n",
       "      <td>There is a C-NOT gate where the condition is t...</td>\n",
       "      <td>['U_{C-NOT}\\\\left|\\\\psi\\\\right\\\\rangle =\\\\alph...</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>152</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Two stars are being studied. It has been obser...</td>\n",
       "      <td>['ln(2) = [ (T_1 - T_2) / (T1*T2)]', 'ln(2) = ...</td>\n",
       "      <td>A</td>\n",
       "      <td>IDK</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  threshold                                           question  \\\n",
       "0  203       0.25  Identify the correct sequence of reagents for ...   \n",
       "1  266       0.25  There is a C-NOT gate where the condition is t...   \n",
       "2  152       0.25  Two stars are being studied. It has been obser...   \n",
       "\n",
       "                                             choices answer predicted_answer  \\\n",
       "0  ['1. NaH; CH3CH2Br 2. H2SO4, HNO3 3. Fe-HCl 4....      D              IDK   \n",
       "1  ['U_{C-NOT}\\\\left|\\\\psi\\\\right\\\\rangle =\\\\alph...      A                C   \n",
       "2  ['ln(2) = [ (T_1 - T_2) / (T1*T2)]', 'ln(2) = ...      A              IDK   \n",
       "\n",
       "   confidence  \n",
       "0        1.00  \n",
       "1        0.75  \n",
       "2        0.00  "
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Normalize strings & types\n",
    "df[\"answer\"] = df[\"answer\"].astype(str).str.strip().str.upper()\n",
    "df[\"predicted_answer\"] = df[\"predicted_answer\"].astype(str).str.strip().str.upper()\n",
    "df[\"confidence\"] = pd.to_numeric(df[\"confidence\"], errors=\"coerce\").fillna(0.0)\n",
    "df[\"threshold\"] = pd.to_numeric(df[\"threshold\"], errors=\"coerce\")\n",
    "\n",
    "print(f\"✅ Loaded {len(df)} rows from {CSV_PATH.name}\")\n",
    "print(\"Thresholds found:\", sorted(df[\"threshold\"].unique()))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "73d7f7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=0.25: 255 rows\n",
      "t=0.5: 255 rows\n",
      "t=0.75: 255 rows\n",
      "t=0.9: 255 rows\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 4 — Split into per-threshold DataFrames =====\n",
    "dfs_by_t = {t: df[df[\"threshold\"] == t].copy() for t in THRESHOLDS}\n",
    "\n",
    "for t in THRESHOLDS:\n",
    "    print(f\"t={t}: {len(dfs_by_t[t])} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3da06947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 5 — Compute metrics per threshold =====\n",
    "metrics_rows = []\n",
    "\n",
    "for t in THRESHOLDS:\n",
    "    df_t = dfs_by_t[t]\n",
    "\n",
    "    metrics_rows.append({\n",
    "        \"threshold\": t,\n",
    "        \"accuracy_at_t\": accuracy_at_threshold(df_t, t),\n",
    "        \"coverage\": coverage(df_t, t),\n",
    "        \"penalty_mean\": penalty_adjusted_mean(df_t, t),\n",
    "        \"overconf_rate\": overconfidence_rate(df_t, t),\n",
    "        \"answered_n\": int((df_t[\"confidence\"] > t).sum()),\n",
    "        \"total_n\": len(df_t)\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_rows).sort_values(\"threshold\").reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "f38ce6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4×4 Evaluation Table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.207143</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>-0.016645</td>\n",
       "      <td>0.435294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.234375</td>\n",
       "      <td>0.501961</td>\n",
       "      <td>-0.248627</td>\n",
       "      <td>0.384314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>-0.962353</td>\n",
       "      <td>0.364706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>0.189655</td>\n",
       "      <td>0.454902</td>\n",
       "      <td>-3.231373</td>\n",
       "      <td>0.368627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "threshold                                                      \n",
       "0.25            0.207143  0.549020     -0.016645       0.435294\n",
       "0.50            0.234375  0.501961     -0.248627       0.384314\n",
       "0.75            0.225000  0.470588     -0.962353       0.364706\n",
       "0.90            0.189655  0.454902     -3.231373       0.368627"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results in: outputs\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 6 — 4×4 evaluation table =====\n",
    "eval_table = (\n",
    "    metrics_df[[\"threshold\", \"accuracy_at_t\", \"coverage\", \"penalty_mean\", \"overconf_rate\"]]\n",
    "    .set_index(\"threshold\")\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# Display\n",
    "print(\"4×4 Evaluation Table:\")\n",
    "display(eval_table)\n",
    "\n",
    "# Save both detailed and compact tables\n",
    "eval_table.to_csv(OUTPUT_PATH / \"gemini-gpqa-metric-eval.csv\")\n",
    "\n",
    "print(f\"Saved results in: {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1292174",
   "metadata": {},
   "source": [
    "### Baseline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "5cc6cbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG — eligible rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threshold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.207143</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>-0.016645</td>\n",
       "      <td>0.435294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>0.234375</td>\n",
       "      <td>0.501961</td>\n",
       "      <td>-0.248627</td>\n",
       "      <td>0.384314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>-0.962353</td>\n",
       "      <td>0.364706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>0.189655</td>\n",
       "      <td>0.454902</td>\n",
       "      <td>-3.231373</td>\n",
       "      <td>0.368627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "threshold                                                      \n",
       "0.25            0.207143  0.549020     -0.016645       0.435294\n",
       "0.50            0.234375  0.501961     -0.248627       0.384314\n",
       "0.75            0.225000  0.470588     -0.962353       0.364706\n",
       "0.90            0.189655  0.454902     -3.231373       0.368627"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected t* = 0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "accuracy_at_t    0.234375\n",
       "coverage         0.501961\n",
       "penalty_mean    -0.248627\n",
       "overconf_rate    0.384314\n",
       "Name: 0.5, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "coverage_floor = 0.3\n",
    "\n",
    "eligible = eval_table[eval_table[\"coverage\"] >= coverage_floor]\n",
    "print(\"DEBUG — eligible rows:\")\n",
    "display(eligible)\n",
    "\n",
    "best_row = eligible.loc[eligible[\"accuracy_at_t\"].idxmax()]\n",
    "t_star = best_row.name\n",
    "\n",
    "print(f\"Selected t* = {t_star}\")\n",
    "display(best_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "414a8322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary-grading baseline:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Binary</th>\n",
       "      <td>0.120588</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.879412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "Binary       0.120588       1.0           NaN       0.879412"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Always-abstain baseline:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Abstain</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         accuracy_at_t  coverage  penalty_mean  overconf_rate\n",
       "Abstain            0.0       0.0           0.0            0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Cell 2 – Compute binary-grading and always-abstain baselines =====\n",
    "import numpy as np\n",
    "\n",
    "# Load the original prediction CSV (same used to compute metrics_df)\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Compute overall accuracy\n",
    "binary_acc = np.mean(df[\"predicted_answer\"] == df[\"answer\"])\n",
    "\n",
    "# Wrong rate (for overconfidence)\n",
    "wrong_rate = 1 - binary_acc\n",
    "\n",
    "# Binary baseline metrics (answers everything)\n",
    "binary_row = {\n",
    "    \"accuracy_at_t\": binary_acc,\n",
    "    \"coverage\": 1.0,\n",
    "    \"penalty_mean\": np.nan,  # you can fill with projected penalty if desired\n",
    "    \"overconf_rate\": wrong_rate,\n",
    "}\n",
    "\n",
    "# Always-abstain baseline metrics\n",
    "abstain_row = {\n",
    "    \"accuracy_at_t\": 0.0,\n",
    "    \"coverage\": 0.0,\n",
    "    \"penalty_mean\": 0.0,\n",
    "    \"overconf_rate\": 0.0,\n",
    "}\n",
    "\n",
    "print(\"Binary-grading baseline:\")\n",
    "display(pd.DataFrame([binary_row], index=[\"Binary\"]))\n",
    "print(\"Always-abstain baseline:\")\n",
    "display(pd.DataFrame([abstain_row], index=[\"Abstain\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "16abc100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 3x4 Headline Evaluation Table ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_at_t</th>\n",
       "      <th>coverage</th>\n",
       "      <th>penalty_mean</th>\n",
       "      <th>overconf_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Confidence-aware (t*=0.5)</th>\n",
       "      <td>0.234375</td>\n",
       "      <td>0.501961</td>\n",
       "      <td>-0.248627</td>\n",
       "      <td>0.384314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Binary grading</th>\n",
       "      <td>0.120588</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.879412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Always abstain</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           accuracy_at_t  coverage  penalty_mean  \\\n",
       "Confidence-aware (t*=0.5)       0.234375  0.501961     -0.248627   \n",
       "Binary grading                  0.120588  1.000000           NaN   \n",
       "Always abstain                  0.000000  0.000000      0.000000   \n",
       "\n",
       "                           overconf_rate  \n",
       "Confidence-aware (t*=0.5)       0.384314  \n",
       "Binary grading                  0.879412  \n",
       "Always abstain                  0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Cell 3 – Build final 3x4 headline table =====\n",
    "\n",
    "headline_df = pd.DataFrame([\n",
    "    best_row[[\"accuracy_at_t\", \"coverage\", \"penalty_mean\", \"overconf_rate\"]],\n",
    "    pd.Series(binary_row),\n",
    "    pd.Series(abstain_row)\n",
    "], index=[f\"Confidence-aware (t*={t_star})\", \"Binary grading\", \"Always abstain\"])\n",
    "\n",
    "print(\"=== 3x4 Headline Evaluation Table ===\")\n",
    "display(headline_df)\n",
    "headline_df.to_csv(OUTPUT_PATH / \"gemini-gpqa-baseline-eval.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac0915f",
   "metadata": {},
   "source": [
    "## Mistral Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db49319e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
