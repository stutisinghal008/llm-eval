{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c3041a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "MISTRAL_KEY= os.getenv(\"MISTRAL_KEY\")\n",
    "GEMINI_APIL_KEY= os.getenv(\"GEMINI_API_KEY\")\n",
    "ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bd9428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "# defining paths to output and data directories to be used throughout notebook\n",
    "BASE_DIR = pathlib.Path.cwd().resolve().parents[1]\n",
    "\n",
    "DATA_DIR = BASE_DIR /\"data\"\n",
    "OUT_DIR = BASE_DIR / \"inference\" / \"outputs\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe9cf3f",
   "metadata": {},
   "source": [
    "## Preparing MMLU Dataset for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd68e004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# loading dataset from hugging face\n",
    "dataset = load_dataset(\"TIGER-Lab/MMLU-Pro\", split=\"test[:1000]\")\n",
    "df = dataset.to_pandas()\n",
    "\n",
    "df = df[[\"question_id\", \"question\", \"options\", \"answer\"]].rename(columns={\"question_id\": \"id\", \"options\": \"choices\"})\n",
    "df.to_csv(DATA_DIR / \"mmlu_pro_1k.csv\", index=False)\n",
    "\n",
    "print(\"Cleaned and saved:\", len(df), \"rows â†’\", DATA_DIR / \"mmlu_pro_1k.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c876811",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "calib_df, eval_df = train_test_split(df, test_size=0.85, random_state=42, shuffle=True)\n",
    "calib_df.to_csv(DATA_DIR / \"calibration_split.csv\", index=False)\n",
    "eval_df.to_csv(DATA_DIR / \"evaluation_split.csv\", index=False)\n",
    "\n",
    "print(\"Split complete:\")\n",
    "print(\"Calibration:\", len(calib_df), \"rows\")\n",
    "print(\"Evaluation :\", len(eval_df), \"rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad17ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calibration sample:\")\n",
    "display(calib_df.head(3))\n",
    "\n",
    "print(\"Evaluation sample:\")\n",
    "display(eval_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e629695",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac643ad",
   "metadata": {},
   "source": [
    "## QWEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27153e8",
   "metadata": {},
   "source": [
    "Loading model to local device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6d8bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import gc\n",
    "\n",
    "major, minor = map(int, transformers.__version__.split('.')[:2])\n",
    "if (major, minor) < (4, 37):\n",
    "    raise ValueError(f\"Transformers {transformers.__version__} too old.\")\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "print(f\"Loading: {MODEL_NAME}\")\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    dtype = torch.float16\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    dtype = torch.float32\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=dtype,\n",
    "    device_map=None, \n",
    "    token=HF_TOKEN\n",
    ").to(device)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Model loaded successfully on {device} with dtype={dtype}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440c211e",
   "metadata": {},
   "source": [
    "Running inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6677c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from collections import Counter\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "THRESHOLDS = [0.25, 0.5, 0.75, 0.9]\n",
    "\n",
    "# helper to extract model's answer\n",
    "# If model says \"I don't know\" -> IDK\n",
    "# otherwise pull first letter\n",
    "def extract_letter(text):\n",
    "    text_clean = text.strip().upper()\n",
    "\n",
    "    if \"I DON'T KNOW\" in text_clean:\n",
    "        return \"IDK\"\n",
    "\n",
    "    m = re.search(r\"\\b([A-D])\\b\", text_clean)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "# helper function to reate the multiple choice prompt for model\n",
    "# dynamically inserts t\n",
    "# formats the choices as A/B/C/D options\n",
    "def build_mcq_prompt(row, t):\n",
    "    opts = row[\"choices\"]\n",
    "    # if choices column is a string  \"['A', 'B', 'C']\" convert it back to list\n",
    "    if isinstance(opts, str):\n",
    "        try:\n",
    "            opts = ast.literal_eval(opts)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    lines = [f\"Q: {row['question']}\", \"Options:\"]\n",
    "    if isinstance(opts, (list, tuple)):\n",
    "        labels = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "        for i, choice in enumerate(opts):\n",
    "            lines.append(f\"{labels[i]}. {choice}\")\n",
    "    else:\n",
    "        lines.append(str(opts))\n",
    "    \n",
    "    # prompt\n",
    "    # confidence instructions\n",
    "    # answer ONLY if its confidence > t\n",
    "    # otherwise explicitly say \"I don't know\".\n",
    "    lines.append(\n",
    "        f\"\\nAnswer only if you are more than {t} confident. \"\n",
    "        f\"Mistakes incur {t}/(1âˆ’{t}) points, correct = +1, IDK = 0. \"\n",
    "        \"If unsure, respond exactly with 'I don't know.'\\n\"\n",
    "        \"Provide only the capital letter of your chosen answer (A, B, C, D, ...) \"\n",
    "        \"or 'I don't know'.\"\n",
    "    )\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# generates n samples using temperature sampling\n",
    "# treating each generation as vote for an answer.\n",
    "# final prediction = majority vote.\n",
    "# p = (# votes for majority) / n.\n",
    "def qwen_answer_and_conf(prompt, n=6, temperature=0.7):\n",
    "    votes = []\n",
    "    for _ in range(n):\n",
    "        try:\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            formatted_prompt = tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "\n",
    "            inputs = tokenizer(\n",
    "                formatted_prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=512\n",
    "            ).to(model.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=10,\n",
    "                    temperature=temperature,\n",
    "                    do_sample=True if temperature > 0 else False,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "\n",
    "            txt = tokenizer.decode(\n",
    "                outputs[0][inputs['input_ids'].shape[1]:],\n",
    "                skip_special_tokens=True\n",
    "            ).strip()\n",
    "\n",
    "            letter = extract_letter(txt)\n",
    "            if letter:\n",
    "                votes.append(letter)\n",
    "\n",
    "            del inputs, outputs\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    if not votes:\n",
    "        return \"IDK\", 0.0\n",
    "\n",
    "    counts = Counter(votes)\n",
    "    pred = counts.most_common(1)[0][0]\n",
    "    conf = counts[pred] / len(votes)\n",
    "    return pred, float(conf)\n",
    "\n",
    "\n",
    "subset_df = pd.read_csv(DATA_DIR/\"evaluation_split.csv\")\n",
    "\n",
    "qwen_rows = []\n",
    "total_q = len(subset_df)\n",
    "\n",
    "# sample n times\n",
    "# convert each output to letter / IDK\n",
    "# majority vote for prediction\n",
    "# p = freq(pred) / n\n",
    "for t in THRESHOLDS:\n",
    "    print(f\"\\nRunning inference for threshold t={t}\")\n",
    "    for i, row in subset_df.iterrows():\n",
    "        # building prompt with curr t\n",
    "        prompt = build_mcq_prompt(row, t=t) \n",
    "        \n",
    "        print(f\"[t={t} | {i+1}/{total_q}] Asking Qwen2.5-1.5B ...\")\n",
    "        pred, conf = qwen_answer_and_conf(prompt, n=6)\n",
    "        \n",
    "        qwen_rows.append({\n",
    "            \"id\": row[\"id\"],\n",
    "            \"threshold\": t,\n",
    "            \"question\": row[\"question\"],\n",
    "            \"choices\": row[\"choices\"],\n",
    "            \"answer\": row[\"answer\"],\n",
    "            \"predicted_answer\": pred,\n",
    "            \"confidence\": conf\n",
    "        })\n",
    "\n",
    "        if (i + 1) % 5 == 0:\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "qwen_out = pd.DataFrame(qwen_rows)\n",
    "qwen_path= OUT_DIR / f\"qwen-mmlu.csv\"\n",
    "qwen_out.to_csv(qwen_path, index=False)\n",
    "print(f\"Qwen2.5-1.5B predictions for all thresholds saved to {qwen_path}\")\n",
    "qwen_out.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78b3c85",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5b479c",
   "metadata": {},
   "source": [
    "## GPT-4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2491be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, time, ast, os\n",
    "from collections import Counter\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "THRESHOLDS = [0.25, 0.50, 0.75, 0.90]\n",
    "\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "DATASET_NAME = \"mmlu\"\n",
    "\n",
    "OUTPUT_PATH = OUT_DIR / f\"gpt-{DATASET_NAME}.csv\"\n",
    "print(\"Saving to:\", OUTPUT_PATH)\n",
    "\n",
    "eval_df = pd.read_csv(DATA_DIR / \"evaluation_split.csv\")\n",
    "print(\"Loaded evaluation_split.csv\", len(eval_df), \"rows\")\n",
    "\n",
    "# helper function to reate the multiple choice prompt for model\n",
    "# dynamically inserts t\n",
    "# formats the choices as A/B/C/D options\n",
    "def build_mcq_prompt(row, t):\n",
    "    opts = row[\"choices\"]\n",
    "    # if choices column is a string  \"['A', 'B', 'C']\" convert it back to list\n",
    "    if isinstance(opts, str):\n",
    "        try:\n",
    "            opts = ast.literal_eval(opts)\n",
    "        except Exception:\n",
    "            pass\n",
    "    labels = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "    lines = [f\"Q: {row['question']}\", \"Options:\"]\n",
    "    # this is what we want\n",
    "    if isinstance(opts, (list, tuple)):\n",
    "        for i, choice in enumerate(opts):\n",
    "            lines.append(f\"{labels[i]}. {choice}\")\n",
    "    else:\n",
    "        lines.append(str(opts))\n",
    "        \n",
    "    # prompt\n",
    "    # confidence instructions\n",
    "    # answer ONLY if its confidence > t\n",
    "    # otherwis explicitly say \"I don't know\"\n",
    "    lines.append(\n",
    "        f\"\\nAnswer only if you are more than {t} confident. \"\n",
    "        f\"Mistakes incur {t}/(1âˆ’{t}) penalty points, correct = +1, IDK = 0. \"\n",
    "        \"If unsure, respond exactly with 'I don't know'.\\n\"\n",
    "        \"Provide ONLY the capital letter (A, B, C, â€¦) OR 'I don't know'.\"\n",
    "    )\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# helper to extract model's answer\n",
    "# If model says \"I don't know\" -> IDK\n",
    "# otherwise pull first letter\n",
    "def extract_letter(text):\n",
    "    clean = text.strip().upper()\n",
    "    if \"I DON'T KNOW\" in clean:\n",
    "        return \"IDK\"\n",
    "    match = re.search(r\"\\b([A-Z])\\b\", clean)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "# generates n samples using temperature sampling\n",
    "# treating each generation as vote for an answer\n",
    "# final prediction = majority vote\n",
    "# p = (# votes for majority) / n\n",
    "def gpt_answer_and_conf(prompt, n=6, temperature=0.7, sleep_s=0.4):\n",
    "    votes = []\n",
    "    for _ in range(n):\n",
    "        try:\n",
    "            resp = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=temperature,\n",
    "                max_tokens=10\n",
    "            )\n",
    "            txt = resp.choices[0].message.content.strip()\n",
    "            letter = extract_letter(txt)\n",
    "            if letter:\n",
    "                votes.append(letter)\n",
    "        except Exception as e:\n",
    "            print(\"GPT Error:\", e)\n",
    "\n",
    "        time.sleep(sleep_s)\n",
    "\n",
    "    if not votes:\n",
    "        return \"IDK\", 0.0\n",
    "\n",
    "    counts = Counter(votes)\n",
    "    pred = counts.most_common(1)[0][0]\n",
    "    conf = counts[pred] / len(votes)\n",
    "    return pred, float(conf)\n",
    "\n",
    "\n",
    "if OUTPUT_PATH.exists():\n",
    "    existing = pd.read_csv(OUTPUT_PATH)\n",
    "    saved_rows = existing.to_dict(\"records\")\n",
    "    done_pairs = set(zip(existing[\"id\"], existing[\"threshold\"]))\n",
    "    print(\"Resuming â€” loaded\", len(saved_rows), \"rows.\")\n",
    "else:\n",
    "    saved_rows = []\n",
    "    done_pairs = set()\n",
    "    print(\"Starting fresh.\")\n",
    "\n",
    "SAVE_EVERY = 10\n",
    "new_since_save = 0\n",
    "total = len(eval_df)\n",
    "\n",
    "\n",
    "# sample n times\n",
    "# convert each output to letter / IDK\n",
    "# majority vote for prediction\n",
    "# p = freq(pred) / n\n",
    "for t in THRESHOLDS:\n",
    "    print(f\"\\n Threshold t={t}\")\n",
    "\n",
    "    for idx, row in eval_df.iterrows():\n",
    "        key = (row[\"id\"], float(t))\n",
    "        if key in done_pairs:\n",
    "            continue\n",
    "\n",
    "        prompt = build_mcq_prompt(row, t)\n",
    "        print(f\"[t={t}] {idx+1}/{total} â†’ GPT...\")\n",
    "\n",
    "        pred, conf = gpt_answer_and_conf(prompt)\n",
    "\n",
    "        saved_rows.append({\n",
    "            \"id\": row[\"id\"],\n",
    "            \"threshold\": t,\n",
    "            \"question\": row[\"question\"],\n",
    "            \"choices\": row[\"choices\"],\n",
    "            \"answer\": row[\"answer\"],\n",
    "            \"predicted_answer\": pred,\n",
    "            \"confidence\": conf\n",
    "        })\n",
    "        done_pairs.add(key)\n",
    "        new_since_save += 1\n",
    "\n",
    "        if new_since_save >= SAVE_EVERY:\n",
    "            pd.DataFrame(saved_rows).to_csv(OUTPUT_PATH, index=False)\n",
    "            print(f\"Saved {len(saved_rows)} rows.\")\n",
    "            new_since_save = 0\n",
    "\n",
    "\n",
    "pd.DataFrame(saved_rows).to_csv(OUTPUT_PATH, index=False)\n",
    "print(\"saved:\", OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ceb7891",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f1f06f",
   "metadata": {},
   "source": [
    "## Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e892b990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, time, ast, os\n",
    "from collections import Counter\n",
    "\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "THRESHOLDS = [0.25, 0.50, 0.75, 0.90]\n",
    "\n",
    "MODEL_NAME = \"gemini-2.5-flash-lite\"\n",
    "DATASET_NAME = \"mmlu\"\n",
    "\n",
    "OUTPUT_PATH = OUT_DIR / f\"{MODEL_NAME}-{DATASET_NAME}.csv\"\n",
    "print(\"Saving to:\", OUTPUT_PATH)\n",
    "\n",
    "eval_df = pd.read_csv(DATA_DIR / \"evaluation_split.csv\")\n",
    "print(\"Loaded evaluation_split.csv\", len(eval_df), \"rows\")\n",
    "\n",
    "\n",
    "# helper function to reate the multiple choice prompt for model\n",
    "# dynamically inserts t\n",
    "# formats the choices as A/B/C/D options\n",
    "def build_mcq_prompt(row, t):\n",
    "    opts = row[\"choices\"]\n",
    "    # if choices column is a string  \"['A', 'B', 'C']\" convert it back to list\n",
    "    if isinstance(opts, str):\n",
    "        try:\n",
    "            opts = ast.literal_eval(opts)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    labels = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "    lines = [f\"Q: {row['question']}\", \"Options:\"]\n",
    "\n",
    "    if isinstance(opts, (list, tuple)):\n",
    "        for i, choice in enumerate(opts):\n",
    "            lines.append(f\"{labels[i]}. {choice}\")\n",
    "    else:\n",
    "        lines.append(str(opts))\n",
    "\n",
    "    # prompt\n",
    "    # confidence instructions\n",
    "    # answer ONLY if its confidence > t\n",
    "    # otherwis explicitly say \"I don't know\"\n",
    "    lines.append(\n",
    "        f\"\\nAnswer only if you are more than {t} confident. \"\n",
    "        f\"Mistakes incur {t}/(1âˆ’{t}) penalty points, correct = +1, IDK = 0. \"\n",
    "        \"If unsure, respond exactly with 'I don't know'.\\n\"\n",
    "        \"Provide ONLY the capital letter (A, B, C, â€¦) OR 'I don't know'.\"\n",
    "    )\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# helper function for gemini reponse\n",
    "def extract_gemini_text(resp):\n",
    "    try:\n",
    "        return resp.text\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        return resp.candidates[0].content.parts[0].text\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "# helper to extract model's answer\n",
    "# If model says \"I don't know\" -> IDK\n",
    "# otherwise pull first letter\n",
    "def extract_letter(text):\n",
    "    clean = text.strip().upper()\n",
    "    if \"I DON'T KNOW\" in clean:\n",
    "        return \"IDK\"\n",
    "    m = re.search(r\"\\b([A-Z])\\b\", clean)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "gemini_model = genai.GenerativeModel(MODEL_NAME)\n",
    "\n",
    "# generates n samples using temperature sampling\n",
    "# treating each generation as vote for an answer\n",
    "# final prediction = majority vote\n",
    "# p = (# votes for majority) / n\n",
    "def gemini_answer_and_conf(prompt, n=6, temperature=0.7, base_sleep=1.5):\n",
    "    votes = []\n",
    "    for _ in range(n):\n",
    "        while True:\n",
    "            try:\n",
    "                resp = gemini_model.generate_content(\n",
    "                    prompt,\n",
    "                    generation_config=genai.types.GenerationConfig(\n",
    "                        temperature=temperature,\n",
    "                        max_output_tokens=10\n",
    "                    )\n",
    "                )\n",
    "                txt = extract_gemini_text(resp).strip()\n",
    "                letter = extract_letter(txt)\n",
    "                if letter:\n",
    "                    votes.append(letter)\n",
    "                time.sleep(base_sleep)\n",
    "                break\n",
    "            # auto retrying on 429 rate limits\n",
    "            # sleeps between calls to refresh limit\n",
    "            except Exception as e:\n",
    "                err = str(e)\n",
    "                print(\"Gemini Error:\", err)\n",
    "                if \"429\" in err or \"Quota exceeded\" in err:\n",
    "                    retry_sec = 45\n",
    "                    m = re.search(r\"retry_delay.*?(\\d+)\", err)\n",
    "                    if m:\n",
    "                        retry_sec = int(m.group(1))\n",
    "                    print(f\"Rate limit hit. Sleeping {retry_sec} secondsâ€¦\")\n",
    "                    time.sleep(retry_sec)\n",
    "                    continue\n",
    "                print(\"Non-quota error. Sleeping 5 secondsâ€¦\")\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "    if not votes:\n",
    "        return \"IDK\", 0.0\n",
    "    counts = Counter(votes)\n",
    "    pred = counts.most_common(1)[0][0]\n",
    "    conf = counts[pred] / len(votes)\n",
    "    return pred, float(conf)\n",
    "\n",
    "if OUTPUT_PATH.exists():\n",
    "    existing = pd.read_csv(OUTPUT_PATH)\n",
    "    saved_rows = existing.to_dict(\"records\")\n",
    "    done_pairs = set(zip(existing[\"id\"], existing[\"threshold\"]))\n",
    "    print(\"Resuming â€” loaded\", len(saved_rows), \"rows.\")\n",
    "else:\n",
    "    saved_rows = []\n",
    "    done_pairs = set()\n",
    "    print(\"Starting fresh.\")\n",
    "\n",
    "SAVE_EVERY = 10\n",
    "new_since_save = 0\n",
    "total = len(eval_df)\n",
    "\n",
    "# sample n times\n",
    "# convert each output to letter / IDK\n",
    "# majority vote for prediction\n",
    "# p = freq(pred) / n\n",
    "for t in THRESHOLDS:\n",
    "    print(f\"\\nThreshold t={t}\")\n",
    "    for idx, row in eval_df.iterrows():\n",
    "        key = (row[\"id\"], float(t))\n",
    "        if key in done_pairs:\n",
    "            continue\n",
    "        prompt = build_mcq_prompt(row, t)\n",
    "        print(f\"[t={t}] {idx+1}/{total} â†’ Gemini...\")\n",
    "        pred, conf = gemini_answer_and_conf(prompt)\n",
    "        saved_rows.append({\n",
    "            \"id\": row[\"id\"],\n",
    "            \"threshold\": t,\n",
    "            \"question\": row[\"question\"],\n",
    "            \"choices\": row[\"choices\"],\n",
    "            \"answer\": row[\"answer\"],\n",
    "            \"predicted_answer\": pred,\n",
    "            \"confidence\": conf\n",
    "        })\n",
    "        done_pairs.add(key)\n",
    "        new_since_save += 1\n",
    "        if new_since_save >= SAVE_EVERY:\n",
    "            pd.DataFrame(saved_rows).to_csv(OUTPUT_PATH, index=False)\n",
    "            print(f\"Saved {len(saved_rows)} rows.\")\n",
    "            new_since_save = 0\n",
    "pd.DataFrame(saved_rows).to_csv(OUTPUT_PATH, index=False)\n",
    "print(\"saved:\", OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36817eff",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3194e04",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2036a0",
   "metadata": {},
   "source": [
    "## Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca40839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mistralai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, time, ast, os\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "client = mistralai.Mistral(api_key=MISTRAL_KEY)\n",
    "\n",
    "MODEL_NAME = \"open-mistral-7b\"\n",
    "DATASET_NAME = \"mmlu\"\n",
    "\n",
    "OUTPUT_PATH = OUT_DIR / f\"{MODEL_NAME}-{DATASET_NAME}.csv\"\n",
    "print(\"Saving to:\", OUTPUT_PATH)\n",
    "\n",
    "eval_df = pd.read_csv(DATA_DIR / \"evaluation_split.csv\")\n",
    "print(\"Loaded evaluation_split.csv â†’\", len(eval_df), \"rows\")\n",
    "\n",
    "# helper function to reate the multiple choice prompt for model\n",
    "# dynamically inserts t\n",
    "# formats the choices as A/B/C/D options\n",
    "def build_mcq_prompt(row, t):\n",
    "    opts = row[\"choices\"]\n",
    "    # if choices column is a string  \"['A', 'B', 'C']\" convert it back to list\n",
    "    if isinstance(opts, str):\n",
    "        try:\n",
    "            opts = ast.literal_eval(opts)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    labels = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "    lines = [f\"Q: {row['question']}\", \"Options:\"]\n",
    "    # this is what we want\n",
    "    if isinstance(opts, (list, tuple)):\n",
    "        for i, c in enumerate(opts):\n",
    "            lines.append(f\"{labels[i]}. {c}\")\n",
    "    else:\n",
    "        lines.append(str(opts))\n",
    "    \n",
    "    # prompt\n",
    "    # confidence instructions\n",
    "    # answer ONLY if its confidence > t\n",
    "    # otherwis explicitly say \"I don't know\"\n",
    "    lines.append(\n",
    "        f\"\\nAnswer only if you are more than {t} confident. \"\n",
    "        f\"Mistakes incur {t}/(1âˆ’{t}) penalty points. \"\n",
    "        \"Correct = +1, IDK = 0. \"\n",
    "        \"If unsure, respond exactly with 'I don't know'.\\n\"\n",
    "        \"Provide ONLY the capital letter (A, B, C, â€¦) OR 'I don't know'.\"\n",
    "    )\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# helper to extract model's answer\n",
    "# If model says \"I don't know\" -> IDK\n",
    "# otherwise pull first letter\n",
    "def extract_letter(text):\n",
    "    clean = text.strip().upper()\n",
    "\n",
    "    if \"I DON'T KNOW\" in clean or clean == \"IDK\" or \"IDK\" in clean:\n",
    "        return \"IDK\"\n",
    "\n",
    "    m = re.search(r\"\\b([A-Z])\\b\", clean)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "# generates n samples using temperature sampling\n",
    "# treating each generation as vote for an answer\n",
    "# final prediction = majority vote\n",
    "# p = (# votes for majority) / n\n",
    "def mistral_answer_and_conf(prompt, n=6, temperature=0.7, base_sleep=1.2):\n",
    "    votes = []\n",
    "\n",
    "    for _ in range(n):\n",
    "        while True:\n",
    "            try:\n",
    "                resp = client.chat.complete(\n",
    "                    model=MODEL_NAME,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    temperature=temperature,\n",
    "                    max_tokens=10,\n",
    "                )\n",
    "\n",
    "                txt = resp.choices[0].message.content.strip()\n",
    "                letter = extract_letter(txt)\n",
    "\n",
    "                if letter:\n",
    "                    votes.append(letter)\n",
    "\n",
    "                time.sleep(base_sleep)\n",
    "                break\n",
    "\n",
    "            except Exception as e:\n",
    "                err = str(e)\n",
    "                print(\"Mistral Error:\", err)\n",
    "\n",
    "                if \"429\" in err or \"rate\" in err.lower():\n",
    "                    print(\"Rate limit hit â€” sleeping 30 sec.\")\n",
    "                    time.sleep(30)\n",
    "                    continue\n",
    "\n",
    "                print(\"Unexpected error â€” sleeping 5 sec.\")\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "\n",
    "    if not votes:\n",
    "        return \"IDK\", 0.0\n",
    "\n",
    "    counts = Counter(votes)\n",
    "    pred = counts.most_common(1)[0][0]\n",
    "    conf = counts[pred] / len(votes)\n",
    "    return pred, conf\n",
    "\n",
    "if OUTPUT_PATH.exists():\n",
    "    existing = pd.read_csv(OUTPUT_PATH)\n",
    "    saved_rows = existing.to_dict(\"records\")\n",
    "    done_pairs = set(zip(existing[\"id\"], existing[\"threshold\"]))\n",
    "    print(\"Resuming â€” loaded\", len(saved_rows), \"rows.\")\n",
    "else:\n",
    "    saved_rows = []\n",
    "    done_pairs = set()\n",
    "    print(\"Starting fresh.\")\n",
    "\n",
    "SAVE_EVERY = 10\n",
    "new_since_save = 0\n",
    "total = len(eval_df)\n",
    "\n",
    "THRESHOLDS = [0.25, 0.50, 0.75, 0.90]\n",
    "\n",
    "# sample n times\n",
    "# convert each output to letter / IDK\n",
    "# majority vote for prediction\n",
    "# p = freq(pred) / n\n",
    "for t in THRESHOLDS:\n",
    "    print(f\"\\n=== Threshold t={t} ===\")\n",
    "\n",
    "    for idx, row in eval_df.iterrows():\n",
    "        key = (row[\"id\"], float(t))\n",
    "        if key in done_pairs:\n",
    "            continue\n",
    "\n",
    "        prompt = build_mcq_prompt(row, t)\n",
    "        print(f\"[t={t}] {idx+1}/{total} â†’ Mistral API...\")\n",
    "\n",
    "        pred, conf = mistral_answer_and_conf(prompt)\n",
    "\n",
    "        saved_rows.append({\n",
    "            \"id\": row[\"id\"],\n",
    "            \"threshold\": t,\n",
    "            \"question\": row[\"question\"],\n",
    "            \"choices\": row[\"choices\"],\n",
    "            \"answer\": row[\"answer\"],\n",
    "            \"predicted_answer\": pred,\n",
    "            \"confidence\": conf\n",
    "        })\n",
    "\n",
    "        done_pairs.add(key)\n",
    "        new_since_save += 1\n",
    "\n",
    "        if new_since_save >= SAVE_EVERY:\n",
    "            pd.DataFrame(saved_rows).to_csv(OUTPUT_PATH, index=False)\n",
    "            print(f\"Saved {len(saved_rows)} rows.\")\n",
    "            new_since_save = 0\n",
    "\n",
    "pd.DataFrame(saved_rows).to_csv(OUTPUT_PATH, index=False)\n",
    "print(\"saved:\", OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715fde9c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b47c74",
   "metadata": {},
   "source": [
    "## llama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad5918a",
   "metadata": {},
   "source": [
    "Loading model to local device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc27e4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch, os\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# logged in and downloaded model from hugging face\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-3B\"\n",
    "DTYPE = torch.bfloat16\n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# loading tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-3B\",\n",
    "    token=os.getenv(\"HF_TOKEN\"),\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-3B\",\n",
    "    token=os.getenv(\"HF_TOKEN\"),\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b434582",
   "metadata": {},
   "source": [
    "Llama Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e24570",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLDS = [0.25, 0.50, 0.75, 0.90]\n",
    "DATASET_NAME = \"mmlu\"\n",
    "OUTPUT_PATH = OUT_DIR /  OUT_DIR / f\"{MODEL_NAME}-{DATASET_NAME}.csv\"\n",
    "\n",
    "print(\"Saving to:\", OUTPUT_PATH)\n",
    "\n",
    "eval_df = pd.read_csv(DATA_DIR / \"evaluation_split.csv\")\n",
    "print(\"Loaded evaluation_split.csv\", len(eval_df), \"rows\")\n",
    "\n",
    "# helper function to reate the multiple choice prompt for model\n",
    "# dynamically inserts t\n",
    "# formats the choices as A/B/C/D options\n",
    "def build_mcq_prompt(row, t):\n",
    "    opts = row[\"choices\"]\n",
    "    # if choices column is a string  \"['A', 'B', 'C']\" convert it back to list\n",
    "    if isinstance(opts, str):\n",
    "        try:\n",
    "            opts = ast.literal_eval(opts)\n",
    "        except:\n",
    "            pass\n",
    "    labels = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "    lines = [f\"Q: {row['question']}\", \"Options:\"]\n",
    "    # this is right\n",
    "    if isinstance(opts, (list, tuple)):\n",
    "        for i, choice in enumerate(opts):\n",
    "            lines.append(f\"{labels[i]}. {choice}\")\n",
    "    else:\n",
    "        lines.append(str(opts))\n",
    "    # prompt\n",
    "    # confidence instructions\n",
    "    # answer ONLY if its confidence > t\n",
    "    # otherwis explicitly say \"I don't know\"\n",
    "    lines.append(\n",
    "        f\"\\nAnswer only if you are more than {t} confident. \"\n",
    "        f\"Mistakes incur {t}/(1âˆ’{t}) penalty points, correct = +1, IDK = 0. \"\n",
    "        \"If unsure, respond exactly with 'I don't know'.\\n\"\n",
    "        \"Provide ONLY the capital letter (A, B, C, â€¦) OR 'I don't know'.\"\n",
    "    )\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# helper to extract model's answer\n",
    "# If model says \"I don't know\" -> IDK\n",
    "# otherwise pull first letter\n",
    "def extract_letter(text):\n",
    "    clean = text.strip().upper()\n",
    "    if \"I DON'T KNOW\" in clean:\n",
    "        return \"IDK\"\n",
    "    m = re.search(r\"\\b([A-Z])\\b\", clean)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "# generates n samples using temperature sampling\n",
    "# treating each generation as vote for an answer\n",
    "# final prediction = majority vote\n",
    "# p = (# votes for majority) / n\n",
    "def llama_answer_and_conf(prompt, n=6, temperature=0.7, top_p=0.9, sleep_s=0.1):\n",
    "    votes = []\n",
    "\n",
    "    for _ in range(n):\n",
    "        inputs = tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=2048\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=12,\n",
    "                do_sample=True,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        gen_ids = out[0][inputs[\"input_ids\"].shape[1]:]\n",
    "        txt = tokenizer.decode(\n",
    "            gen_ids,\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True\n",
    "        )\n",
    "\n",
    "        letter = extract_letter(txt)\n",
    "        if letter:\n",
    "            votes.append(letter)\n",
    "\n",
    "        time.sleep(sleep_s)\n",
    "\n",
    "    if not votes:\n",
    "        return \"IDK\", 0.0\n",
    "\n",
    "    counts = Counter(votes)\n",
    "    pred = counts.most_common(1)[0][0]\n",
    "    conf = counts[pred] / len(votes)\n",
    "    return pred, float(conf)\n",
    "\n",
    "saved_rows = []\n",
    "done_pairs = set()\n",
    "print(\"Starting fresh from scratch.\")\n",
    "\n",
    "\n",
    "SAVE_EVERY = 10\n",
    "new_since_save = 0\n",
    "total = len(eval_df)\n",
    "\n",
    "\n",
    "# sample n times\n",
    "# convert each output to letter / IDK\n",
    "# majority vote for prediction\n",
    "# p = freq(pred) / n\n",
    "for t in THRESHOLDS:\n",
    "    print(f\"\\n=== Threshold t={t} ===\")\n",
    "\n",
    "    for idx, row in eval_df.iterrows():\n",
    "        key = (row[\"id\"], float(t))\n",
    "        if key in done_pairs:\n",
    "            continue\n",
    "\n",
    "        print(f\"[t={t}] {idx+1}/{total} â†’ Llama...\")\n",
    "\n",
    "        prompt = build_mcq_prompt(row, t)\n",
    "        pred, conf = llama_answer_and_conf(prompt)\n",
    "\n",
    "        saved_rows.append({\n",
    "            \"id\": row[\"id\"],\n",
    "            \"threshold\": t,\n",
    "            \"question\": row[\"question\"],\n",
    "            \"choices\": row[\"choices\"],\n",
    "            \"answer\": row[\"answer\"],\n",
    "            \"predicted_answer\": pred,\n",
    "            \"confidence\": conf\n",
    "        })\n",
    "\n",
    "        done_pairs.add(key)\n",
    "        new_since_save += 1\n",
    "\n",
    "        if new_since_save >= SAVE_EVERY:\n",
    "            pd.DataFrame(saved_rows).to_csv(OUTPUT_PATH, index=False)\n",
    "            print(f\"Saved {len(saved_rows)} rows.\")\n",
    "            new_since_save = 0\n",
    "\n",
    "# Final save\n",
    "pd.DataFrame(saved_rows).to_csv(OUTPUT_PATH, index=False)\n",
    "print(\"Saved:\", OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a803a266",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948758c6",
   "metadata": {},
   "source": [
    "## Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b075a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, time, ast, os\n",
    "from collections import Counter\n",
    "\n",
    "client = Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "THRESHOLDS = [0.25, 0.50, 0.75, 0.90]\n",
    "\n",
    "MODEL_NAME = \"claude\"\n",
    "DATASET_NAME = \"mmlu\"\n",
    "\n",
    "OUTPUT_PATH = OUT_DIR / f\"{MODEL_NAME}-{DATASET_NAME}.csv\"\n",
    "print(\"Saving to:\", OUTPUT_PATH)\n",
    "\n",
    "eval_df = pd.read_csv(DATA_DIR / \"evaluation_split.csv\")\n",
    "print(\"Loaded evaluation_split.csv \", len(eval_df), \"rows\")\n",
    "\n",
    "# helper function to reate the multiple choice prompt for model\n",
    "# dynamically inserts t\n",
    "# formats the choices as A/B/C/D options\n",
    "def build_mcq_prompt(row, t):\n",
    "    opts = row[\"choices\"]\n",
    "    # if choices column is a string  \"['A', 'B', 'C']\" convert it back to list\n",
    "    if isinstance(opts, str):\n",
    "        try:\n",
    "            opts = ast.literal_eval(opts)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    labels = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "    lines = [f\"Q: {row['question']}\", \"Options:\"]\n",
    "    # this is how we want it\n",
    "    if isinstance(opts, (list, tuple)):\n",
    "        for i, choice in enumerate(opts):\n",
    "            lines.append(f\"{labels[i]}. {choice}\")\n",
    "    else:\n",
    "        lines.append(str(opts))\n",
    "    # prompt\n",
    "    # confidence instructions\n",
    "    # answer ONLY if its confidence > t\n",
    "    # otherwis explicitly say \"I don't know\"\n",
    "    lines.append(\n",
    "        f\"\\nAnswer only if you are more than {t} confident. \"\n",
    "        f\"Mistakes incur {t}/(1âˆ’{t}) penalty points, correct = +1, IDK = 0. \"\n",
    "        \"If unsure, respond exactly with 'I don't know'.\\n\"\n",
    "        \"Provide ONLY the capital letter (A, B, C, â€¦) OR 'I don't know'.\"\n",
    "    )\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# claude specific helper to parse its answer\n",
    "def extract_claude_text(resp):\n",
    "    try:\n",
    "        return resp.content[0].text\n",
    "    except:\n",
    "        return \"\"\n",
    "    \n",
    "# helper to extract model's answer\n",
    "# If model says \"I don't know\" -> IDK\n",
    "# otherwise pull first letter\n",
    "def extract_letter(text):\n",
    "    clean = text.strip().upper()\n",
    "    if \"I DON'T KNOW\" in clean:\n",
    "        return \"IDK\"\n",
    "    m = re.search(r\"\\b([A-Z])\\b\", clean)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "# generates n samples using temperature sampling\n",
    "# treating each generation as vote for an answer\n",
    "# final prediction = majority vote\n",
    "# p = (# votes for majority) / n\n",
    "def claude_answer_and_conf(prompt, n=6, temperature=0.7, base_sleep=1.2):\n",
    "    votes = []\n",
    "\n",
    "    for _ in range(n):\n",
    "        while True:\n",
    "            try:\n",
    "                resp = client.messages.create(\n",
    "                    model=\"claude-haiku-4-5\",\n",
    "                    max_tokens=20,\n",
    "                    temperature=temperature,\n",
    "                    messages=[\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                txt = extract_claude_text(resp).strip()\n",
    "                letter = extract_letter(txt)\n",
    "                if letter:\n",
    "                    votes.append(letter)\n",
    "\n",
    "                time.sleep(base_sleep)\n",
    "                break\n",
    "\n",
    "            except Exception as e:\n",
    "                err = str(e)\n",
    "                print(\"Claude Error:\", err)\n",
    "\n",
    "                # sleeping in case rate limit is hit\n",
    "                if \"rate_limit\" in err or \"429\" in err:\n",
    "                    print(\"ðŸ” Rate limit â€” sleeping 30sâ€¦\")\n",
    "                    time.sleep(30)\n",
    "                    continue\n",
    "                print(\"âš ï¸ Non-rate error â€” sleeping 5sâ€¦\")\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "\n",
    "    if not votes:\n",
    "        return \"IDK\", 0.0\n",
    "\n",
    "    counts = Counter(votes)\n",
    "    pred = counts.most_common(1)[0][0]\n",
    "    conf = counts[pred] / len(votes)\n",
    "    return pred, float(conf)\n",
    "\n",
    "if OUTPUT_PATH.exists():\n",
    "    existing = pd.read_csv(OUTPUT_PATH)\n",
    "    saved_rows = existing.to_dict(\"records\")\n",
    "    done_pairs = set(zip(existing[\"id\"], existing[\"threshold\"]))\n",
    "    print(\"Resuming â€” loaded\", len(saved_rows), \"rows.\")\n",
    "else:\n",
    "    saved_rows = []\n",
    "    done_pairs = set()\n",
    "    print(\"Starting fresh.\")\n",
    "\n",
    "SAVE_EVERY = 10\n",
    "new_since_save = 0\n",
    "total = len(eval_df)\n",
    "\n",
    "\n",
    "# sample n times\n",
    "# convert each output to letter / IDK\n",
    "# majority vote for prediction\n",
    "# p = freq(pred) / n\n",
    "for t in THRESHOLDS:\n",
    "    print(f\"\\n=== Threshold t={t} ===\")\n",
    "\n",
    "    for idx, row in eval_df.iterrows():\n",
    "        key = (row[\"id\"], float(t))\n",
    "        if key in done_pairs:\n",
    "            continue\n",
    "\n",
    "        prompt = build_mcq_prompt(row, t)\n",
    "\n",
    "        print(f\"[t={t}] {idx+1}/{total} â†’ Claude...\")\n",
    "\n",
    "        pred, conf = claude_answer_and_conf(prompt)\n",
    "\n",
    "        saved_rows.append({\n",
    "            \"id\": row[\"id\"],\n",
    "            \"threshold\": t,\n",
    "            \"question\": row[\"question\"],\n",
    "            \"choices\": row[\"choices\"],\n",
    "            \"answer\": row[\"answer\"],\n",
    "            \"predicted_answer\": pred,\n",
    "            \"confidence\": conf\n",
    "        })\n",
    "\n",
    "        done_pairs.add(key)\n",
    "        new_since_save += 1\n",
    "\n",
    "        if new_since_save >= SAVE_EVERY:\n",
    "            pd.DataFrame(saved_rows).to_csv(OUTPUT_PATH, index=False)\n",
    "            print(f\"Saved {len(saved_rows)} rows.\")\n",
    "            new_since_save = 0\n",
    "\n",
    "pd.DataFrame(saved_rows).to_csv(OUTPUT_PATH, index=False)\n",
    "print(\"Saved:\", OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
